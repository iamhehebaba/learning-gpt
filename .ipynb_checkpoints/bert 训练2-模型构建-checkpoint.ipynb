{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dab1faa-db45-40c3-9d5a-17bd5a426cb8",
   "metadata": {},
   "source": [
    "# 构建BERT(Transformer Encoder)模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453d008f-18b6-41e2-b478-dd421412ab98",
   "metadata": {},
   "source": [
    "# 1 使用pytorch Dataset格式读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86d6c14c-c34e-4b87-a85d-fee6bc887e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "input_path=\"./data/bert_output_data2.json\"\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        self.data = []\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                data = json.loads(line)\n",
    "                self.data.append(data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(item[\"input_ids\"]),\n",
    "            \"input_mask\": torch.tensor(item[\"input_mask\"]),\n",
    "            \"segment_ids\": torch.tensor(item[\"segment_ids\"]),\n",
    "            \"masked_lm_ids\": torch.tensor(item[\"masked_lm_ids\"]),\n",
    "            \"masked_lm_positions\": torch.tensor(item[\"masked_lm_positions\"]),\n",
    "            \"masked_lm_weights\": torch.tensor(item[\"masked_lm_weights\"]),\n",
    "            \"next_sentence_labels\": torch.tensor(item[\"next_sentence_labels\"])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3be040c6-b855-4bfe-9a67-994d66ff9dda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  6121,   749,  ...,  1762,  1765,   102],\n",
       "         [  101,   704,  1744,  ...,   680,   762,   102],\n",
       "         [  101,  6656,   103,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,   782,  1920,  ...,  4852,   833,   102],\n",
       "         [  101,  6587,  2398,  ...,     0,     0,     0],\n",
       "         [  101,   100, 14602,  ...,     0,     0,     0]]),\n",
       " 'input_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'segment_ids': tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
       "         [0, 0, 0,  ..., 1, 1, 1],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 1, 1, 1],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]),\n",
       " 'masked_lm_ids': tensor([[1348,  800,  738, 4995, 6371, 1542, 1447, 6150, 7368, 1146, 4638, 6716,\n",
       "           511, 7008, 3198, 3300, 2376, 6413, 8024, 4638],\n",
       "         [ 752, 2845, 1361,  818, 2773, 4372, 4886, 6592,  511, 2519, 2900, 7463,\n",
       "          1462, 7339, 1298, 3189, 6858,  704, 6809, 1506],\n",
       "         [4708, 1184, 2689, 1914, 4212, 6848, 6443,  881, 6422, 3796, 4125, 7755,\n",
       "          6944, 7676, 2458, 5517, 8024, 1377,  683, 7030],\n",
       "         [5739, 1092,  671, 2408, 1762, 6764, 1780, 1346,  511, 3299,  704, 7339,\n",
       "          1765, 5273, 8024, 1925, 4788, 1744, 6226,  683],\n",
       "         [1814,  684, 3613, 4638, 2577, 5307,  511, 1079, 1071, 1378, 2772,  924,\n",
       "          4955, 4638, 5632, 1355, 3566, 8024,  718, 2094],\n",
       "         [6134,  749, 2399,  697, 4638, 6392, 6574, 4638, 1939, 3791, 2137,  976,\n",
       "           754, 2141, 4638, 6821, 6448, 4862, 1282, 1920],\n",
       "         [7555, 3837, 4684, 2137, 7555, 1168, 5277,  511, 6587, 4680, 6598, 6574,\n",
       "          2487, 6444, 7309, 2418, 7557, 7481, 1392, 5390],\n",
       "         [ 100,  116, 3563, 2458, 5052, 6436, 3519, 3266, 5966,  510, 6206,  511,\n",
       "          6121,  897, 7032, 1486, 5468, 6084,  680, 5440]]),\n",
       " 'masked_lm_positions': tensor([[ 70,  76,  77, 135, 197, 212, 225, 234, 270, 279, 340, 341, 343, 370,\n",
       "          372, 436, 438, 443, 444, 454],\n",
       "         [ 19,  94, 192, 195, 199, 207, 230, 236, 273, 359, 366, 372, 400, 408,\n",
       "          418, 441, 459, 476, 500, 502],\n",
       "         [  2,   6,  17,  51,  66,  78,  81,  83,  91, 122, 146, 150, 164, 166,\n",
       "          169, 170, 171, 196, 208, 297],\n",
       "         [  2,   4,   7,  18,  24,  29,  37, 101, 104, 106, 125, 140, 150, 172,\n",
       "          190, 198, 204, 207, 234, 247],\n",
       "         [ 36,  43,  45,  76,  87, 105, 150, 162, 191, 217, 239, 291, 295, 319,\n",
       "          340, 385, 435, 453, 482, 499],\n",
       "         [  9,  10, 134, 159, 164, 174, 178, 181, 223, 239, 253, 317, 343, 351,\n",
       "          363, 410, 414, 463, 488, 495],\n",
       "         [  7,  42,  52,  55,  60,  65,  75,  85,  90,  95, 109, 110, 115, 116,\n",
       "          159, 171, 181, 184, 222, 235],\n",
       "         [  1,   2,   9,  15,  17,  20,  21,  27,  28,  39,  56,  58,  69,  90,\n",
       "           93, 105, 108, 111, 149, 163]]),\n",
       " 'masked_lm_weights': tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.]]),\n",
       " 'next_sentence_labels': tensor([1, 0, 0, 0, 1, 1, 1, 0])}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = MyDataset(input_path)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "for data in train_loader:\n",
    "    data\n",
    "    break\n",
    "print(data[\"input_ids\"].shape)\n",
    "print(data[\"input_mask\"].shape)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8564178-6ee5-4e24-926a-4c22dbe53d8f",
   "metadata": {},
   "source": [
    "# BERT 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b1b82b8-6bee-4ddc-a097-7dc904eb920d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21128"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BertConfig:\n",
    "    def __init__(self, vocab_size, hidden_size=144, num_hidden_layers=3, num_attention_heads=12,\n",
    "                 intermediate_size=512, hidden_act='gelu', hidden_dropout_prob=0.1,\n",
    "                 attention_probs_dropout_prob=0.1, max_position_embeddings=512,\n",
    "                 type_vocab_size=2, initializer_range=0.02):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.hidden_act = hidden_act\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.hidden_dropout_prob = hidden_dropout_prob\n",
    "        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.type_vocab_size = type_vocab_size\n",
    "        self.initializer_range = initializer_range\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('/Users/wangaijun/pythoncode/github/model/bert-base-chinese')\n",
    "vocab_words = list(tokenizer.vocab.keys())\n",
    "config=BertConfig(len(vocab_words))\n",
    "config.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb372d03-c4af-4e56-aaa1-584ed9791389",
   "metadata": {},
   "source": [
    "# 2.1 Embedding层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27a5ad05-8c5b-4641-9688-e1c874ed5802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids shape: torch.Size([8, 512]) token_type_ids shape torch.Size([8, 512])\n"
     ]
    }
   ],
   "source": [
    "input_ids=data[\"input_ids\"]\n",
    "token_type_ids=data[\"segment_ids\"]\n",
    "print(\"input_ids shape:\",input_ids.shape,\"token_type_ids shape\",token_type_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2127ce72-2172-4f9f-ae36-44d605be4b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 512, 144]) torch.Size([512, 144]) torch.Size([8, 512, 144])\n"
     ]
    }
   ],
   "source": [
    "word_embeddings = nn.Embedding(21128, config.hidden_size)\n",
    "position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
    "\n",
    "token_emb=word_embeddings(input_ids)\n",
    "position_emb=position_embeddings(torch.arange(input_ids.shape[1], dtype=torch.long, device=input_ids.device))\n",
    "sentence_emb=token_type_embeddings(token_type_ids)\n",
    "\n",
    "print(token_emb.shape,position_emb.shape,sentence_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05d5a630-19fb-4dac-b9e4-39cd7d227a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEmbeddings(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(BertEmbeddings, self).__init__()\n",
    "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None):\n",
    "        seq_length = input_ids.size(1)\n",
    "        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros_like(input_ids)\n",
    "\n",
    "        words_embeddings = self.word_embeddings(input_ids)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
    "\n",
    "        embeddings = words_embeddings + position_embeddings + token_type_embeddings\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "beaa797a-534a-4506-a640-3f3838d95e37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 512, 144])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embModel=BertEmbeddings(config)\n",
    "x_emb=embModel(input_ids,token_type_ids)\n",
    "x_emb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6883a335-b150-4e6e-831b-dffcb35cd4ad",
   "metadata": {},
   "source": [
    "# 2.2 attention 层"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83367081-4808-4411-8fd4-80fc2789858d",
   "metadata": {},
   "source": [
    "### 2.2.1 q,k,v加工及多头变换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4716e00-aa0b-4c86-b9bc-9aa7c466ef0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 512, 144]) torch.Size([8, 512, 144]) torch.Size([8, 512, 144])\n",
      "torch.Size([8, 12, 512, 12]) torch.Size([8, 12, 512, 12]) torch.Size([8, 12, 512, 12])\n"
     ]
    }
   ],
   "source": [
    "c_attn = nn.Linear(config.hidden_size, 3 * config.hidden_size)\n",
    "\n",
    "# q,k,v 都来自于x \n",
    "q, k, v  = c_attn(x_emb).split(config.hidden_size, dim=2)\n",
    "print(q.shape,k.shape,v.shape)\n",
    "\n",
    "B,T,C=x_emb.shape\n",
    "# 给q,k,v 增加head\n",
    "q = q.view(B, T, config.num_attention_heads, C // config.num_attention_heads).transpose(1, 2) # (B, nh, T, hs)\n",
    "k = k.view(B, T, config.num_attention_heads, C // config.num_attention_heads).transpose(1, 2) # (B, nh, T, hs)\n",
    "v = v.view(B, T, config.num_attention_heads, C // config.num_attention_heads).transpose(1, 2) # (B, nh, T, hs)\n",
    "print(q.shape,k.shape,v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cd1794-2895-48fc-b166-357c05381cbe",
   "metadata": {},
   "source": [
    "#### 2.2.2 attention score 计算以及mask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ae97d39-1829-434e-8787-ea458270a24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_scores shape : torch.Size([8, 12, 512, 512])\n",
      "attention_mask shape : torch.Size([8, 1, 1, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 512, 144])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "attention_scores = torch.matmul(q, k.transpose(-1, -2))/ math.sqrt(q.shape[-1])\n",
    "print(\"attention_scores shape :\",attention_scores.shape)\n",
    "\n",
    "attention_mask=data[\"input_mask\"]\n",
    "attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "print(\"attention_mask shape :\",attention_mask.shape)\n",
    "\n",
    "attention_scores = attention_scores.masked_fill(attention_mask == 0, float('-inf'))\n",
    "\n",
    "# Normalize the attention scores to probabilities.\n",
    "attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
    "\n",
    "context_layer = torch.matmul(attention_probs, v)\n",
    "context_layer = context_layer.transpose(1, 2).contiguous().view(B, T, C) \n",
    "context_layer.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90857b92-e575-4807-9c5c-cd43df751a1c",
   "metadata": {
    "scrolled": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fc0265a6-78a2-4c56-bc47-e266b09bc066",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(BertSelfAttention, self).__init__()\n",
    "        if config.hidden_size % config.num_attention_heads!= 0:\n",
    "            raise ValueError(\n",
    "                \"The hidden size (%d) is not a multiple of the number of attention heads (%d)\" % (\n",
    "                    config.hidden_size, config.num_attention_heads))\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
    "        self.hidden_size=config.hidden_size\n",
    "\n",
    "        self.query = nn.Linear(config.hidden_size, self.hidden_size)\n",
    "        self.key = nn.Linear(config.hidden_size, self.hidden_size)\n",
    "        self.value = nn.Linear(config.hidden_size, self.hidden_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask):\n",
    "        B, T, C = hidden_states.size()\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "        mixed_key_layer = self.key(hidden_states)\n",
    "        mixed_value_layer = self.value(hidden_states)\n",
    "        \n",
    "        query_layer = mixed_query_layer.view(B, T, self.num_attention_heads, C // self.num_attention_heads).transpose(1, 2) # (B, nh, T, hs)\n",
    "        key_layer = mixed_key_layer.view(B, T, self.num_attention_heads, C // self.num_attention_heads).transpose(1, 2) # (B, nh, T, hs)\n",
    "        value_layer = mixed_value_layer.view(B, T, self.num_attention_heads, C // self.num_attention_heads).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "            attention_scores = attention_scores.masked_fill(attention_mask == 0, float('-inf'))\n",
    "\n",
    "        # Normalize the attention scores to probabilities.\n",
    "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
    "\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        context_layer = context_layer.transpose(1, 2).contiguous().view(B, T, C) \n",
    "        return context_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5d5ba4f1-6b6c-4bb0-9299-9edf06172d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_mask shape torch.Size([8, 512])\n",
      "torch.Size([8, 12, 512, 12])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 512, 144])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_mask=data[\"input_mask\"]\n",
    "print(\"input_mask shape\",input_mask.shape)\n",
    "attenModel=BertSelfAttention(config)\n",
    "x_att=attenModel(x_emb,input_mask)\n",
    "x_att.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099eae3a-47c3-4f84-964b-a0900e8ec428",
   "metadata": {},
   "source": [
    "## 3 attention +add&Norm\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"images/encoder_atten.png\" alt=\"Image\" style=\"width:300px;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e718e0d5-b628-4908-bbb9-5cc96c2a5482",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSelfOutput(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(BertSelfOutput, self).__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states\n",
    "\n",
    "class BertAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(BertAttention, self).__init__()\n",
    "        self.self = BertSelfAttention(config)\n",
    "        self.output = BertSelfOutput(config)\n",
    "\n",
    "    def forward(self, input_tensor, attention_mask):\n",
    "        self_output = self.self(input_tensor, attention_mask)\n",
    "        attention_output = self.output(self_output, input_tensor)\n",
    "        return attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74839b6b-e119-45e2-9089-62ab46835082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 512, 144])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bertAttModel=BertAttention(config)\n",
    "x_att=bertAttModel(x_emb,input_mask)\n",
    "x_att.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b21de25-391a-4705-8879-71b541f43071",
   "metadata": {},
   "source": [
    "## 4 FFN层\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"images/encoder_mlp.png\" alt=\"Image\" style=\"width:300px;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8bed3288-d88a-481c-a096-1845296951a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(FFN, self).__init__()\n",
    "        self.dense1 = nn.Linear(config.hidden_size, config.intermediate_size)        \n",
    "        self.dense2 = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        \n",
    "    def gelu(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden_states=x\n",
    "        hidden_states = self.dense1(hidden_states)\n",
    "        hidden_states = self.gelu(hidden_states)\n",
    "        hidden_states = self.dense2(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + x)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8990bf2-cae7-4c5d-b516-d83bc8c61c31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 512, 144])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp=FFN(config)\n",
    "x_mlp=mlp(x_att)\n",
    "x_mlp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e149bf2-3ca6-4399-8f46-2950c2e81d6d",
   "metadata": {},
   "source": [
    "## 4 Block层\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"images/encoder_block.png\" alt=\"Image\" style=\"width:300px;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d090f55-1ae0-4e8a-8c74-12991178fb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOCK 层\n",
    "class BertLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(BertLayer, self).__init__()\n",
    "        self.attention = BertAttention(config)\n",
    "        self.mlp = FFN(config)\n",
    "       \n",
    "    def forward(self, hidden_states, attention_mask):\n",
    "        attention_output = self.attention(hidden_states, attention_mask)\n",
    "        layer_output = self.mlp(attention_output)\n",
    "    \n",
    "        return layer_output\n",
    "\n",
    "# 多层BLOCK层\n",
    "class BertEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(BertEncoder, self).__init__()\n",
    "        self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask, output_all_encoded_layers=True):\n",
    "        all_encoder_layers = []\n",
    "        for layer_module in self.layer:\n",
    "            hidden_states = layer_module(hidden_states, attention_mask)\n",
    "            if output_all_encoded_layers:\n",
    "                all_encoder_layers.append(hidden_states)\n",
    "        if not output_all_encoded_layers:\n",
    "            all_encoder_layers.append(hidden_states)\n",
    "        return all_encoder_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf81a306-c001-4076-885f-5268fbdfc7c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 512, 144])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model=BertEncoder(config)\n",
    "x_bloks=bert_model(x_emb,input_mask)\n",
    "x_bloks[-1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcdd7e3-db24-4a85-a461-cd8b710dab74",
   "metadata": {},
   "source": [
    "## 5 CLS 输出层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "563e9e54-7e4f-4850-a730-7206c0a72f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertPooler(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(BertPooler, self).__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
    "        # to the first token.\n",
    "        first_token_tensor = hidden_states[:, 0]\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5dbf6012-85db-4b88-b691-e57f2a0477f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 144])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clsModel=BertPooler(config)\n",
    "x_cls=clsModel(x_bloks[-1])\n",
    "x_cls.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9617bd29-93b0-4faa-89b7-6b408fa43bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 512, 144]) torch.Size([8, 512])\n",
      "torch.Size([8, 12, 512, 12])\n",
      "torch.Size([8, 12, 512, 12])\n",
      "torch.Size([8, 12, 512, 12])\n",
      "torch.Size([8, 144]) torch.Size([8, 512, 144]) 3\n",
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(21128, 144)\n",
      "    (position_embeddings): Embedding(512, 144)\n",
      "    (token_type_embeddings): Embedding(2, 144)\n",
      "    (LayerNorm): LayerNorm((144,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-2): 3 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=144, out_features=144, bias=True)\n",
      "            (key): Linear(in_features=144, out_features=144, bias=True)\n",
      "            (value): Linear(in_features=144, out_features=144, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=144, out_features=144, bias=True)\n",
      "            (LayerNorm): LayerNorm((144,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (mlp): FFN(\n",
      "          (dense1): Linear(in_features=144, out_features=512, bias=True)\n",
      "          (dense2): Linear(in_features=512, out_features=144, bias=True)\n",
      "          (LayerNorm): LayerNorm((144,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=144, out_features=144, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class BertModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(BertModel, self).__init__()\n",
    "        self.embeddings = BertEmbeddings(config)\n",
    "        self.encoder = BertEncoder(config)\n",
    "        self.pooler = BertPooler(config)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None):\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones_like(input_ids)\n",
    "        embedding_output = self.embeddings(input_ids, token_type_ids)\n",
    "        print(embedding_output.shape,attention_mask.shape)\n",
    "        encoded_layers = self.encoder(embedding_output, attention_mask)\n",
    "        sequence_output = encoded_layers[-1]\n",
    "        pooled_output = self.pooler(sequence_output)\n",
    "        return pooled_output, sequence_output, encoded_layers\n",
    "\n",
    "bert_model = BertModel(config)\n",
    "\n",
    "input_mask=data[\"input_mask\"]\n",
    "input_ids=data[\"input_ids\"]\n",
    "token_type_ids=data[\"segment_ids\"]\n",
    "pooled_output, sequence_output, encoded_layers=bert_model(input_ids,token_type_ids,input_mask)\n",
    "print(pooled_output.shape,sequence_output.shape,len(encoded_layers))\n",
    "print(bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ebb01d-26bf-4908-9aea-0ce9bf7dc555",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9432f98b-889f-4e99-90d2-17c0bbc65e36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 512])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_type_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "539219dd-69a0-42fc-8d75-451962d81ef1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1, 1, 512])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20cc4df-6d21-4882-adeb-b2b460c16722",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "torch2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
