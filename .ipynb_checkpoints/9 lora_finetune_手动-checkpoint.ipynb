{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "610bba07-81ae-48fc-a30d-cd0dd4d4e1bb",
   "metadata": {},
   "source": [
    "# 手动实现GPT模型 LORA训练\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923e407e-3c72-4762-b094-33f6a6ac56b7",
   "metadata": {},
   "source": [
    "# 1 加载原始模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae30af08-a27f-474e-8e24-15f79c158157",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "class config:\n",
    "    block_size=1024\n",
    "    vocab_size=50304\n",
    "    n_layer=12\n",
    "    n_head=12\n",
    "    n_embd=768\n",
    "    dropout=0.0\n",
    "    bias=False\n",
    "    \n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.gelu    = nn.GELU()\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
    "            # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                        .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        if self.flash:\n",
    "            # efficient attention using Flash Attention CUDA kernels\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
    "        else:\n",
    "            # manual implementation of attention\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 =  nn.LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 =  nn.LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        self.config = config\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd, bias=config.bias),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        # with weight tying when using torch.compile() some warnings get generated:\n",
    "        # \"UserWarning: functional_call was passed multiple values for tied weights.\n",
    "        # This behavior is deprecated and will be an error in future versions\"\n",
    "        # not 100% sure what this is, so far seems to be harmless. TODO investigate\n",
    "        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying\n",
    "\n",
    "        # init all weights\n",
    "        self.apply(self._init_weights)\n",
    "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "        # report number of parameters\n",
    "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "\n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        \"\"\"\n",
    "        Return the number of parameters in the model.\n",
    "        For non-embedding count (default), the position embeddings get subtracted.\n",
    "        The token embeddings would too, except due to the parameter sharing these\n",
    "        params are actually used as weights in the final layer, so we include them.\n",
    "        \"\"\"\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.transformer.wpe.weight.numel()\n",
    "        return n_params\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
    "\n",
    "        # forward the GPT model itself\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "        \n",
    "        if targets is not None:\n",
    "            # if we are given some desired targets also calculate the loss\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
    "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe53e921-718a-43d1-9cb8-c75eb22dd946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 123.59M\n",
      "Model weights saved to /Users/wangaijun/pythoncode/github/model/test_lora_model/gpt2_trained_model.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(50304, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (c_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=False)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2_model=GPT(config)\n",
    "# 保存模型权重\n",
    "model_save_path = '/Users/wangaijun/pythoncode/github/model/test_lora_model/gpt2_trained_model.pth'\n",
    "torch.save(gpt2_model.state_dict(), model_save_path)\n",
    "print(f\"Model weights saved to {model_save_path}\")\n",
    "\n",
    "gpt2_model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db75740-d087-4fab-8e79-314f2af95287",
   "metadata": {},
   "source": [
    "# 2 选择哪一层进行lora 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a806a59f-5248-4b47-b253-3760dbda8791",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "transformer\n",
      "transformer.wte\n",
      "transformer.wpe\n",
      "transformer.drop\n",
      "transformer.h\n",
      "transformer.h.0\n",
      "transformer.h.0.ln_1\n",
      "transformer.h.0.attn\n",
      "transformer.h.0.attn.c_attn\n",
      "transformer.h.0.attn.c_proj\n",
      "transformer.h.0.attn.attn_dropout\n",
      "transformer.h.0.attn.resid_dropout\n",
      "transformer.h.0.ln_2\n",
      "transformer.h.0.mlp\n",
      "transformer.h.0.mlp.c_fc\n",
      "transformer.h.0.mlp.gelu\n",
      "transformer.h.0.mlp.c_proj\n",
      "transformer.h.0.mlp.dropout\n",
      "transformer.h.1\n",
      "transformer.h.1.ln_1\n",
      "transformer.h.1.attn\n",
      "transformer.h.1.attn.c_attn\n",
      "transformer.h.1.attn.c_proj\n",
      "transformer.h.1.attn.attn_dropout\n",
      "transformer.h.1.attn.resid_dropout\n",
      "transformer.h.1.ln_2\n",
      "transformer.h.1.mlp\n",
      "transformer.h.1.mlp.c_fc\n",
      "transformer.h.1.mlp.gelu\n",
      "transformer.h.1.mlp.c_proj\n",
      "transformer.h.1.mlp.dropout\n",
      "transformer.h.2\n",
      "transformer.h.2.ln_1\n",
      "transformer.h.2.attn\n",
      "transformer.h.2.attn.c_attn\n",
      "transformer.h.2.attn.c_proj\n",
      "transformer.h.2.attn.attn_dropout\n",
      "transformer.h.2.attn.resid_dropout\n",
      "transformer.h.2.ln_2\n",
      "transformer.h.2.mlp\n",
      "transformer.h.2.mlp.c_fc\n",
      "transformer.h.2.mlp.gelu\n",
      "transformer.h.2.mlp.c_proj\n",
      "transformer.h.2.mlp.dropout\n",
      "transformer.h.3\n",
      "transformer.h.3.ln_1\n",
      "transformer.h.3.attn\n",
      "transformer.h.3.attn.c_attn\n",
      "transformer.h.3.attn.c_proj\n",
      "transformer.h.3.attn.attn_dropout\n",
      "transformer.h.3.attn.resid_dropout\n",
      "transformer.h.3.ln_2\n",
      "transformer.h.3.mlp\n",
      "transformer.h.3.mlp.c_fc\n",
      "transformer.h.3.mlp.gelu\n",
      "transformer.h.3.mlp.c_proj\n",
      "transformer.h.3.mlp.dropout\n",
      "transformer.h.4\n",
      "transformer.h.4.ln_1\n",
      "transformer.h.4.attn\n",
      "transformer.h.4.attn.c_attn\n",
      "transformer.h.4.attn.c_proj\n",
      "transformer.h.4.attn.attn_dropout\n",
      "transformer.h.4.attn.resid_dropout\n",
      "transformer.h.4.ln_2\n",
      "transformer.h.4.mlp\n",
      "transformer.h.4.mlp.c_fc\n",
      "transformer.h.4.mlp.gelu\n",
      "transformer.h.4.mlp.c_proj\n",
      "transformer.h.4.mlp.dropout\n",
      "transformer.h.5\n",
      "transformer.h.5.ln_1\n",
      "transformer.h.5.attn\n",
      "transformer.h.5.attn.c_attn\n",
      "transformer.h.5.attn.c_proj\n",
      "transformer.h.5.attn.attn_dropout\n",
      "transformer.h.5.attn.resid_dropout\n",
      "transformer.h.5.ln_2\n",
      "transformer.h.5.mlp\n",
      "transformer.h.5.mlp.c_fc\n",
      "transformer.h.5.mlp.gelu\n",
      "transformer.h.5.mlp.c_proj\n",
      "transformer.h.5.mlp.dropout\n",
      "transformer.h.6\n",
      "transformer.h.6.ln_1\n",
      "transformer.h.6.attn\n",
      "transformer.h.6.attn.c_attn\n",
      "transformer.h.6.attn.c_proj\n",
      "transformer.h.6.attn.attn_dropout\n",
      "transformer.h.6.attn.resid_dropout\n",
      "transformer.h.6.ln_2\n",
      "transformer.h.6.mlp\n",
      "transformer.h.6.mlp.c_fc\n",
      "transformer.h.6.mlp.gelu\n",
      "transformer.h.6.mlp.c_proj\n",
      "transformer.h.6.mlp.dropout\n",
      "transformer.h.7\n",
      "transformer.h.7.ln_1\n",
      "transformer.h.7.attn\n",
      "transformer.h.7.attn.c_attn\n",
      "transformer.h.7.attn.c_proj\n",
      "transformer.h.7.attn.attn_dropout\n",
      "transformer.h.7.attn.resid_dropout\n",
      "transformer.h.7.ln_2\n",
      "transformer.h.7.mlp\n",
      "transformer.h.7.mlp.c_fc\n",
      "transformer.h.7.mlp.gelu\n",
      "transformer.h.7.mlp.c_proj\n",
      "transformer.h.7.mlp.dropout\n",
      "transformer.h.8\n",
      "transformer.h.8.ln_1\n",
      "transformer.h.8.attn\n",
      "transformer.h.8.attn.c_attn\n",
      "transformer.h.8.attn.c_proj\n",
      "transformer.h.8.attn.attn_dropout\n",
      "transformer.h.8.attn.resid_dropout\n",
      "transformer.h.8.ln_2\n",
      "transformer.h.8.mlp\n",
      "transformer.h.8.mlp.c_fc\n",
      "transformer.h.8.mlp.gelu\n",
      "transformer.h.8.mlp.c_proj\n",
      "transformer.h.8.mlp.dropout\n",
      "transformer.h.9\n",
      "transformer.h.9.ln_1\n",
      "transformer.h.9.attn\n",
      "transformer.h.9.attn.c_attn\n",
      "transformer.h.9.attn.c_proj\n",
      "transformer.h.9.attn.attn_dropout\n",
      "transformer.h.9.attn.resid_dropout\n",
      "transformer.h.9.ln_2\n",
      "transformer.h.9.mlp\n",
      "transformer.h.9.mlp.c_fc\n",
      "transformer.h.9.mlp.gelu\n",
      "transformer.h.9.mlp.c_proj\n",
      "transformer.h.9.mlp.dropout\n",
      "transformer.h.10\n",
      "transformer.h.10.ln_1\n",
      "transformer.h.10.attn\n",
      "transformer.h.10.attn.c_attn\n",
      "transformer.h.10.attn.c_proj\n",
      "transformer.h.10.attn.attn_dropout\n",
      "transformer.h.10.attn.resid_dropout\n",
      "transformer.h.10.ln_2\n",
      "transformer.h.10.mlp\n",
      "transformer.h.10.mlp.c_fc\n",
      "transformer.h.10.mlp.gelu\n",
      "transformer.h.10.mlp.c_proj\n",
      "transformer.h.10.mlp.dropout\n",
      "transformer.h.11\n",
      "transformer.h.11.ln_1\n",
      "transformer.h.11.attn\n",
      "transformer.h.11.attn.c_attn\n",
      "transformer.h.11.attn.c_proj\n",
      "transformer.h.11.attn.attn_dropout\n",
      "transformer.h.11.attn.resid_dropout\n",
      "transformer.h.11.ln_2\n",
      "transformer.h.11.mlp\n",
      "transformer.h.11.mlp.c_fc\n",
      "transformer.h.11.mlp.gelu\n",
      "transformer.h.11.mlp.c_proj\n",
      "transformer.h.11.mlp.dropout\n",
      "transformer.ln_f\n",
      "lm_head\n"
     ]
    }
   ],
   "source": [
    "for name, module in gpt2_model.named_modules():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb273620-fa66-4c85-939e-fceb77313ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.h.0.attn\n",
      "transformer.h.1.attn\n",
      "transformer.h.2.attn\n",
      "transformer.h.3.attn\n",
      "transformer.h.4.attn\n",
      "transformer.h.5.attn\n",
      "transformer.h.6.attn\n",
      "transformer.h.7.attn\n",
      "transformer.h.8.attn\n",
      "transformer.h.9.attn\n",
      "transformer.h.10.attn\n",
      "transformer.h.11.attn\n"
     ]
    }
   ],
   "source": [
    "for name, module in gpt2_model.named_modules():\n",
    "    if isinstance(module, CausalSelfAttention):  # 假设你想找多头注意力机制\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32b97799-ef5b-4bb6-b1b6-0e01c829fb18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found .c_attn layer: transformer.h.0.attn.c_attn\n",
      "Weight shape of transformer.h.0.attn.c_attn: torch.Size([2304, 768])\n",
      "Found .c_attn layer: transformer.h.1.attn.c_attn\n",
      "Weight shape of transformer.h.1.attn.c_attn: torch.Size([2304, 768])\n",
      "Found .c_attn layer: transformer.h.2.attn.c_attn\n",
      "Weight shape of transformer.h.2.attn.c_attn: torch.Size([2304, 768])\n",
      "Found .c_attn layer: transformer.h.3.attn.c_attn\n",
      "Weight shape of transformer.h.3.attn.c_attn: torch.Size([2304, 768])\n",
      "Found .c_attn layer: transformer.h.4.attn.c_attn\n",
      "Weight shape of transformer.h.4.attn.c_attn: torch.Size([2304, 768])\n",
      "Found .c_attn layer: transformer.h.5.attn.c_attn\n",
      "Weight shape of transformer.h.5.attn.c_attn: torch.Size([2304, 768])\n",
      "Found .c_attn layer: transformer.h.6.attn.c_attn\n",
      "Weight shape of transformer.h.6.attn.c_attn: torch.Size([2304, 768])\n",
      "Found .c_attn layer: transformer.h.7.attn.c_attn\n",
      "Weight shape of transformer.h.7.attn.c_attn: torch.Size([2304, 768])\n",
      "Found .c_attn layer: transformer.h.8.attn.c_attn\n",
      "Weight shape of transformer.h.8.attn.c_attn: torch.Size([2304, 768])\n",
      "Found .c_attn layer: transformer.h.9.attn.c_attn\n",
      "Weight shape of transformer.h.9.attn.c_attn: torch.Size([2304, 768])\n",
      "Found .c_attn layer: transformer.h.10.attn.c_attn\n",
      "Weight shape of transformer.h.10.attn.c_attn: torch.Size([2304, 768])\n",
      "Found .c_attn layer: transformer.h.11.attn.c_attn\n",
      "Weight shape of transformer.h.11.attn.c_attn: torch.Size([2304, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "model=gpt2_model\n",
    "def find_c_attn_layers(model, prefix=\"\"):\n",
    "    c_attn_layers = []\n",
    "    for name, module in model.named_children():\n",
    "        full_name = f\"{prefix}.{name}\" if prefix else name\n",
    "        if 'c_attn' in full_name:\n",
    "            c_attn_layers.append((full_name, module))\n",
    "        # 递归地在子模块中查找.c_attn层\n",
    "        c_attn_layers.extend(find_c_attn_layers(module, full_name))\n",
    "    return c_attn_layers\n",
    "\n",
    "# 假设model是你的大模型实例\n",
    "c_attn_layers = find_c_attn_layers(model)\n",
    "\n",
    "for layer_name, layer in c_attn_layers:\n",
    "    print(f\"Found .c_attn layer: {layer_name}\")\n",
    "    # 获取权重和偏置\n",
    "    weight = layer.weight.data\n",
    "    bias = layer.bias.data if hasattr(layer, 'bias') and layer.bias is not None else None\n",
    "    # 打印或修改权重和偏置\n",
    "    print(f\"Weight shape of {layer_name}: {weight.shape}\")\n",
    "    if bias is not None:\n",
    "        print(f\"Bias shape of {layer_name}: {bias.shape}\")\n",
    "    \n",
    "    # 这里可以对权重和偏置进行操作，例如：\n",
    "    # layer.weight.data = new_weight\n",
    "    # layer.bias.data = new_bias if new_bias is not None else layer.bias.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a7b380-bb09-4c78-b250-2de25a5d1a4f",
   "metadata": {},
   "source": [
    "# 3 改造 attention ,注入 lora "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be3f6d4e-0655-46c1-9601-46048f29520e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class CausalSelfAttentionLora(CausalSelfAttention):\n",
    "    def __init__(self, config, r=4, alpha=4.0):\n",
    "        super().__init__(config)\n",
    "        self.r = r\n",
    "        self.alpha = alpha\n",
    "        self.n_embd = config.n_embd\n",
    "        hidden_size = 3 * self.n_embd  # Q, K, V are concatenated\n",
    "        # 假设self.c_attn是你要进行LoRA的线性层\n",
    "        self.c_attn_original = self.c_attn  # 保存原始的c_attn层\n",
    "        \n",
    "        # 创建新的A和B矩阵用于LoRA\n",
    "        # 只为Q和V创建额外的参数，不包括K\n",
    "        self.q_adapter_a = nn.Linear(self.n_embd, r, bias=False)\n",
    "        self.v_adapter_a = nn.Linear(self.n_embd, r, bias=False)\n",
    "        self.q_adapter_b = nn.Linear(r, self.n_embd, bias=False)\n",
    "        self.v_adapter_b = nn.Linear(r, self.n_embd, bias=False)\n",
    "\n",
    "        # 初始化新增的权重\n",
    "        nn.init.zeros_(self.q_adapter_a.weight)\n",
    "        nn.init.zeros_(self.v_adapter_a.weight)\n",
    "        nn.init.zeros_(self.q_adapter_b.weight)\n",
    "        nn.init.zeros_(self.v_adapter_b.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "        # 获取父类的输出\n",
    "        qkv_output = self.c_attn_original(x)  # (batch, seq_len, 3 * n_embd)\n",
    "        # 分割qkv_output以获取Q, K, V\n",
    "        q, k, v = qkv_output.split(self.n_embd, dim=2)\n",
    "        # 应用LoRA到Q和V\n",
    "        lora_q = self.q_adapter_b(self.q_adapter_a(x))\n",
    "        lora_v = self.v_adapter_b(self.v_adapter_a(x))\n",
    "        # 将LoRA的输出添加到原始Q和V上\n",
    "        q = q + self.alpha / self.r * lora_q\n",
    "        v = v + self.alpha / self.r * lora_v\n",
    "\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        if self.flash:\n",
    "            # efficient attention using Flash Attention CUDA kernels\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
    "        else:\n",
    "            # manual implementation of attention\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "\n",
    "        return qkv_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af7d526d-790c-4a0b-b5dd-665226ff34c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "class config:\n",
    "    block_size=1024\n",
    "    vocab_size=50304\n",
    "    n_layer=4\n",
    "    n_head=8\n",
    "    n_embd=768\n",
    "    dropout=0.0\n",
    "    bias=False\n",
    "    \n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.gelu    = nn.GELU()\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class CausalSelfAttentionLora(CausalSelfAttention):\n",
    "    def __init__(self, config, r=4, alpha=4.0):\n",
    "        super().__init__(config)\n",
    "        self.r = r\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        self.n_embd = config.n_embd\n",
    "        hidden_size = 3 * self.n_embd  # Q, K, V are concatenated\n",
    "\n",
    "        # 假设self.c_attn是你要进行LoRA的线性层\n",
    "        self.c_attn_original = self.c_attn  # 保存原始的c_attn层\n",
    "        \n",
    "        # 创建新的A和B矩阵用于LoRA\n",
    "        # 只为Q和V创建额外的参数，不包括K\n",
    "        self.q_adapter_a = nn.Linear(self.n_embd, r, bias=False)\n",
    "        self.v_adapter_a = nn.Linear(self.n_embd, r, bias=False)\n",
    "        self.q_adapter_b = nn.Linear(r, self.n_embd, bias=False)\n",
    "        self.v_adapter_b = nn.Linear(r, self.n_embd, bias=False)\n",
    "\n",
    "        # 初始化新增的权重\n",
    "        nn.init.normal_(self.q_adapter_a.weight)\n",
    "        nn.init.normal_(self.v_adapter_a.weight)\n",
    "        nn.init.zeros_(self.q_adapter_b.weight)\n",
    "        nn.init.zeros_(self.v_adapter_b.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "        # 获取父类的输出\n",
    "        qkv_output = self.c_attn_original(x)  # (batch, seq_len, 3 * n_embd)\n",
    "        # 分割qkv_output以获取Q, K, V\n",
    "        q, k, v = qkv_output.split(self.n_embd, dim=2)\n",
    "        # 应用LoRA到Q和V\n",
    "        lora_q = self.q_adapter_b(self.q_adapter_a(x))\n",
    "        lora_v = self.v_adapter_b(self.v_adapter_a(x))\n",
    "        # 将LoRA的输出添加到原始Q和V上\n",
    "        q = q + self.alpha / self.r * lora_q\n",
    "        v = v + self.alpha / self.r * lora_v\n",
    "\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        if self.flash:\n",
    "            # efficient attention using Flash Attention CUDA kernels\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
    "        else:\n",
    "            # manual implementation of attention\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "       \n",
    "        return y\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 =  nn.LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn = CausalSelfAttentionLora(config)\n",
    "        self.ln_2 =  nn.LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "       \n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        self.config = config\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd, bias=config.bias),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        # with weight tying when using torch.compile() some warnings get generated:\n",
    "        # \"UserWarning: functional_call was passed multiple values for tied weights.\n",
    "        # This behavior is deprecated and will be an error in future versions\"\n",
    "        # not 100% sure what this is, so far seems to be harmless. TODO investigate\n",
    "        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying\n",
    "\n",
    "        # init all weights\n",
    "        self.apply(self._init_weights)\n",
    "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "        # report number of parameters\n",
    "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "\n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        \"\"\"\n",
    "        Return the number of parameters in the model.\n",
    "        For non-embedding count (default), the position embeddings get subtracted.\n",
    "        The token embeddings would too, except due to the parameter sharing these\n",
    "        params are actually used as weights in the final layer, so we include them.\n",
    "        \"\"\"\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.transformer.wpe.weight.numel()\n",
    "        return n_params\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
    "\n",
    "        # forward the GPT model itself\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "        \n",
    "        if targets is not None:\n",
    "            # if we are given some desired targets also calculate the loss\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
    "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03e1b379-3818-4256-a8f1-2c5051c3709f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 67.00M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(50304, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-3): 4 x Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttentionLora(\n",
       "          (c_attn): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (c_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (c_attn_original): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (q_adapter_a): Linear(in_features=768, out_features=4, bias=False)\n",
       "          (v_adapter_a): Linear(in_features=768, out_features=4, bias=False)\n",
       "          (q_adapter_b): Linear(in_features=4, out_features=768, bias=False)\n",
       "          (v_adapter_b): Linear(in_features=4, out_features=768, bias=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=False)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_model=GPT(config)\n",
    "lora_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb09bfd-96fe-479f-8d73-ad2e930203f8",
   "metadata": {},
   "source": [
    "# 4 冻结原始模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28389e21-b13a-4717-be54-32cf149a49da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(50304, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-3): 4 x Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttentionLora(\n",
       "          (c_attn): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (c_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (c_attn_original): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (q_adapter_a): Linear(in_features=768, out_features=4, bias=False)\n",
       "          (v_adapter_a): Linear(in_features=768, out_features=4, bias=False)\n",
       "          (q_adapter_b): Linear(in_features=4, out_features=768, bias=False)\n",
       "          (v_adapter_b): Linear(in_features=4, out_features=768, bias=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=False)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def freeze_model(model):\n",
    "    \"\"\"Freeze all layers except LoRA adapters.\"\"\"\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'adapter' not in name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "# Load pre-trained weights\n",
    "pretrained_state_dict = torch.load(model_save_path)\n",
    "lora_model.load_state_dict(pretrained_state_dict, strict=False)\n",
    "# Freeze all layers except LoRA adapters\n",
    "freeze_model(lora_model)\n",
    "lora_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c159b79-7b9c-41a3-a079-6375b976930d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 49152 out of 67787520 total parameters. 0.07250892199626126%\n"
     ]
    }
   ],
   "source": [
    "# Print the number of trainable parameters after applying LoRA\n",
    "trainable_params = sum(p.numel() for p in lora_model.parameters() if p.requires_grad)\n",
    "all_params = sum(p.numel() for p in lora_model.parameters())\n",
    "print(f\"Trainable parameters: {trainable_params} out of {all_params} total parameters. {trainable_params/all_params*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce65f409-f2cf-462b-a398-78ce40d35c43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trainable Parameters:\n",
      "transformer.h.0.attn.q_adapter_a.weight requires_grad: True\n",
      "transformer.h.0.attn.v_adapter_a.weight requires_grad: True\n",
      "transformer.h.0.attn.q_adapter_b.weight requires_grad: True\n",
      "transformer.h.0.attn.v_adapter_b.weight requires_grad: True\n",
      "transformer.h.1.attn.q_adapter_a.weight requires_grad: True\n",
      "transformer.h.1.attn.v_adapter_a.weight requires_grad: True\n",
      "transformer.h.1.attn.q_adapter_b.weight requires_grad: True\n",
      "transformer.h.1.attn.v_adapter_b.weight requires_grad: True\n",
      "transformer.h.2.attn.q_adapter_a.weight requires_grad: True\n",
      "transformer.h.2.attn.v_adapter_a.weight requires_grad: True\n",
      "transformer.h.2.attn.q_adapter_b.weight requires_grad: True\n",
      "transformer.h.2.attn.v_adapter_b.weight requires_grad: True\n",
      "transformer.h.3.attn.q_adapter_a.weight requires_grad: True\n",
      "transformer.h.3.attn.v_adapter_a.weight requires_grad: True\n",
      "transformer.h.3.attn.q_adapter_b.weight requires_grad: True\n",
      "transformer.h.3.attn.v_adapter_b.weight requires_grad: True\n"
     ]
    }
   ],
   "source": [
    "# Verify that only LoRA adapters are trainable\n",
    "print(\"\\nTrainable Parameters:\")\n",
    "for name, param in lora_model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name} requires_grad: {param.requires_grad}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "623ba850-ccf5-4785-b65d-4dd4ff805330",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Frozen Parameters:\n",
      "transformer.wte.weight requires_grad: False\n",
      "transformer.wpe.weight requires_grad: False\n",
      "transformer.h.0.ln_1.weight requires_grad: False\n",
      "transformer.h.0.attn.c_attn.weight requires_grad: False\n",
      "transformer.h.0.attn.c_proj.weight requires_grad: False\n",
      "transformer.h.0.ln_2.weight requires_grad: False\n",
      "transformer.h.0.mlp.c_fc.weight requires_grad: False\n",
      "transformer.h.0.mlp.c_proj.weight requires_grad: False\n",
      "transformer.h.1.ln_1.weight requires_grad: False\n",
      "transformer.h.1.attn.c_attn.weight requires_grad: False\n",
      "transformer.h.1.attn.c_proj.weight requires_grad: False\n",
      "transformer.h.1.ln_2.weight requires_grad: False\n",
      "transformer.h.1.mlp.c_fc.weight requires_grad: False\n",
      "transformer.h.1.mlp.c_proj.weight requires_grad: False\n",
      "transformer.h.2.ln_1.weight requires_grad: False\n",
      "transformer.h.2.attn.c_attn.weight requires_grad: False\n",
      "transformer.h.2.attn.c_proj.weight requires_grad: False\n",
      "transformer.h.2.ln_2.weight requires_grad: False\n",
      "transformer.h.2.mlp.c_fc.weight requires_grad: False\n",
      "transformer.h.2.mlp.c_proj.weight requires_grad: False\n",
      "transformer.h.3.ln_1.weight requires_grad: False\n",
      "transformer.h.3.attn.c_attn.weight requires_grad: False\n",
      "transformer.h.3.attn.c_proj.weight requires_grad: False\n",
      "transformer.h.3.ln_2.weight requires_grad: False\n",
      "transformer.h.3.mlp.c_fc.weight requires_grad: False\n",
      "transformer.h.3.mlp.c_proj.weight requires_grad: False\n",
      "transformer.ln_f.weight requires_grad: False\n"
     ]
    }
   ],
   "source": [
    "# Verify that other parameters are frozen\n",
    "print(\"\\nFrozen Parameters:\")\n",
    "for name, param in lora_model.named_parameters():\n",
    "    if not param.requires_grad:\n",
    "        print(f\"{name} requires_grad: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe4c438-dd03-4572-b367-3de5a8cbaf66",
   "metadata": {},
   "source": [
    "# 5 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42df78fb-53b3-4dab-a400-f4376cc657be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zstandard as zstd\n",
    "import json\n",
    "import io\n",
    "import os\n",
    "import glob\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import tiktoken\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset, Dataset, IterableDataset\n",
    "from typing import Any, Optional, Tuple\n",
    "\n",
    "# 定义一个生成器函数来读取 .jsonl.zst 文件\n",
    "def read_jsonl_zst(file_path):\n",
    "    with open(file_path, 'rb') as fh:\n",
    "        dctx = zstd.ZstdDecompressor()\n",
    "        stream_reader = dctx.stream_reader(fh)\n",
    "        text_stream = io.TextIOWrapper(stream_reader, encoding='utf-8')\n",
    "        for line in text_stream:\n",
    "            yield json.loads(line)\n",
    "\n",
    "# 定义一个生成器函数来读取所有 .jsonl.zst 文件\n",
    "def read_all_jsonl_zst(files):\n",
    "    for file_path in files:\n",
    "        yield from read_jsonl_zst(file_path)\n",
    "\n",
    "\n",
    "def process(example):\n",
    "    ids = enc.encode_ordinary(example['text'])\n",
    "    ids.append(enc.eot_token)\n",
    "    return {'ids': ids, 'len': len(ids)}\n",
    "\n",
    "class StreamingParquetDataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, jsonl_zst_files, split, block_size, num_proc=14):\n",
    "        self.data_files = jsonl_zst_files\n",
    "        self.split = split\n",
    "        self.block_size = block_size\n",
    "        self.num_proc = num_proc\n",
    "        self.dataset = IterableDataset.from_generator( lambda: read_all_jsonl_zst(jsonl_zst_files))\n",
    "#              load_dataset(\"arrow\", data_files={split: data_files}, streaming=True)\n",
    "        self.tokenized = self.dataset.map(process)\n",
    "    def __iter__(self):\n",
    "        for example in self.tokenized:\n",
    "            ids = example['ids']\n",
    "            for i in range(0, len(ids) - self.block_size, self.block_size):\n",
    "                x = torch.tensor(ids[i:i + self.block_size], dtype=torch.int64)\n",
    "                y = torch.tensor(ids[i + 1:i + 1 + self.block_size], dtype=torch.int64)\n",
    "                yield x, y\n",
    "\n",
    "# 示例函数：获取一个批次的数据\n",
    "def get_batch(loader, device,device_type):\n",
    "    for x, y in loader:\n",
    "        if device_type == 'cuda':\n",
    "            x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "        else:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "        yield x, y\n",
    "        \n",
    "def get_train_data_from_stream_data(data_path_root,enc,batch_size=32,block_size=128):\n",
    "    block_size = block_size  # 根据你的模型设置合适的块大小\n",
    "    batch_size = batch_size  # 根据你的硬件设置合适的批次大小\n",
    " \n",
    "    # 查找所有 .jsonl.zst 文件\n",
    "    jsonl_zst_files = glob.glob(f'{data_path_root}/*.jsonl.zst', recursive=True)\n",
    "\n",
    "    # 创建数据集\n",
    "    train_dataset = StreamingParquetDataset(jsonl_zst_files[:-1], 'train', block_size)\n",
    "    val_dataset = StreamingParquetDataset([jsonl_zst_files[-1]], 'val', block_size)\n",
    "\n",
    "    # 创建数据加载器\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "    return train_loader,val_loader\n",
    "    \n",
    "device_type = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.device(device_type)\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "data_path_root = \"/Users/wangaijun/pythoncode/github/data/text/chinese-c4\"\n",
    "batch_size=5\n",
    "max_seq_len=512\n",
    "train_loader,val_loader=get_train_data_from_stream_data(data_path_root,enc,batch_size=batch_size,block_size=max_seq_len)\n",
    "\n",
    "# adamw optimizer\n",
    "learning_rate = 6e-4 # max learning rate\n",
    "max_iters = 600000 # total number of training iterations\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.95\n",
    "grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n",
    "optimizer = torch.optim.AdamW(lora_model.parameters(), lr=learning_rate, betas=(beta1,beta2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4688e2c2-7241-43ef-99ce-eef44911f947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 512]) torch.Size([5, 512])\n",
      "embedding weight   step 0 tensor([[-0.0147,  0.0138,  0.0063, -0.0032,  0.0002],\n",
      "        [-0.0081, -0.0119, -0.0373,  0.0124,  0.0242],\n",
      "        [ 0.0131, -0.0033,  0.0047, -0.0069, -0.0034]])\n",
      "q_adapter_a weight step 0 tensor([[ 0.0104, -0.0040,  0.0067, -0.0158,  0.0230],\n",
      "        [ 0.0229, -0.0011, -0.0151,  0.0403,  0.0193],\n",
      "        [-0.0211,  0.0218, -0.0109, -0.0043, -0.0091]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "torch.Size([5, 512]) torch.Size([5, 512])\n",
      "embedding weight   step 1 tensor([[-0.0147,  0.0138,  0.0063, -0.0032,  0.0002],\n",
      "        [-0.0081, -0.0119, -0.0373,  0.0124,  0.0242],\n",
      "        [ 0.0131, -0.0033,  0.0047, -0.0069, -0.0034]])\n",
      "q_adapter_a weight step 1 tensor([[ 0.0098, -0.0034,  0.0061, -0.0164,  0.0224],\n",
      "        [ 0.0235, -0.0005, -0.0145,  0.0397,  0.0187],\n",
      "        [-0.0205,  0.0224, -0.0103, -0.0037, -0.0085]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "torch.Size([5, 512]) torch.Size([5, 512])\n",
      "embedding weight   step 2 tensor([[-0.0147,  0.0138,  0.0063, -0.0032,  0.0002],\n",
      "        [-0.0081, -0.0119, -0.0373,  0.0124,  0.0242],\n",
      "        [ 0.0131, -0.0033,  0.0047, -0.0069, -0.0034]])\n",
      "q_adapter_a weight step 2 tensor([[ 9.1567e-03, -3.1191e-03,  5.5553e-03, -1.6962e-02,  2.1847e-02],\n",
      "        [ 2.3996e-02,  3.5828e-05, -1.3952e-02,  3.9254e-02,  1.8083e-02],\n",
      "        [-2.0172e-02,  2.2861e-02, -9.7287e-03, -3.4969e-03, -7.9015e-03]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "torch.Size([5, 512]) torch.Size([5, 512])\n",
      "embedding weight   step 3 tensor([[-0.0147,  0.0138,  0.0063, -0.0032,  0.0002],\n",
      "        [-0.0081, -0.0119, -0.0373,  0.0124,  0.0242],\n",
      "        [ 0.0131, -0.0033,  0.0047, -0.0069, -0.0034]])\n",
      "q_adapter_a weight step 3 tensor([[ 0.0086, -0.0034,  0.0054, -0.0174,  0.0215],\n",
      "        [ 0.0246,  0.0006, -0.0134,  0.0387,  0.0175],\n",
      "        [-0.0197,  0.0233, -0.0092, -0.0035, -0.0073]],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "\n",
    "for x, y in get_batch(train_loader, device,\"cpu\"):\n",
    "    optimizer.zero_grad()  # 清零梯度\n",
    "    # 在这里进行模型训练\n",
    "    print(x.shape, y.shape)\n",
    "    print(f\"embedding weight   step {i}\",lora_model.transformer.wte.weight[:3,:5])\n",
    "    print(f\"q_adapter_a weight step {i}\",lora_model.transformer.h[0].attn.q_adapter_a.weight[:3,:5])\n",
    "    logits, loss = lora_model(x, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i==3:\n",
    "        break  # 只打印一个批次的数据\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d132ce8-d49d-4b06-b091-97fb43bca8b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b674155-43a9-438d-9999-24c3f617fab9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdde983f-9c4a-4c69-933e-00c2126bceb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "torch2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
