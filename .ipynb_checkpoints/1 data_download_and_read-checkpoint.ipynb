{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cafab497-ab2c-4ac1-8bc8-ee1fc82a7512",
   "metadata": {},
   "source": [
    "# 数据集获取及读取"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b28660-1c5f-469d-b889-7afab0e62b4a",
   "metadata": {},
   "source": [
    "# 参考资料：\n",
    "# 1 https://hf-mirror.com/datasets\n",
    "# 2 https://modelscope.cn/datasets\n",
    "# 3 https://github.com/fighting41love/funNLP?tab=readme-ov-file#%E8%AF%AD%E6%96%99%E5%BA%93"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379d1a47-c0f6-4c37-8157-cc8f165c35fb",
   "metadata": {},
   "source": [
    "# 1 中文诗词数据集\n",
    "# git clone https://www.modelscope.cn/datasets/modelscope/chinese-poetry-collection.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7179a96c-8767-4c45-a614-7b5352677d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text1'],\n",
      "        num_rows: 388599\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text1'],\n",
      "        num_rows: 1710\n",
      "    })\n",
      "})\n",
      "{'text1': ['半生长以客为家，罢直初来瀚海槎。始信人间行不尽，天涯更复有天涯。', '南州未识异州苹，初向沙头问水神。料得行藏无用卜，乘桴人是北来人。', '商船夜说指江西，欲托音书未忍题。收拾乡心都在纸，两声杜宇傍人啼。', '自出琼州古郭门，更无平衍似中原。重重叶暗桄桹雨，知是黎人第几村？', '恐倾南海成秋雨，急唤西风作晚凉。净扫黎山须见骨，莫令楚客屡回肠。']}\n",
      "{'text1': ['云髻高梳鬓不分，扫除虚室事元君。新糊白纸屏风上，尽画蓬莱五色云。', '山色摇光入袖凉，松阴十丈印回廊。老僧读罢楞严咒，一殿神风柏子香。', '经月愁闻雨，新年苦忆君。青华为客久，白发著书勤。酒共邻僧饮，蔬从野老分。何时共登眺，整屐待晴云。', '石势虎蹲伏，山形龙屈盘。寺开唐殿阁，坟掩宋衣冠。幽涧泉声细，斜阳塔影寒。近城多战鼓，栖息此中安。', '相见谈经史，江楼坐夜阑。风声吹屋响，灯影照人寒。俗薄交游尽，时危出处难。衰年逢二妙，亦得闷怀宽。']}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# 定义数据集的路径\n",
    "data_path_root=\"/Users/wangaijun/pythoncode/github/data/text\"\n",
    "data_files = {\n",
    "    'train': f'{data_path_root}/chinese-poetry-collection/train.csv',\n",
    "    'test': f'{data_path_root}/chinese-poetry-collection/test.csv'\n",
    "}\n",
    "\n",
    "# 加载数据集\n",
    "dataset = load_dataset('csv', data_files=data_files)\n",
    "\n",
    "# 打印数据集信息\n",
    "print(dataset)\n",
    "\n",
    "# 查看训练集的前几条数据\n",
    "print(dataset['train'][:5])\n",
    "\n",
    "# 查看测试集的前几条数据\n",
    "print(dataset['test'][:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1341c161-1938-4cd6-bd3d-193242043be4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daddd490-2895-494f-852d-959ce5201bcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64e0fbb1-2c58-40d8-9a5e-58e8fed8e3d9",
   "metadata": {},
   "source": [
    "# 2 chinese-c4\n",
    "## git clone https://www.modelscope.cn/datasets/swift/chinese-c4.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7296649-5b66-48f2-9ede-42d3ea16a1a4",
   "metadata": {},
   "source": [
    " Introduction\n",
    "Chinese-C4 is a clean Chinese internet dataset based on Common Crawl. The dataset is 46.29GB and has undergone multiple cleaning strategies, including Chinese filtering, heuristic cleaning based on punctuation, line-based hashing for deduplication, and repetition removal.\n",
    "\n",
    "The dataset is open source and free for commercial use, and you are welcome to use the data and the cleaning strategies provided and contribute your cleaning strategies.\n",
    "\n",
    "You can find the cleaning script for the dataset on GitHub c4-dataset-script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a655b948-30cb-4227-8e7d-ef96c67bf838",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'url': ['http://sports.sina.com.cn/c/2011-01-05/08555394624.shtml', 'http://www.wg39.com/article_detail/2932', 'http://www.hwjyw.com/zhwh/qywh/kjwh/renwu/200709/t20070925_7620.shtml'], 'timestamp': ['2022-12-08T23:26:29Z', '2022-12-08T01:55:52Z', '2022-12-01T18:11:24Z'], 'content_language': ['zho,eng', 'zho', 'zho'], 'content_type': ['text/plain', 'text/plain', 'text/plain'], 'text': ['尽管淡出中国足坛好多年，但“龙哥”还是个在亚足联甚至国际足联都举足轻重的人物，他现在头上那个“中国足协副主席”的头衔在国内属于前朝遗物，他目前是体育总局对外体育交流中心主任、党委副书记，但亚足联第一副主席这个顶戴却是货真价实的，此番前往西亚，张吉龙是想更上一层楼，拿下国际足联执委的位置。\\n本次竞选地被安排在多哈喜来登大酒店，在通过一系列决议后，竞选将从当地时间6日下午开始。竞选顺序依次为国际足联副主席竞选、国际足联执委竞选、亚足联副主席竞选及亚足联其他要职的换届选举，亚足联每个会员协会拥有一张选票，竞选以简单多数为原则，即得票多者获胜，得票无需过半数。\\n根据国际足联和亚足联规定，参加国际足联副主席竞选的韩国人郑梦准、约旦阿里侯赛因王子两人之间的获胜者将自动获得下一届国际足联执委资格。按照亚足联以往竞选惯例，东亚区竞选人一旦竞得国际足联副主席之位，那么该地区在接下来的国际足联执委竞选中很可能陷入不利境地。毕竟在亚足联东西亚势力博弈的背景下，在亚足联45个正式会员协会中占据绝对比重的西亚足球不希望东亚兼得国际足联副主席和执委两大要职。而中国首轮若投给郑梦准，那么张吉龙在竞选国际足联执委时很可能将失去有利地位；若投给侯赛因王子，那么对东亚足球的整体利益也会造成间接伤害。\\n足协一位相关人士表示：“韦主任把票投给谁，代表团还要商议，他投票原则是确保中国足球利益最大化，就像去年5月中国足协将票意外投给哈曼一样。”而韦迪提前3天抵达多哈，其目的就是与亚足联和各会员协会进行游说公关，确保张吉龙能够成功当选国际足联执委。\\n张吉龙在阎世铎当政后期逐渐在中国足协“失宠”，从而形成一个奇特的现象――这位在国际足坛最负盛名的中国足球官员，却被调离了足管中心，虽然在2008年奥运会后确认为正厅级足管中心主任，但却是为他调往对外交流中心做铺垫，所以他现在头上所戴的足协副主席帽子，仅仅是因为足协已经很长时间没有改选的原因。而且，他在亚足联的位置，一直都被足管中心一些领导觊觎着。这一次韦迪亲自助阵张吉龙竞选，应该是一次明智的选择，因为国内无人可以匹敌张吉龙的人气，而张吉龙若顺利当选国际足联执委，对于韦迪的“世界杯申办”有益无害。', '随着社会的进步，文化的发展，很多人开始发明出新的技术，注册自己的专利权。但是注册了专利权之后，也需要注意保护期限。专利保护期限是多久？\\n1、发明：发明是指对产品、方法或者其改进所提出的新的技术方案。法定保护期为20年。\\n2、实用新型：实用新型是指对产品的形状、构造或者其结合所提出的适于实用的新的技术方案。法定保护期为10年。\\n3、外观设计：外观设计是指对产品的形状、图案或其结合以及色彩与形状、图案的结合所作出的富有美感并适于工业应用的新设计。法定保护期为10年。\\n以上介绍专利使用年限，需要说明的是三种专利使用期限均从申请日起计算，并且到期不能续展。\\n根据《中华人民共和国专利法》第四十四条规定：有下列情形之一的，专利权在期限届满前终止：\\n二、专利年限的规定有何意义？\\n专利的年限的规定对专利权的提倡与制约，一方面体现专利的保护作用，另一方面不可能让一项专利永远享受长期的专利权保护，因此专利权有他的年限规定；专利一旦到期，国家知识产权局就会对该专利进行社会公告，是全社会的人都能够享有该专利技术的使用权利。\\n三、专利权人如何变更？\\n（一）填写“著录项目变更申报书”，同时提供著录项目变更证明材料。\\n１、申请人或者专利权人因权利归属纠纷发生权利转移以及发明人因资格纠纷发生变更的，如果纠纷是通过协商解决的，应当提交全体当事人签名或盖章的权利转移协议书；如果纠纷是由人民法院判决确定的，应当提交发生法律效力的人民法院的判决书，专利局收到判决书后，应当通知其他当事人，查询是否提起上诉，在指定的期限（两个月）内未答复或明确未上诉的，判决书发生法律效力；提起上诉的，当事人应当出具上诉受理通知书，原人民法院判决书不发生法律效力。如果纠纷是由地方知识产权局（或相应职能部门）调处决定的，专利局收到调处决定后，应当通知其他当事人，查询是否向法院提起诉讼；在指定期限（两个月）内未答复或明确未起诉的，调处决定发生法律效力；提起诉讼的，当事人应出具法院受理通知书，原调处决定不发生法律效力。\\n２、专利申请人或专利权人因权利的转让或赠予发生权利转移，要求变更专利申请人或专利权人的，必须提交转让或赠予合同的原件或经公证的复印件；该合同是由法人订立的，必须由法定代表人或者授权的人在合同上签名或盖章，并加盖法人的公章或者合同专用章；必要时须提交公证文件。公民订立合同的，由本人签名或者盖章；必要时须提交公证文件。有多个专利申请人或专利权人的，应提交全体权利人同意转让或赠予的证明材料。\\n（二）涉及境外居民或法人的专利申请权或专利权的转让，应当符合下列规定：\\n（４）上述专利申请权或专利权转让的著录项目变更手续，必须由转让方的申请人或专利权人或者其委托的专利代理机构办理。\\n上述（１）－（３）中的境外居民或法人是指在中国大陆没有经常居所或营业所的外国人、外国企业，港、澳地区及台湾的居民或法人；在中国大陆有经常居所或营业所的，可按中国居民或法人专利申请权和专利权转让的规定办理。\\n下一篇： 如何查询专利申请号？', '中国伟大的无产阶级革命家、政治家、军事家，中华人民共和国元帅、中国人民解放军的缔造者之一。1897年4月28日生于广东省梅县雁洋堡。少年时在丙村三堡学堂和梅县东山中学读书，受到辛亥革命影响，立志报效国家。1916年随父赴南洋。翌年回国，入云南讲武堂学习，毕业后追随孙中山先生，投身于民主革命。1920年夏，叶剑英参加了孙中山组织的驱逐桂系军阀之役。翌年10月，随大总统孙中山出巡广西。1922年6月，军阀陈炯明叛变。任海军陆战队营长的叶剑英率部护卫孙中山脱险，同叛军英勇作战。嗣后，前往福建任东路讨贼军第八旅参谋长，随军入粤讨伐陈炯明。1924年初，叶剑英任建国粤军第二师参谋长。受廖仲恺先生邀请，参加创建黄埔陆军军官学校，任教授部副主任，是当时很有威望的教官之一。这时，他接受了马克思列宁主义并要求加入中国共产党。\\n1925年，叶剑英参加讨伐陈炯明的两次东征。英勇善战，指挥果断，显露了出众的军事才能。1926年7月，参加北伐战争。初期任国民革命军第一军总预备队指挥部参谋长。攻克南昌后，任国民革命军新编第二师师长。1927年4月12日，蒋介石发动反革命政变。叶剑英毅然通电反蒋。随即奔赴武汉，任国民革命军第四军参谋长。7月，在严重的白色恐怖中秘密加入中国共产党。\\n1927年南昌起义前，叶剑英得知汪精卫阴谋加害叶挺、贺龙，立即冒着风险找他们商量对策，决定叶挺和贺龙指挥的部队迅速向南昌开进，使敌人阴谋没有得逞，南昌起义得以实现。起义军撤出南昌后，叶剑英极力劝阻了张发奎对起义军的追击。8月上旬，兼任第四军军官教导团团长，使这支革命武装成为广州起义的主力。他促成警卫团扩编，介绍共产党员梁秉枢当团长，使警卫团也成为广州起义的武装力量。12月11日，他与张太雷、叶挺等领导了广州起义，任工农红军副总指挥。这次起义，和南昌起义、秋收起义相连接，成为第二次国内革命战争与创立中国工农红军的伟大开端。\\n1928年冬，叶剑英赴苏联共产主义劳动大学特别班学习，1930年下半年回国。1931年初到达中央苏区，历任中央革命军事委员会委员兼总参谋部部长(即总参谋长)，红一方面军参谋长，闽赣军区、福建军区司令员等职务，参与指挥第二、三、四次反“围剿”战役。在此期间，还担任中国工农红军学校校长兼政治委员，培养了大批军政干部。\\n1934年10月，中央红军进行长征。叶剑英任军委第一纵队司令员。部队进入广西山区，他在一次敌机轰炸中负伤，带伤坚持行军、作战。1935年1月，党中央在遵义召开政治局扩大会议。在这个重大历史转折中，叶剑英坚决拥护毛泽东的正确主张。3月，调任三军团参谋长。7月，任红军前敌总指挥部参谋长。8月，党中央政治局在毛儿盖开会，决定部队分左、右两路军过草地北上甘南。率领左路军的张国焘，进行分裂党和红军的活动，拒绝执行党中央的北上方针，并企图危害党中央。叶剑英识破了张国焘的阴谋，立即报告毛泽东。党中央在巴西召开紧急会议，决定迅速率领红一方面军主力北上，终于脱离险境。叶剑英在这个危急关头保护了党中央。毛泽东后来屡次称赞这是叶剑英在关键时刻为党为革命建立的一个大功。9月中旬，叶剑英任由一、三军团改编的中国工农红军陕甘支队参谋长。中央红军到达陕北后，任红一方面军和军委参谋长。\\n1936年，抗日先锋军东渡黄河，叶剑英指挥中路军作战，钳制敌主力，有力地支援了左、右两路军的军事行动。7月，被党中央委派到安塞，进行联合东北军一致抗日的工作。9月，被派往西安，积极联络各方面的爱国力量。12月12日，张学良、杨虎城两将军发动西安事变。叶剑英协助周恩来坚决执行党中央关于和平解决西安事变的正确方针，迫使蒋介石停止内战，促成了国共两党再次合作、共同抗日的局面。\\n1937年7月，抗日战争爆发。8月，叶剑英与周恩来、朱德一起到南京参加蒋介石召开的国防会议。红军改编为国民革命军第八路军，叶剑英任参谋长。10月，任我军驻南京代表，从此，在国民党统治区积极开展抗日民族统一战线工作，协助周恩来先后营救出许多被国民党监禁的革命同志，不少人后来成为党的领导骨干。1937年至1941年，叶剑英先后任中共中央长江局委员、南方局常委，在国民党统治区宣传我党抗日主张，广泛联络国民党上层人士，并多次参与同国民党谈判。1939年2月，叶剑英参与创办国民党南岳游击干部训练班，任副教育长，讲授抗日游击战战略战术，宣传持久战思想，产生了深远影响。1940年3月，他出席蒋介石在重庆召开的全国参谋长会议，作了《作战与磨擦问题》的长篇发言，用大量事实宣传我军抗日的业绩，驳斥国民党顽固派对八路军的种种污蔑，取得了广泛同情，被誉为“舌战群儒”。这一年，出版了《叶剑英抗战言论集》。\\n1941年2月，叶剑英返回延安，任中央军委参谋长兼十八集团军参谋长。他审时度势，精心运筹，协助毛泽东、朱德指挥我军对日作战。他重视我军参谋工作建设，领导制定了一系列加强参谋工作的制度和措施。11月，兼任中央教育委员会委员、军事学院副院长。1943年6、7月间，在国民党顽固派发动第三次反共高潮时，叶剑英向党中央提出以智取胜的政治作战方案，大力开展宣传战，对打退国民党反共高潮起了重要作用。1944年6、7月间，叶剑英受党中央的委托，先后向在延安的中外记者参观团和美军观察组介绍我军在敌后各抗日民主根据地的作战情况和战绩，有力地驳斥了国民党顽固派对我军的污蔑，在国内外产生了广泛影响。1945年夏，叶剑英在党的第七次全国代表大会上被选为中央委员。\\n抗战胜利后，叶剑英多次参加同国民党的谈判斗争。1945年12月，他参加以周恩来为首的代表团，到重庆进行停战谈判，出席政治协商会议。1946年1月，叶剑英赴北平任军事调处执行部中共代表，与国民党代表、美国代表一起，调处国共军事冲突和监督双方执行停战协议。他率领我方同志进行了艰巨复杂的斗争，赢得了一系列胜利，发展了党的力量，通过各种方式同各方面的爱国民主人士联系，扩大了反蒋统一战线。1947年2月，他返回延安。3月，赴晋西北任中共中央后方委员会书记。7月至9月，出席全国土地会议，在会上作了军事问题的报告。12月，出席中央在米脂县召开的工作会议，同任弼时一起，主持了土地问题的讨论。在整个后委工作期间，他正确贯彻中央的土改政策，积极协助中央领导全国范围的解放战争，为保证党中央和毛泽东等同志转战陕北、指挥全国作战作出了贡献。1948年5月，叶剑英任华北军政大学校长兼政治委员。他主持制定正确的教育方针，培养和建立教员队伍，发扬教学民主，按照实战需要训练干部，为部队和地方输送了大批军政人才。\\n1949年初，任北平市军事管制委员会主任兼市长的叶剑英和聂荣臻、彭真一起，促成了北平和平解放，领导了对旧军队的改编和对旧北平市政府、学校、厂矿等各方面的接管工作，致力于北平的市政建设，维护社会安定，恢复发展生产，改善文化教育，为首都的建设和发展奠定了基础。4月，他参加以周恩来为首的中共代表团同以张治中为首的南京国民党政府代表团的和平谈判，达成《国内和平协定》，但被南京政府所拒绝。8月，任中共中央华南分局第一书记、广东军区司令员兼政治委员。\\n中华人民共和国成立后，1949年10月初，叶剑英和陈赓指挥广东战役，14日解放广州。他先后任广东省人民政府主席兼广州市市长，中南军政委员会副主席，华南军区司令员，中南军区代司令员，中共中央中南局代书记等职务。1949年底至1953年，他领导了华南地区的剿匪斗争、经济建设、民主改革以及广州的市政建设等各项重大工作，取得了显著成效。1950年2月，叶剑英主持制定解放海南岛战役的作战方针和战役计划。5月1日，我军解放海南岛。从1950年春开始，他在领导广东省的土地改革中，根据党中央的方针政策，同广东省的实际相结合所制定的一系列具体政策，注意保护华侨和民族工商业者的利益，历史证明是完全正确的。他还曾兼任华南垦殖局局长，亲自领导开拓了我国橡胶和热带作物的生产事业。\\n1954年10月，叶剑英回到北京。先后任中央人民政府革命军事委员会副主席，中华人民共和国国防委员会副主席，中国人民解放军武装力量监察部部长。1955年4月，任训练总监部代部长，主持全军的军事训练工作。他强调从实战需要出发，进行现代条件下的军事训练，把人民解放军建设成优良的正规化、现代化的革命军队。9月，被授予中华人民共和国元帅军衔。11月，组织并主持了辽东半岛方面军抗登陆战役中集团军海岸防御的军事演习，认真探索现代战争条件下训练和作战的经验。1956年6月，他主持召开全军院校会议，强调办好院校对部队现代化建设的重大意义。9月，出席党的第八次全国代表大会，再次当选为中央委员。\\n1956年12月，叶剑英率中国军事代表团访问缅甸。之后，他还率军事代表团访问了苏联、印度、波兰等国家，发展了我军同这些国家军队和人民之间的友谊。\\n1958年3月，叶剑英建议并受命创办军事科学院，任院长兼政治委员，主持制定了军事科学研究的正确方针、原则和方法，建设了一支科研干部队伍，培养了一批科研人才。同年，兼任高等军事学院院长。他是我军杰出的战略家、军事教育家和现代军事科学研究的开拓者。1959年9月，叶剑英任中央军委常委。1960年，任军委军事训练和军事学术研究委员会主任。他坚决贯彻党中央、毛泽东制定的积极防御的战略方针，参与领导研究国家防御作战问题；主张军事训练和军事科学研究相结合，以总结我军的经验为主，探讨在现代条件下的战争指导规律；主持制定人民解放军一系列条令条例；参与领导军事科学技术的发展工作，为把我军建设成强大的现代化正规化革命军队，为坚持和发展毛泽东军事思想，作出了多方面建树。1963年12月，叶剑英建议在全军推广郭兴福教学法，得到毛泽东和军委的赞许。之后，群众性练兵热潮迅速地在全军开展起来，取得显著成果。1965年，任第四届政协全国委员会副主席。1966年1月，任中央军委副主席。5月，任中共中央书记处书记、中央军委秘书长，主持军委日常工作。8月，在党的八届十一中全会上当选为中央政治局委员。\\n1971年7月，叶剑英受毛泽东、周恩来委托，主持接待秘密来华访问的美国总统国家安全事务助理基辛格。1972年参加接待先后来华访问的美国总统尼克松和日本国总理大臣田中角荣。1973年，又协助周恩来接待第二次来访的基辛格。他为建立中美、中日外交关系作出了重要贡献。1973年8月，在党的十届一中全会上，他当选为党中央副主席。1974年1月，叶剑英受毛泽东、周恩来委托，同邓小平一起指挥西沙自卫还击作战，收回了被侵占的岛屿。\\n1975年1月，在四届人大第一次会议上，他被任命为国防部长。6、7月间，叶剑英和邓小平一起，主持召开了具有重要历史意义的军委扩大会议。他在会上就国际形势、压缩军队定额、调整编制体制、安排超编干部等问题作了重要讲话。会议前后，他同许多高级干部谈话，向他们通气，讲了毛泽东对“四人帮”的严厉批评，揭露江青等人背着中央，插手军队的阴谋活动。在他主持下，对全军二十几个大单位的领导班子进行了调整配备。这些为后来粉碎江青反革命集团，稳定军队与全国形势，创造了重要条件。1976年，在所谓“反击右倾翻案风”运动中，叶剑英再次被中止了领导工作。1976年，周恩来、朱德、毛泽东相继逝世，江青反革命集团加紧进行篡夺党和国家最高领导权的阴谋活动。10月，在党和国家面临危险的紧急时刻，叶剑英和党中央政治局其他同志一道，根据政治局多数同志的意见，代表党和人民的意志，毅然粉碎了江青反革命集团，从危难中挽救了党。在这场关系着党和国家命运的斗争中，叶剑英起了决定性的作用。\\n1979年1月，叶剑英主持全国人大常委会通过发表了《告台湾同胞书》，指出“实现中国的统一，是人心所向，大势所趋”，并提出在海峡两岸“发展贸易，互通有无，进行经济交流”和“双方尽快实现通航通邮”的方针。1979年9月30日，叶剑英代表中共中央在庆祝中华人民共和国成立三十周年大会上发表重要讲话，论述建国三十年来的成就和失误，总结“文化大革命”的教训，批判“两个凡是”的错误，明确提出在建设高度物质文明的同时，要建设高度的社会主义精神文明，号召全国人民团结一致，向着四个现代化的宏伟目标奋勇前进。\\n1980年9月，叶剑英担任宪法修改委员会主任委员，主持制定了五届人大第五次会议通过的《中国人民共和国宪法》。他还领导制定了《刑法》、《刑事诉讼法》等二十二个法律，为健全和完善我国的法制做了大量工作。1981年9月30日，叶剑英发表了著名的《关于台湾回归祖国，实现和平统一的方针政策》的谈话，进一步提出了实现祖国统一的九项具体政策，建议举行国共两党对等谈判，实现第三次国共合作。这一重要谈话对祖国统一大业发生着愈来愈大的影响。他还经常会见回国观光、探亲的港澳同胞和海外侨胞，鼓励大家为祖国的统一和建设多作贡献。1982年9月，叶剑英在党的第十二次全国代表大会上发表重要讲话，强调开创社会主义现代化建设的新局面，必须有一大批年富力强的同志走上中央和各级领导岗位，强调中央委员会必须坚持民主集中制的领导原则。在十二届一中全会上当选为中央政治局常务委员会委员。\\n1983年2月25日，在第五届全国人民代表大会任期将满的时候，叶剑英写信给全国人大常委会，请求不再提名选他当六届全国人大代表，不再将他列为六届全国人大常委会委员长候选人。3月5日，人大常委会复信同意他的要求，热情地赞扬了他在长达半个多世纪的充满艰难险阻的革命斗争中所建立的丰功伟绩，赞扬他为社会主义建设事业和为实现祖国统一大业作出的卓越贡献。6月，在第六届全国人大第一次会议上，他被任命为中华人民共和国中央军事委员会副主席。10月，参与主持党的十二届二中全会。这一年，出版了《叶剑英诗词选集》。他在晚年，还多次与王震等老战友一起，到全国各地视察，关心和支持一线同志的工作。1985年9月，叶剑英同其他六十三位老同志一起致函党的十二届四中全会，请求不再担任中央委员，以便让一批比较年轻的德才兼备的同志进入中央委员会，进一步实现中央领导机构成员的新老交替。十二届四中全会同意他的请求，并给他写了致敬信，高度评价了他的光辉业绩。\\n1986年10月22日因病在北京逝世，享年89岁。']}\n"
     ]
    }
   ],
   "source": [
    "import zstandard as zstd\n",
    "import json\n",
    "import io\n",
    "import os\n",
    "from datasets import load_dataset, Dataset\n",
    "data_path_root=\"/Users/wangaijun/pythoncode/github/data/text\"\n",
    "\n",
    "# 查找所有 .jsonl.zst 文件\n",
    "jsonl_zst_files = glob.glob(f'{data_path_root}/chinese-c4/*.jsonl.zst', recursive=True)\n",
    "# 定义你要读取的 .jsonl.zst 文件路径\n",
    "file_path = jsonl_zst_files[0]\n",
    "\n",
    "# 定义一个函数来读取 .jsonl.zst 文件\n",
    "def read_jsonl_zst(file_path):\n",
    "    with open(file_path, 'rb') as fh:\n",
    "        dctx = zstd.ZstdDecompressor()\n",
    "        stream_reader = dctx.stream_reader(fh)\n",
    "        text_stream = io.TextIOWrapper(stream_reader, encoding='utf-8')\n",
    "        for line in text_stream:\n",
    "            yield json.loads(line)\n",
    "\n",
    "# 读取 .jsonl.zst 文件并存储为列表\n",
    "data = list(read_jsonl_zst(file_path))\n",
    "\n",
    "# 将数据转换为 Hugging Face Dataset\n",
    "dataset = Dataset.from_list(data)\n",
    "\n",
    "# 打印前几条数据以验证\n",
    "print(dataset[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a434182-f786-42c1-9e95-e7c9952d9be7",
   "metadata": {},
   "source": [
    "## 2.2 大数据集读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8a5f842a-5a60-41c4-850c-b1c6e213613c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([ 101, 2226, 5052, 3909, 1139,  704, 1744, 6639, 1781, 1962, 1914, 2399,\n",
      "        8024,  852,  100, 7987, 1520,  100, 6820, 3221,  702, 1762,  762, 6639,\n",
      "        5468, 4493, 5635, 1744, 7354, 6639, 5468, 6963,  715, 6639, 6768, 7028,\n",
      "        4638,  782, 4289, 8024,  800, 4385, 1762, 1928,  677, 6929,  702,  100,\n",
      "         704, 1744, 6639, 1291, 1199,  712, 2375,  100, 4638, 1928, 6124, 1762,\n",
      "        1744, 1079, 2247,  754, 1184, 3308, 6890, 4289, 8024,  800, 4680, 1184,\n",
      "        3221,  860, 5509, 2600, 2229, 2190, 1912,  860, 5509,  769, 3837,  704,\n",
      "        2552,  712,  818,  510, 1054, 1999, 1199,  741, 6381, 8024,  852,  762,\n",
      "        6639, 5468, 5018,  671, 1199,  712, 2375, 6821,  702, 7553, 2785, 1316,\n",
      "        3221, 6573, 4696,  817, 2141, 4638, 8024, 3634, 4528, 1184, 2518, 6205,\n",
      "         762, 8024, 2476, 1395, 7987, 3221, 2682,  102]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([ 101, 7390, 4708, 4852,  833, 4638, 6822, 3635, 8024, 3152, 1265, 4638,\n",
      "        1355, 2245, 8024, 2523, 1914,  782, 2458, 1993, 1355, 3209, 1139, 3173,\n",
      "        4638, 2825, 3318, 8024, 3800, 1085, 5632, 2346, 4638,  683, 1164, 3326,\n",
      "         511,  852, 3221, 3800, 1085,  749,  683, 1164, 3326,  722, 1400, 8024,\n",
      "         738, 7444, 6206, 3800, 2692,  924, 2844, 3309, 7361,  511,  683, 1164,\n",
      "         924, 2844, 3309, 7361, 3221, 1914,  719, 8043,  122,  510, 1355, 3209,\n",
      "        8038, 1355, 3209, 3221, 2900, 2190,  772, 1501,  510, 3175, 3791, 2772,\n",
      "        5442, 1071, 3121, 6822, 2792, 2990, 1139, 4638, 3173, 4638, 2825, 3318,\n",
      "        3175, 3428,  511, 3791, 2137,  924, 2844, 3309,  711, 8113, 2399,  511,\n",
      "         123,  510, 2141, 4500, 3173, 1798, 8038, 2141, 4500, 3173, 1798, 3221,\n",
      "        2900, 2190,  772, 1501, 4638, 2501, 4307,  102]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import zstandard as zstd\n",
    "import json\n",
    "import io\n",
    "from datasets import Dataset, IterableDataset, Features, Value\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "\n",
    "# 定义一个生成器函数来读取 .jsonl.zst 文件\n",
    "def read_jsonl_zst_generator(file_paths):\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, 'rb') as fh:\n",
    "            dctx = zstd.ZstdDecompressor()\n",
    "            stream_reader = dctx.stream_reader(fh)\n",
    "            text_stream = io.TextIOWrapper(stream_reader, encoding='utf-8')\n",
    "            for line in text_stream:\n",
    "                yield json.loads(line)\n",
    "\n",
    "# 创建一个可迭代的数据集\n",
    "features = Features({\n",
    "    'text': Value('string'),  # 假设你的 JSON 对象有一个 'text' 字段\n",
    "    # 如果有其他字段，可以在这里添加\n",
    "})\n",
    "\n",
    "iterable_dataset = IterableDataset.from_generator(\n",
    "    lambda: read_jsonl_zst_generator(jsonl_zst_files),\n",
    "    features=features\n",
    ")\n",
    "\n",
    "# 加载预训练的 BERT 分词器\n",
    "tokenizer = BertTokenizer.from_pretrained('/Users/wangaijun/pythoncode/github/model/bert-base-chinese')\n",
    "\n",
    "# 定义一个映射函数，用于将文本转换为 token ID\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "# 应用映射函数\n",
    "# tokenized_dataset = iterable_dataset.map(tokenize_function, batched=True)\n",
    "# 应用映射函数，并自定义 batch_size 和 num_proc\n",
    "\n",
    "tokenized_dataset = iterable_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    batch_size=1000,  # 你可以根据需要调整这个值\n",
    "\n",
    ")\n",
    "\n",
    "# 创建自定义的 PyTorch 可迭代数据集\n",
    "class CustomIterableDataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, dataset):\n",
    "        super(CustomIterableDataset, self).__init__()\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __iter__(self):\n",
    "        for item in self.dataset:\n",
    "            input_ids = torch.tensor(item['input_ids'])\n",
    "            attention_mask = torch.tensor(item['attention_mask'])\n",
    "            # 如果有标签，可以在这里添加\n",
    "            yield {\n",
    "                'input_ids': input_ids,\n",
    "                'attention_mask': attention_mask,\n",
    "                # 'labels': labels (如果有标签)\n",
    "            }\n",
    "\n",
    "# 将 Hugging Face 数据集转换为 PyTorch 数据集\n",
    "pytorch_dataset = CustomIterableDataset(tokenized_dataset)\n",
    "\n",
    "# 打印前几条数据以验证\n",
    "for i, data in enumerate(pytorch_dataset):\n",
    "    if i < 2:\n",
    "        print(data)\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "42695acc-0a05-455f-b711-88db4b7518d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IterableDataset({\n",
       "    features: ['text'],\n",
       "    n_shards: 1\n",
       "})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2fd114a-d900-465a-a3d5-766c72373df0",
   "metadata": {},
   "source": [
    "# 2 openwebtext\n",
    "## git clone https://www.modelscope.cn/datasets/mapjack/openwebtextSample.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6df6b43-fc42-480c-a8f2-bdf2a6fb13d0",
   "metadata": {},
   "source": [
    " OpenAIWebText数据集是开源的。数据集大小大约100G左右，训练RWKV模型的可用数据，该数据都是从网上爬取的，并经过清洗的数据，重复度比较低的数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "814518f5-71b4-4e11-ad9e-a28b6217ff01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting num_proc from 8 to 2 for the train split as it only contains 2 shards.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b8697c35f92435798c21b42a001bfd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 119475\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 60\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from datasets import load_dataset # huggingface datasets\n",
    "\n",
    "local_data_path=\"/Users/wangaijun/pythoncode/github/model/openwebtext\"\n",
    "num_proc_load_dataset = 8\n",
    "\n",
    "# takes 54GB in huggingface .cache dir, about 8M documents (8,013,769)\n",
    "# dataset = load_dataset(\"openwebtext\", num_proc=num_proc_load_dataset)\n",
    "arow_files = [os.path.join(local_data_path, f) for f in os.listdir(local_data_path) if f.endswith('.arrow')]\n",
    "dataset = load_dataset('arrow', data_files={'train': arow_files}, num_proc=num_proc_load_dataset)\n",
    "# owt by default only contains the 'train' split, so create a test split\n",
    "split_dataset = dataset[\"train\"].train_test_split(test_size=0.0005, seed=2357, shuffle=True)\n",
    "split_dataset['val'] = split_dataset.pop('test') # rename the test split to val\n",
    "\n",
    "split_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "87fb2c21-e906-4b17-b554-8549c8d44b3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"Satnews Daily\\n\\nTeam AMRO Fabricating Proudly Presents Panels For NASA's Space Launch System\\n\\nTeams at AMRO Fabricating Corp. in South El Monte, California, show completed flight and structural test article hardware panels, arranged in order, for each section of NASA’s new rocket, the Space Launch System. SLS will be the most powerful rocket ever built and, with NASA’s Orion spacecraft, will launch America into a new era of exploration to destinations beyond Earth’s orbit. AMRO is an industry partner helping build panels for the SLS core stage, launch vehicle stage adapter (LVSA) and Orion spacecraft. The panels, above, from bottom to top, represent the SLS engine section, liquid hydrogen tank, intertank, liquid oxygen tank, forward skirt, LVSA and Orion. For more information about SLS, visit www.nasa.gov/sls. Image Credit: AMRO Teams at AMRO Fabricating Corp. in South El Monte, California, show completed flight and structural test article hardware panels, arranged in order, for each section of NASA’s new rocket, the Space Launch System. SLS will be the most powerful rocket ever built and, with NASA’s Orion spacecraft, will launch America into a new era of exploration to destinations beyond Earth’s orbit. AMRO is an industry partner helping build panels for the SLS core stage, launch vehicle stage adapter (LVSA) and Orion spacecraft. The panels, above, from bottom to top, represent the SLS engine section, liquid hydrogen tank, intertank, liquid oxygen tank, forward skirt, LVSA and Orion. For more information about SLS, visit www.nasa.gov/sls. Image Credit: AMRO\\n\\n[Satnews] In spaceflight, the first eight minutes are critical.\\n\\nThis is when the greatest opposing forces of thrust and gravity are impacting the launch vehicle. The new NASA Space Launch System (SLS) will weigh 5.5 million pounds at liftoff, or roughly the weight of eight fully loaded 747 jets. Everything comes down to weight and the integrity of design and fabrication to insure success. Today, it costs $10,000 to send one pound of payload into orbit; since the entire launch vehicle makes the trip to low-Earth orbit, its net weight is a big consideration. The lighter the launch vehicle, the greater the payload can be.\\n\\nNASA’s Space Launch System (SLS), once fully assembled, will reach 371 feet high in its crew configuration.\\n\\nAMRO Fabricating of El Monte, California has been manufacturing precision machined and roll-formed Isogrid and Orthogrid rocket body panels since 1986. The company’s greatest challenge has always been weight. According to AMRO, if a single rib is out of tolerance by as little as .002, it will translate to hundreds of pounds of added weight across the launch vehicle. At the same time, being under size will compromise the integrity of the panel. To put this into perspective, if you were to scale down the SLS to the size of an aluminum soda can, the walls of the SLS would be ten times thinner than those of the soda can.\\n\\nEvery step in the manufacturing process of rocket body panels at AMRO is guided, inspected, and reported using metrology software, laser trackers, portable coordinate measuring machines (CMMs), ultrasonic gauges, and precision check fixtures. By the time a completed panel is crated for shipment, it has been probed, scanned, and inspected more than 20 times. Each panel is 12 feet × 24 feet × 4 inches thick; hogged out Isogrid and Orthogrid geometry provides lightweight structural integrity. Following completion of the manufacturing process, the finished formed panels are assembled into rings. Eight panels are used to create one ring measuring 27.5 feet in diameter and standing 24 feet tall. Five of these assemblies are stacked to create one SLS core stage. Once fully assembled, the height of the SLS will reach 371 feet in its crew configuration.\\n\\nAMRO Quality Engineer Rodrigo Delgadillo inspects a flat, machined grid panel prior to forming using Verisurf inspection software and a Leica laser tracker with wireless T Probe. AMRO Quality Engineer Rodrigo Delgadillo inspects a flat, machined grid panel prior to forming using Verisurf inspection software and a Leica laser tracker with wireless T Probe.\\n\\nAluminum plate material, milled to size, is hogged out to an Isogrid and Orthogrid structure using one of AMRO’s gantry CNC machining centers. Precision holes are drilled for fastening points and other features. While still on the machine bed, the part is deburred and spot-checked using ultrasonic thickness gauges. Design and machining considerations to take into account the final machined part will be formed into a curved structure.\\n\\nThe finished machined part is moved to a special vacuum table where it is subject to a defined inspection routine. Using Verisurf model-based definition inspection software, a laser tracker, and Leica T-Probe, all geometry, wall thicknesses, floor radius locations, hole positions, and diameters are confirmed within tolerances based the CAD model. The Leica T-Probe is an armless, wireless, “walk-around” device that allows the laser tracker to remain in one position while probing hidden, hard-to-reach surfaces such as behind grid walls. Verisurf software provides a 3D on-screen virtual view of the part and relative position of the probe during the inspection process. Real-time graphical inspection data is displayed showing the operator each inspection target as being in tolerance, out of tolerance low, or out of tolerance high. Following inspection, the software generates an AS9102 Inspection Report for the part.\\n\\nThe Leica wireless T Probe interacts directly with the laser tracker, allowing it to remain in a fixed position. The probe allows the laser tracker to accurately measure behind grid walls and other hard-to-reach surfaces. The Leica wireless T Probe interacts directly with the laser tracker, allowing it to remain in a fixed position. The probe allows the laser tracker to accurately measure behind grid walls and other hard-to-reach surfaces.\\n\\nUsing one of five high-capacity break presses, finished machined parts are formed or rolled into specified curved profiles. The process relieves and distributes part stress inherent in the manufacturing of aluminum.\\n\\nPrecision check fixtures are used to insure the finished curved profile of the part is accurate, and overall dimensions are within tolerance. Measurement software is once again used to guide the assembly and inspection of these fixtures. Using Verisurf Reverse and model-based definition, AMRO is able to create precision build tools to match the surface profiles of the panels and support their manufacturing process. Parts are placed in check fixtures and adjusted until a perfect fit is accomplished.\\n\\nFollowing forming, the part is placed in a trim fixture designed to precision-trim the outside dimensions to tolerance. In each case, metrology software and portable CMMs are used to create the fixture tools.\\n\\nFinished parts undergo a final QC inspection using Verisurf software and a laser tracker. All dimensions, hole locations, diameters, radii, and surface profiles are inspected. A final AS9102 Inspection Report is generated.\\n\\nThis information is courtesy of NASA's Tech Briefs\"}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_dataset[\"val\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2ab297-65a3-46ee-8198-5d4aeab23b39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "torch2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
