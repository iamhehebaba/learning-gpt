{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e1efe7b-9ed1-4775-83b0-f626e08f1e8b",
   "metadata": {},
   "source": [
    "# GPT训练2-文本分词\n",
    "## https://github.com/openai/tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40605e4-2786-4c5e-81bf-f380b64e7683",
   "metadata": {},
   "source": [
    "# 常用分词方法：\n",
    "## 1 Byte Pair Encoding (BPE)：BPE是一种基于频率的统计方法，通过逐步合并最频繁出现的字符对来构建词汇表。\n",
    "## 2 Byte-Level BPE：将文本转换为字节序列，然后应用BPE算法进行子词分割。\n",
    "## 3 WordPiece：类似于BPE，但通过最大化似然性来选择子词\n",
    "## 4 SentencePiece：由Google开发，是一种无监督的文本分词方法，可以直接从原始文本中学习词汇表。\n",
    "## 5 Unigram Language Model\n",
    "## 6 Word-level Tokenization\n",
    "## 7 Character-level Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dcdbed-2b23-467f-8ef4-fe19a91c4b86",
   "metadata": {},
   "source": [
    "# 1 加载预训练模型分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1036acb8-d017-432e-994f-a4b407f092c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text1'],\n",
      "        num_rows: 388599\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text1'],\n",
      "        num_rows: 1710\n",
      "    })\n",
      "})\n",
      "{'text1': ['半生长以客为家，罢直初来瀚海槎。始信人间行不尽，天涯更复有天涯。']}\n",
      "{'text1': ['云髻高梳鬓不分，扫除虚室事元君。新糊白纸屏风上，尽画蓬莱五色云。', '山色摇光入袖凉，松阴十丈印回廊。老僧读罢楞严咒，一殿神风柏子香。']}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# 定义数据集的路径\n",
    "data_path_root=\"/Users/wangaijun/pythoncode/github/data/text\"\n",
    "data_files = {\n",
    "    'train': f'{data_path_root}/chinese-poetry-collection/train.csv',\n",
    "    'test': f'{data_path_root}/chinese-poetry-collection/test.csv'\n",
    "}\n",
    "# 加载数据集\n",
    "dataset = load_dataset('csv', data_files=data_files)\n",
    "# 打印数据集信息\n",
    "print(dataset)\n",
    "# 查看训练集的前几条数据\n",
    "print(dataset['train'][:1])\n",
    "# 查看测试集的前几条数据\n",
    "print(dataset['test'][:2])\n",
    "\n",
    "# 测试tokenizer\n",
    "text = \"举头望明月，低头思故乡。\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b959b767-a1fd-4888-962f-367801f3c5b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['举', '头', '望', '明', '月', '，', '低', '头', '思', '故', '乡', '。']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "# 加载预训练的 BERT 分词器\n",
    "tokenizer = BertTokenizer.from_pretrained('/Users/wangaijun/pythoncode/github/model/bert-base-chinese')\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d0aa7982-0548-4e1e-a458-a960ff20e271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['半生长以客为家，罢直初来瀚海槎。始信人间行不尽，天涯更复有天涯。']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 1288, 4495, 7270, 809, 2145, 711, 2157, 8024, 5387, 4684, 1159, 3341, 4108, 3862, 3542, 511, 1993, 928, 782, 7313, 6121, 679, 2226, 8024, 1921, 3889, 3291, 1908, 3300, 1921, 3889, 511, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=dataset['train'][:1][\"text1\"]\n",
    "print(data)\n",
    "tokenizer(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "42ef8600-6fde-41bb-bc28-4e42093dc2b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['举', '头', '望', '明', '月', '，', '低', '头', '思', '故', '乡', '。']\n",
      "{'input_ids': [101, 715, 1928, 3307, 3209, 3299, 8024, 856, 1928, 2590, 3125, 740, 511, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "Vocabulary size: 21128\n",
      "Return fields: dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
      "Special tokens: {'cls_token': '[CLS]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'unk_token': '[UNK]', 'mask_token': '[MASK]'}\n",
      "Maximum sequence length: 1000000000000000019884624838656\n",
      "Tokenizer configuration: {'do_lower_case': False, 'do_basic_tokenize': True, 'never_split': None, 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]', 'tokenize_chinese_chars': True, 'strip_accents': None, 'clean_up_tokenization_spaces': True, 'tokenizer_file': '/Users/wangaijun/pythoncode/github/model/bert-base-chinese/tokenizer.json', 'name_or_path': '/Users/wangaijun/pythoncode/github/model/bert-base-chinese'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "loaded_tokenizer=tokenizer\n",
    "# 验证分词器\n",
    "print(loaded_tokenizer.tokenize(text))\n",
    "print(loaded_tokenizer(text))\n",
    "# 检查词汇表大小\n",
    "print(f'Vocabulary size: {loaded_tokenizer.vocab_size}')\n",
    "\n",
    "# 查看返回字段\n",
    "\n",
    "tokenized_example = loaded_tokenizer(text, return_tensors='pt')\n",
    "print(f'Return fields: {tokenized_example.keys()}')\n",
    "\n",
    "# 查看特殊标记\n",
    "special_tokens = {\n",
    "    'cls_token': loaded_tokenizer.cls_token,\n",
    "    'sep_token': loaded_tokenizer.sep_token,\n",
    "    'pad_token': loaded_tokenizer.pad_token,\n",
    "    'unk_token': loaded_tokenizer.unk_token,\n",
    "    'mask_token': loaded_tokenizer.mask_token,\n",
    "}\n",
    "print(f'Special tokens: {special_tokens}')\n",
    "\n",
    "# 查看最大序列长度\n",
    "max_length = loaded_tokenizer.model_max_length\n",
    "print(f'Maximum sequence length: {max_length}')\n",
    "\n",
    "# 查看分词器配置\n",
    "config = loaded_tokenizer.init_kwargs\n",
    "print(f'Tokenizer configuration: {config}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce040db-51a5-4d8b-b681-8241f53c78e3",
   "metadata": {},
   "source": [
    "# 2 预训练模型添加新词//"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "618ff498-961d-4c23-ad30-6ff14b94e0f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer 长度： 21128\n",
      "tokenizer  ['举', '头', '望', '明', '月', '，', '低', '头', '思', '故', '乡', '。']\n",
      "Number of tokens added: 4\n",
      "new_tokenizer 长度： 21132\n",
      "new_tokenizer ['举', '头望', '明', '月', '，', '低', '头', '思', '故', '乡', '。']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "# 加载预训练的 BERT 分词器\n",
    "tokenizer = BertTokenizer.from_pretrained('/Users/wangaijun/pythoncode/github/model/bert-base-chinese')\n",
    "print(\"tokenizer 长度：\",len(tokenizer))\n",
    "print(\"tokenizer \",tokenizer.tokenize(text))\n",
    "# 新词列表\n",
    "new_tokens = ['新词1', '新词2', '新词3',\"头望\",'举', '头', '望', '明', '月']\n",
    "# 添加新词\n",
    "num_added_toks = tokenizer.add_tokens(new_tokens)\n",
    "print(f'Number of tokens added: {num_added_toks}')\n",
    "\n",
    "# 保存新的分词器到指定路径\n",
    "new_tokenizer_path = f'{data_path_root}/custom_tokenizer3'\n",
    "tokenizer.save_pretrained(new_tokenizer_path)\n",
    "new_tokenizer = BertTokenizer.from_pretrained(new_tokenizer_path)\n",
    "\n",
    "print(\"new_tokenizer 长度：\",len(new_tokenizer))\n",
    "print(\"new_tokenizer\",new_tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb89226-a951-4380-8da5-67d9f68b2477",
   "metadata": {},
   "source": [
    "# 3 自定义分词"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079cc722-05f0-4157-9197-3ccea588dc9b",
   "metadata": {},
   "source": [
    "## 3.1 sentencepiece模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e357cbe9-786f-42ae-b047-8d126cc4b1f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=/Users/wangaijun/pythoncode/github/data/text/chinese-poetry.txt --model_prefix=tokenizer/spm_poerty_tokenizer --vocab_size=10000  --character_coverage=1.0 --model_type=bpe --num_threads=8\n",
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /Users/wangaijun/pythoncode/github/data/text/chinese-poetry.txt\n",
      "  input_format: \n",
      "  model_prefix: tokenizer/spm_poerty_tokenizer\n",
      "  model_type: BPE\n",
      "  vocab_size: 10000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 1\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 8\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: /Users/wangaijun/pythoncode/github/data/text/chinese-poetry.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 390309 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=14800983\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=9735\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 390309 sentences.\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 390309\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 389998\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=8591 min_freq=19\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3992 size=20 all=1653173 active=107439 piece=江南\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2814 size=40 all=1674224 active=128490 piece=何人\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2369 size=60 all=1693552 active=147818 piece=归去\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2139 size=80 all=1709935 active=164201 piece=先生\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1900 size=100 all=1722412 active=176678 piece=不如\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1893 min_freq=18\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1728 size=120 all=1736759 active=99571 piece=处处\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1601 size=140 all=1750718 active=113530 piece=桃李\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1530 size=160 all=1763740 active=126552 piece=从此\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1443 size=180 all=1777856 active=140668 piece=无事\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1352 size=200 all=1790053 active=152865 piece=▁月\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1349 min_freq=17\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1296 size=220 all=1802872 active=102063 piece=此地\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1246 size=240 all=1816378 active=115569 piece=南山\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1198 size=260 all=1828323 active=127514 piece=▁寒\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: tokenizer/spm_poerty_tokenizer.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: tokenizer/spm_poerty_tokenizer.vocab\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 加载数据集\n",
    "data_path_root = \"/Users/wangaijun/pythoncode/github/data/text\"\n",
    "data_files = {\n",
    "    'train': f'{data_path_root}/chinese-poetry-collection/train.csv',\n",
    "    'test': f'{data_path_root}/chinese-poetry-collection/test.csv'\n",
    "}\n",
    "dataset = load_dataset('csv', data_files=data_files)\n",
    "\n",
    "# 准备用于训练的数据文件\n",
    "with open(f'{data_path_root}/chinese-poetry.txt', 'w', encoding='utf-8') as f:\n",
    "    for split in ['train','test']:\n",
    "        for item in dataset[split]:\n",
    "            # 假设你的CSV中有一个列叫做\"text\"包含了文本内容\n",
    "            text = item['text1']\n",
    "            f.write(text + '\\n')\n",
    "tokenizer_save_path=\"tokenizer/spm_poerty_tokenizer\"\n",
    "# 训练SentencePiece模型\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    '--input={} --model_prefix={} --vocab_size={}  --character_coverage={} --model_type=bpe --num_threads=8'.format(\n",
    "        f'{data_path_root}/chinese-poetry.txt',\n",
    "        tokenizer_save_path,\n",
    "        10000,  # 你可以根据需要调整词汇表大小\n",
    "        1.0  # 字符覆盖率，通常设置为1.0以覆盖所有字符\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37186d39-ad5a-473e-94d6-5dd584a51435",
   "metadata": {},
   "source": [
    "## 3.2 使用支持sentencepiece的分词器加载和保存分词模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "39e22f56-0931-4f32-bad4-f6923169e878",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wangaijun/miniforge3/envs/torch2/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2102: FutureWarning: Calling XLMRobertaTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/Users/wangaijun/pythoncode/github/data/text/custom_tokenizer/tokenizer_config.json',\n",
       " '/Users/wangaijun/pythoncode/github/data/text/custom_tokenizer/special_tokens_map.json',\n",
       " '/Users/wangaijun/pythoncode/github/data/text/custom_tokenizer/sentencepiece.bpe.model',\n",
       " '/Users/wangaijun/pythoncode/github/data/text/custom_tokenizer/added_tokens.json')"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import XLMRobertaTokenizer, XLMRobertaTokenizerFast\n",
    "\n",
    "# 创建一个基于SentencePiece的Tokenizer\n",
    "model_path=\"tokenizer/spm_poerty_tokenizer.model\"\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(model_path, keep_accents=True)\n",
    "# 保存tokenizer到指定目录\n",
    "output_dir = f'{data_path_root}/custom_tokenizer'\n",
    "tokenizer.save_pretrained(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2b7f9d3e-726d-46c5-bd04-355669e0fadd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer: ['▁', '举', '头', '望', '明月', ',', '低', '头', '思', '故乡', '。']\n",
      "Encoded:   [0, 268, 1023, 347, 429, 12, 267, 755, 347, 392, 187, 266, 2]\n",
      "Decoded: <s> 举头望明月,低头思故乡。</s>\n"
     ]
    }
   ],
   "source": [
    "# 加载已保存的tokenizer\n",
    "tokenizer = XLMRobertaTokenizerFast.from_pretrained(output_dir)\n",
    "# 测试tokenizer\n",
    "text = \"举头望明月，低头思故乡。\"\n",
    "encoded = tokenizer.encode(text)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(\"tokenizer:\",tokenizer.tokenize(text))\n",
    "print(\"Encoded:  \", encoded)\n",
    "print(\"Decoded:\", decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52fe1197-ff32-48f3-8044-bbf5c8bc2fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path=\"tokenizer/spm_poerty_tokenizer.model\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9885bfa7-0d94-4880-9dc4-4c49d91bcc17",
   "metadata": {},
   "source": [
    "## 2.2 使用自定义的分词器（兼容transfomer）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "bf761f42-ba89-416d-9efd-757a2685cd22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded: [267, 1022, 346, 428, 11, 266, 754, 346, 391, 186, 265]\n",
      "Decoded: 举头望明月,低头思故乡。\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizer, AddedToken\n",
    "import os\n",
    "import json\n",
    "import sentencepiece as spm\n",
    "import shutil\n",
    "from typing import Optional,Dict,Tuple\n",
    "\n",
    "\n",
    "class CustomSentencePieceTokenizer(PreTrainedTokenizer):\n",
    "    def __init__(self, model_file, *args, **kwargs):\n",
    "        \n",
    "        self.sp_model = spm.SentencePieceProcessor()\n",
    "        self.sp_model.load(model_file)\n",
    "        self.model_file=model_file\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # 设置特殊标记\n",
    "        self.pad_token_id = self.sp_model.piece_to_id(\"<pad>\")\n",
    "        self.unk_token_id = self.sp_model.piece_to_id(\"<unk>\")\n",
    "        self.bos_token_id = self.sp_model.piece_to_id(\"<s>\")\n",
    "        self.eos_token_id = self.sp_model.piece_to_id(\"</s>\")\n",
    "        self.mask_token_id = self.sp_model.piece_to_id(\"<mask>\")\n",
    "        self.cls_token_id = self.sp_model.piece_to_id(\"<s>\")   # 使用<s>作为CLS token\n",
    "        self.sep_token_id = self.sp_model.piece_to_id(\"</s>\")  # 使用</s>作为SEP token\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return self.sp_model.get_piece_size()\n",
    "\n",
    "    def _tokenize(self, text, **kwargs):\n",
    "        return self.sp_model.encode_as_pieces(text)\n",
    "\n",
    "    def _convert_token_to_id(self, token):\n",
    "        return self.sp_model.piece_to_id(token)\n",
    "\n",
    "    def _convert_id_to_token(self, index):\n",
    "        return self.sp_model.id_to_piece(index)\n",
    "\n",
    "    def encode(self, text, **kwargs):\n",
    "        return self.sp_model.encode_as_ids(text)\n",
    "\n",
    "    def decode(self, ids, **kwargs):\n",
    "        return self.sp_model.decode(ids)\n",
    "\n",
    "    def get_vocab(self):\n",
    "        # 返回词汇表字典\n",
    "        return {self._convert_id_to_token(i): i for i in range(self.vocab_size)}\n",
    "\n",
    "    def save_pretrained(self, save_directory, **kwargs):\n",
    "        # 复制SentencePiece模型\n",
    "        shutil.copyfile(self.model_file, os.path.join(save_directory, 'spiece.model'))\n",
    "        \n",
    "        # 保存其他必要的文件\n",
    "        with open(os.path.join(save_directory, 'tokenizer_config.json'), 'w') as f:\n",
    "            f.write(json.dumps({\n",
    "                \"model_max_length\": 512,\n",
    "                \"padding_side\": \"right\",\n",
    "                \"truncation_side\": \"right\"\n",
    "            }))\n",
    "        \n",
    "        # 保存词汇表文件\n",
    "        with open(os.path.join(save_directory, 'vocab.json'), 'w') as f:\n",
    "            vocab = self.get_vocab()\n",
    "            json.dump(vocab, f)\n",
    "\n",
    "        # 保存特殊标记\n",
    "        special_tokens_map = {\n",
    "            \"unk_token\": self.sp_model.id_to_piece(self.unk_token_id),\n",
    "            \"sep_token\": self.sp_model.id_to_piece(self.sep_token_id),\n",
    "            \"pad_token\": self.sp_model.id_to_piece(self.pad_token_id),\n",
    "            \"cls_token\": self.sp_model.id_to_piece(self.cls_token_id),\n",
    "            \"mask_token\": self.sp_model.id_to_piece(self.mask_token_id),\n",
    "            \"bos_token\": self.sp_model.id_to_piece(self.bos_token_id),\n",
    "            \"eos_token\": self.sp_model.id_to_piece(self.eos_token_id)\n",
    "        }\n",
    "        with open(os.path.join(save_directory, 'special_tokens_map.json'), 'w') as f:\n",
    "            json.dump(special_tokens_map, f)\n",
    "\n",
    "        super().save_pretrained(save_directory, **kwargs)\n",
    "    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n",
    "        if filename_prefix is None:\n",
    "            filename_prefix = \"\"\n",
    "        \n",
    "        # 保存词汇表到纯文本文件\n",
    "        vocab_filename = os.path.join(save_directory, f\"{filename_prefix}vocab.txt\")\n",
    "        with open(vocab_filename, 'w', encoding='utf-8') as f:\n",
    "            for i in range(self.vocab_size):\n",
    "                token = self._convert_id_to_token(i)\n",
    "                f.write(f\"{token}\\n\")\n",
    "\n",
    "        # 返回保存的文件路径\n",
    "        return (vocab_filename,)\n",
    "        \n",
    "    @classmethod\n",
    "    def from_pretrained(cls, pretrained_model_name_or_path, *args, **kwargs):\n",
    "        if isinstance(pretrained_model_name_or_path, str) and os.path.isdir(pretrained_model_name_or_path):\n",
    "            model_file = os.path.join(pretrained_model_name_or_path, 'spiece.model')\n",
    "        else:\n",
    "            model_file = pretrained_model_name_or_path\n",
    "        return cls(model_file, *args, **kwargs)\n",
    "\n",
    "\n",
    "# 创建自定义的tokenizer\n",
    "tokenizer = CustomSentencePieceTokenizer(model_file=model_path)\n",
    "\n",
    "# 保存tokenizer到指定目录\n",
    "output_dir = f'{data_path_root}/custom_tokenizer2'\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# 加载已保存的tokenizer\n",
    "tokenizer = CustomSentencePieceTokenizer.from_pretrained(output_dir)\n",
    "\n",
    "\n",
    "encoded = tokenizer.encode(text)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "\n",
    "print(\"Encoded:\", encoded)\n",
    "print(\"Decoded:\", decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c940a48-2ba4-408c-ad02-d3cee40013ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁', '举', '头', '望', '明月', ',', '低', '头', '思', '故乡', '。']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4af93e5c-9053-496e-a112-3e8fb557b118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905ebbd0-d5cc-4d59-9291-028beb491ae5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2673ba3-4462-4c94-9c9e-5220e440aea7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3082096d-03cd-492c-8389-f11de249f35a",
   "metadata": {},
   "source": [
    "# 4 tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "fe355998-5fd2-42e6-8549-5006d659a37b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[48;5;167mt\u001b[48;5;179mo\u001b[48;5;185mk\u001b[48;5;77me\u001b[48;5;80mn\u001b[48;5;68mi\u001b[48;5;134mz\u001b[48;5;167ma\u001b[48;5;179mt\u001b[48;5;185mi\u001b[48;5;77mo\u001b[48;5;80mn\u001b[0m\n",
      "\u001b[48;5;167mt\u001b[48;5;179mo\u001b[48;5;185mk\u001b[48;5;77me\u001b[48;5;80mn\u001b[48;5;68mi\u001b[48;5;134mz\u001b[48;5;167ma\u001b[48;5;179mt\u001b[48;5;185mi\u001b[48;5;77mon\u001b[0m\n",
      "\u001b[48;5;167mt\u001b[48;5;179mo\u001b[48;5;185mk\u001b[48;5;77me\u001b[48;5;80mn\u001b[48;5;68mi\u001b[48;5;134mz\u001b[48;5;167mat\u001b[48;5;185mi\u001b[48;5;77mon\u001b[0m\n",
      "\u001b[48;5;167mt\u001b[48;5;179mo\u001b[48;5;185mk\u001b[48;5;77men\u001b[48;5;68mi\u001b[48;5;134mz\u001b[48;5;167mat\u001b[48;5;185mi\u001b[48;5;77mon\u001b[0m\n",
      "\u001b[48;5;167mt\u001b[48;5;179mo\u001b[48;5;185mk\u001b[48;5;77men\u001b[48;5;68mi\u001b[48;5;134mz\u001b[48;5;167mat\u001b[48;5;185mion\u001b[0m\n",
      "\u001b[48;5;167mt\u001b[48;5;179mo\u001b[48;5;185mk\u001b[48;5;77men\u001b[48;5;68mi\u001b[48;5;134mz\u001b[48;5;167mation\u001b[0m\n",
      "\u001b[48;5;167mt\u001b[48;5;179mo\u001b[48;5;185mk\u001b[48;5;77men\u001b[48;5;68miz\u001b[48;5;167mation\u001b[0m\n",
      "\u001b[48;5;167mt\u001b[48;5;179mok\u001b[48;5;77men\u001b[48;5;68miz\u001b[48;5;167mation\u001b[0m\n",
      "\u001b[48;5;167mt\u001b[48;5;179moken\u001b[48;5;68miz\u001b[48;5;167mation\u001b[0m\n",
      "\u001b[48;5;167mt\u001b[48;5;179moken\u001b[48;5;68mization\u001b[0m\n",
      "\u001b[48;5;167mtoken\u001b[48;5;68mization\u001b[0m\n",
      "\n",
      "\u001b[48;5;167m�\u001b[48;5;179m�\u001b[48;5;185m�\u001b[48;5;77m�\u001b[48;5;80m�\u001b[48;5;68m�\u001b[48;5;134m�\u001b[48;5;167m�\u001b[48;5;179m�\u001b[0m\n",
      "\u001b[48;5;167m�\u001b[48;5;179m�\u001b[48;5;185m�\u001b[48;5;77m�\u001b[48;5;80m�\u001b[48;5;68m�\u001b[48;5;134m�\u001b[48;5;167m�\u001b[0m\n",
      "\u001b[48;5;167m�\u001b[48;5;179m�\u001b[48;5;185m�\u001b[48;5;77m�\u001b[48;5;80m�\u001b[48;5;68m�\u001b[48;5;134m�\u001b[0m\n",
      "\u001b[48;5;167m�\u001b[48;5;179m�\u001b[48;5;185m�\u001b[48;5;77m�\u001b[48;5;80m�\u001b[48;5;68m�\u001b[0m\n",
      "\u001b[48;5;167m�\u001b[48;5;179m�\u001b[48;5;185m头\u001b[48;5;77m�\u001b[48;5;80m�\u001b[0m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3574, 122, 65455, 4916, 249]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tiktoken._educational import *\n",
    "\n",
    "# # Train a BPE tokeniser on a small amount of text\n",
    "# enc = train_simple_encoding()\n",
    "\n",
    "# Visualise how the GPT-4 encoder encodes text\n",
    "enc = SimpleBytePairEncoding.from_tiktoken(\"cl100k_base\")\n",
    "enc.encode(\"tokenization\")\n",
    "\n",
    "enc.encode(\"举头望\")# 9 字符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "acb5b4d3-7307-4534-8373-f8016138461d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "编码 [3574, 122, 65455, 4916, 249]\n",
      "解码 举头望\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[5963, 2065]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# 创建一个编码器实例\n",
    "encoder = tiktoken.get_encoding(\"cl100k_base\")  # 使用特定的编码方案\n",
    "# 编码文本\n",
    "text = \"举头望\"\n",
    "tokens = encoder.encode(text)\n",
    "print(\"编码\",tokens)  # 输出 token ID 列表\n",
    "# 解码 token\n",
    "decoded_text = encoder.decode(tokens)\n",
    "print(\"解码\",decoded_text)  # 输出原始文本\n",
    "encoder.encode(\"tokenization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "317a9063-aed6-41bb-85f9-78476675bb98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "编码 [10310, 122, 13783, 112, 17312, 249]\n",
      "解码 举头望\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[30001, 1634]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# 创建一个编码器实例\n",
    "encoder = tiktoken.get_encoding(\"gpt2\")  # 使用特定的编码方案\n",
    "# 编码文本\n",
    "text = \"举头望\"\n",
    "tokens = encoder.encode(text)\n",
    "print(\"编码\",tokens)  # 输出 token ID 列表\n",
    "# 解码 token\n",
    "decoded_text = encoder.decode(tokens)\n",
    "print(\"解码\",decoded_text)  # 输出原始文本\n",
    "encoder.encode(\"tokenization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f350523f-a8b5-4e9c-a784-889933f5c0ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 100277\n",
      "Token ID 3574 corresponds to: �\n"
     ]
    }
   ],
   "source": [
    "# 获取词汇表大小\n",
    "vocab_size = encoder.n_vocab\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "# 获取单个 token\n",
    "token_id = 65455\n",
    "token_str = encoder.decode([token_id])\n",
    "print(f\"Token ID {token_id} corresponds to: {token_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c8d84bc6-c198-462b-8cb9-84cf29754a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded and Encoded: [3574, 122, 65455, 4916, 249]\n",
      "Loaded and Decoded: 举头望\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizer\n",
    "import tiktoken\n",
    "\n",
    "class CustomTiktokenTokenizer(PreTrainedTokenizer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        \n",
    "        self.encoder = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return self.encoder.n_vocab\n",
    "\n",
    "    def _tokenize(self, text, **kwargs):\n",
    "        return [str(token) for token in self.encoder.encode(text)]\n",
    "\n",
    "    def _convert_token_to_id(self, token):\n",
    "        if isinstance(token, str):\n",
    "            token = int(token)\n",
    "        return token\n",
    "\n",
    "    def _convert_id_to_token(self, index):\n",
    "        return str(index)\n",
    "\n",
    "    def encode(self, text, **kwargs):\n",
    "        return self.encoder.encode(text)\n",
    "\n",
    "    def decode(self, ids, **kwargs):\n",
    "        return self.encoder.decode(ids)\n",
    "\n",
    "    def get_vocab(self):\n",
    "        # 返回词汇表字典\n",
    "        return {str(i): i for i in range(self.vocab_size)}\n",
    "\n",
    "    def save_pretrained(self, save_directory, **kwargs):\n",
    "        # 保存词汇表文件\n",
    "        with open(os.path.join(save_directory, 'vocab.json'), 'w') as f:\n",
    "            json.dump(self.get_vocab(), f)\n",
    "\n",
    "        # 保存特殊标记\n",
    "        special_tokens_map = {\n",
    "            \"unk_token\": str(self.encoder.eot_token),\n",
    "            \"sep_token\": str(self.encoder.eot_token),\n",
    "            \"pad_token\": str(self.encoder.eot_token),\n",
    "            \"cls_token\": str(self.encoder.eot_token),\n",
    "            \"mask_token\": str(self.encoder.eot_token),\n",
    "            \"bos_token\": str('<|startoftext|>'),\n",
    "            \"eos_token\": str(self.encoder.eot_token)\n",
    "        }\n",
    "\n",
    "        \n",
    "        with open(os.path.join(save_directory, 'special_tokens_map.json'), 'w') as f:\n",
    "            json.dump(special_tokens_map, f)\n",
    "\n",
    "        # 保存配置文件\n",
    "        with open(os.path.join(save_directory, 'tokenizer_config.json'), 'w') as f:\n",
    "            f.write(json.dumps({\n",
    "                \"model_max_length\": 2048,\n",
    "                \"padding_side\": \"right\",\n",
    "                \"truncation_side\": \"right\"\n",
    "            }))\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, pretrained_model_name_or_path, *args, **kwargs):\n",
    "        return cls(*args, **kwargs)\n",
    "\n",
    "# 示例使用\n",
    "tokenizer = CustomTiktokenTokenizer()\n",
    "\n",
    "\n",
    "# 保存tokenizer到指定目录\n",
    "output_dir = f'{data_path_root}/custom_tokenizer4'\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# 加载 tokenizer\n",
    "loaded_tokenizer = CustomTiktokenTokenizer.from_pretrained(output_dir)\n",
    "encoded_loaded = loaded_tokenizer.encode(text)\n",
    "decoded_loaded = loaded_tokenizer.decode(encoded_loaded)\n",
    "\n",
    "print(\"Loaded and Encoded:\", encoded_loaded)\n",
    "print(\"Loaded and Decoded:\", decoded_loaded)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d7ac41-59ab-4560-a512-4fa55f17cfef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2e5b102-af99-419b-8aa1-f605f7f34a3e",
   "metadata": {},
   "source": [
    "#### 4.3 Extending tiktoken\n",
    "添加新的token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "aed4484c-3057-4f85-831d-72b937da8023",
   "metadata": {},
   "outputs": [],
   "source": [
    "cl100k_base = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# In production, load the arguments directly instead of accessing private attributes\n",
    "# See openai_public.py for examples of arguments for specific encodings\n",
    "enc = tiktoken.Encoding(\n",
    "    # If you're changing the set of special tokens, make sure to use a different name\n",
    "    # It should be clear from the name what behaviour to expect.\n",
    "    name=\"cl100k_im\",\n",
    "    pat_str=cl100k_base._pat_str,\n",
    "    mergeable_ranks=cl100k_base._mergeable_ranks,\n",
    "    special_tokens={\n",
    "        **cl100k_base._special_tokens,\n",
    "        \"<|im_start|>\": 100264,\n",
    "        \"<|im_end|>\": 100269,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "910b37df-babf-45ac-989d-6bc8450ec1bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100269]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'<|endoftext|>': 100257,\n",
       " '<|fim_prefix|>': 100258,\n",
       " '<|fim_middle|>': 100259,\n",
       " '<|fim_suffix|>': 100260,\n",
       " '<|endofprompt|>': 100276,\n",
       " '<|im_start|>': 100264,\n",
       " '<|im_end|>': 100269}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 需要传参数 ：allowed_special\n",
    "print(enc.encode(\"<|im_end|>\",allowed_special=set(enc._special_tokens.keys())))\n",
    "enc._special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c1698936-688a-4546-8fc0-d369123bd75c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'cl100k_im',\n",
       " 'pat_str': \"'(?i:[sdmt]|ll|ve|re)|[^\\\\r\\\\n\\\\p{L}\\\\p{N}]?+\\\\p{L}++|\\\\p{N}{1,3}+| ?[^\\\\s\\\\p{L}\\\\p{N}]++[\\\\r\\\\n]*+|\\\\s++$|\\\\s*[\\\\r\\\\n]|\\\\s+(?!\\\\S)|\\\\s\",\n",
       " 'mergeable_ranks': {b'!': 0,\n",
       "  b'\"': 1,\n",
       "  b'#': 2,\n",
       "  b'$': 3,\n",
       "  b'%': 4,\n",
       "  b'&': 5,\n",
       "  b\"'\": 6,\n",
       "  b'(': 7,\n",
       "  b')': 8,\n",
       "  b'*': 9,\n",
       "  b'+': 10,\n",
       "  b',': 11,\n",
       "  b'-': 12,\n",
       "  b'.': 13,\n",
       "  b'/': 14,\n",
       "  b'0': 15,\n",
       "  b'1': 16,\n",
       "  b'2': 17,\n",
       "  b'3': 18,\n",
       "  b'4': 19,\n",
       "  b'5': 20,\n",
       "  b'6': 21,\n",
       "  b'7': 22,\n",
       "  b'8': 23,\n",
       "  b'9': 24,\n",
       "  b':': 25,\n",
       "  b';': 26,\n",
       "  b'<': 27,\n",
       "  b'=': 28,\n",
       "  b'>': 29,\n",
       "  b'?': 30,\n",
       "  b'@': 31,\n",
       "  b'A': 32,\n",
       "  b'B': 33,\n",
       "  b'C': 34,\n",
       "  b'D': 35,\n",
       "  b'E': 36,\n",
       "  b'F': 37,\n",
       "  b'G': 38,\n",
       "  b'H': 39,\n",
       "  b'I': 40,\n",
       "  b'J': 41,\n",
       "  b'K': 42,\n",
       "  b'L': 43,\n",
       "  b'M': 44,\n",
       "  b'N': 45,\n",
       "  b'O': 46,\n",
       "  b'P': 47,\n",
       "  b'Q': 48,\n",
       "  b'R': 49,\n",
       "  b'S': 50,\n",
       "  b'T': 51,\n",
       "  b'U': 52,\n",
       "  b'V': 53,\n",
       "  b'W': 54,\n",
       "  b'X': 55,\n",
       "  b'Y': 56,\n",
       "  b'Z': 57,\n",
       "  b'[': 58,\n",
       "  b'\\\\': 59,\n",
       "  b']': 60,\n",
       "  b'^': 61,\n",
       "  b'_': 62,\n",
       "  b'`': 63,\n",
       "  b'a': 64,\n",
       "  b'b': 65,\n",
       "  b'c': 66,\n",
       "  b'd': 67,\n",
       "  b'e': 68,\n",
       "  b'f': 69,\n",
       "  b'g': 70,\n",
       "  b'h': 71,\n",
       "  b'i': 72,\n",
       "  b'j': 73,\n",
       "  b'k': 74,\n",
       "  b'l': 75,\n",
       "  b'm': 76,\n",
       "  b'n': 77,\n",
       "  b'o': 78,\n",
       "  b'p': 79,\n",
       "  b'q': 80,\n",
       "  b'r': 81,\n",
       "  b's': 82,\n",
       "  b't': 83,\n",
       "  b'u': 84,\n",
       "  b'v': 85,\n",
       "  b'w': 86,\n",
       "  b'x': 87,\n",
       "  b'y': 88,\n",
       "  b'z': 89,\n",
       "  b'{': 90,\n",
       "  b'|': 91,\n",
       "  b'}': 92,\n",
       "  b'~': 93,\n",
       "  b'\\xa1': 94,\n",
       "  b'\\xa2': 95,\n",
       "  b'\\xa3': 96,\n",
       "  b'\\xa4': 97,\n",
       "  b'\\xa5': 98,\n",
       "  b'\\xa6': 99,\n",
       "  b'\\xa7': 100,\n",
       "  b'\\xa8': 101,\n",
       "  b'\\xa9': 102,\n",
       "  b'\\xaa': 103,\n",
       "  b'\\xab': 104,\n",
       "  b'\\xac': 105,\n",
       "  b'\\xae': 106,\n",
       "  b'\\xaf': 107,\n",
       "  b'\\xb0': 108,\n",
       "  b'\\xb1': 109,\n",
       "  b'\\xb2': 110,\n",
       "  b'\\xb3': 111,\n",
       "  b'\\xb4': 112,\n",
       "  b'\\xb5': 113,\n",
       "  b'\\xb6': 114,\n",
       "  b'\\xb7': 115,\n",
       "  b'\\xb8': 116,\n",
       "  b'\\xb9': 117,\n",
       "  b'\\xba': 118,\n",
       "  b'\\xbb': 119,\n",
       "  b'\\xbc': 120,\n",
       "  b'\\xbd': 121,\n",
       "  b'\\xbe': 122,\n",
       "  b'\\xbf': 123,\n",
       "  b'\\xc0': 124,\n",
       "  b'\\xc1': 125,\n",
       "  b'\\xc2': 126,\n",
       "  b'\\xc3': 127,\n",
       "  b'\\xc4': 128,\n",
       "  b'\\xc5': 129,\n",
       "  b'\\xc6': 130,\n",
       "  b'\\xc7': 131,\n",
       "  b'\\xc8': 132,\n",
       "  b'\\xc9': 133,\n",
       "  b'\\xca': 134,\n",
       "  b'\\xcb': 135,\n",
       "  b'\\xcc': 136,\n",
       "  b'\\xcd': 137,\n",
       "  b'\\xce': 138,\n",
       "  b'\\xcf': 139,\n",
       "  b'\\xd0': 140,\n",
       "  b'\\xd1': 141,\n",
       "  b'\\xd2': 142,\n",
       "  b'\\xd3': 143,\n",
       "  b'\\xd4': 144,\n",
       "  b'\\xd5': 145,\n",
       "  b'\\xd6': 146,\n",
       "  b'\\xd7': 147,\n",
       "  b'\\xd8': 148,\n",
       "  b'\\xd9': 149,\n",
       "  b'\\xda': 150,\n",
       "  b'\\xdb': 151,\n",
       "  b'\\xdc': 152,\n",
       "  b'\\xdd': 153,\n",
       "  b'\\xde': 154,\n",
       "  b'\\xdf': 155,\n",
       "  b'\\xe0': 156,\n",
       "  b'\\xe1': 157,\n",
       "  b'\\xe2': 158,\n",
       "  b'\\xe3': 159,\n",
       "  b'\\xe4': 160,\n",
       "  b'\\xe5': 161,\n",
       "  b'\\xe6': 162,\n",
       "  b'\\xe7': 163,\n",
       "  b'\\xe8': 164,\n",
       "  b'\\xe9': 165,\n",
       "  b'\\xea': 166,\n",
       "  b'\\xeb': 167,\n",
       "  b'\\xec': 168,\n",
       "  b'\\xed': 169,\n",
       "  b'\\xee': 170,\n",
       "  b'\\xef': 171,\n",
       "  b'\\xf0': 172,\n",
       "  b'\\xf1': 173,\n",
       "  b'\\xf2': 174,\n",
       "  b'\\xf3': 175,\n",
       "  b'\\xf4': 176,\n",
       "  b'\\xf5': 177,\n",
       "  b'\\xf6': 178,\n",
       "  b'\\xf7': 179,\n",
       "  b'\\xf8': 180,\n",
       "  b'\\xf9': 181,\n",
       "  b'\\xfa': 182,\n",
       "  b'\\xfb': 183,\n",
       "  b'\\xfc': 184,\n",
       "  b'\\xfd': 185,\n",
       "  b'\\xfe': 186,\n",
       "  b'\\xff': 187,\n",
       "  b'\\x00': 188,\n",
       "  b'\\x01': 189,\n",
       "  b'\\x02': 190,\n",
       "  b'\\x03': 191,\n",
       "  b'\\x04': 192,\n",
       "  b'\\x05': 193,\n",
       "  b'\\x06': 194,\n",
       "  b'\\x07': 195,\n",
       "  b'\\x08': 196,\n",
       "  b'\\t': 197,\n",
       "  b'\\n': 198,\n",
       "  b'\\x0b': 199,\n",
       "  b'\\x0c': 200,\n",
       "  b'\\r': 201,\n",
       "  b'\\x0e': 202,\n",
       "  b'\\x0f': 203,\n",
       "  b'\\x10': 204,\n",
       "  b'\\x11': 205,\n",
       "  b'\\x12': 206,\n",
       "  b'\\x13': 207,\n",
       "  b'\\x14': 208,\n",
       "  b'\\x15': 209,\n",
       "  b'\\x16': 210,\n",
       "  b'\\x17': 211,\n",
       "  b'\\x18': 212,\n",
       "  b'\\x19': 213,\n",
       "  b'\\x1a': 214,\n",
       "  b'\\x1b': 215,\n",
       "  b'\\x1c': 216,\n",
       "  b'\\x1d': 217,\n",
       "  b'\\x1e': 218,\n",
       "  b'\\x1f': 219,\n",
       "  b' ': 220,\n",
       "  b'\\x7f': 221,\n",
       "  b'\\x80': 222,\n",
       "  b'\\x81': 223,\n",
       "  b'\\x82': 224,\n",
       "  b'\\x83': 225,\n",
       "  b'\\x84': 226,\n",
       "  b'\\x85': 227,\n",
       "  b'\\x86': 228,\n",
       "  b'\\x87': 229,\n",
       "  b'\\x88': 230,\n",
       "  b'\\x89': 231,\n",
       "  b'\\x8a': 232,\n",
       "  b'\\x8b': 233,\n",
       "  b'\\x8c': 234,\n",
       "  b'\\x8d': 235,\n",
       "  b'\\x8e': 236,\n",
       "  b'\\x8f': 237,\n",
       "  b'\\x90': 238,\n",
       "  b'\\x91': 239,\n",
       "  b'\\x92': 240,\n",
       "  b'\\x93': 241,\n",
       "  b'\\x94': 242,\n",
       "  b'\\x95': 243,\n",
       "  b'\\x96': 244,\n",
       "  b'\\x97': 245,\n",
       "  b'\\x98': 246,\n",
       "  b'\\x99': 247,\n",
       "  b'\\x9a': 248,\n",
       "  b'\\x9b': 249,\n",
       "  b'\\x9c': 250,\n",
       "  b'\\x9d': 251,\n",
       "  b'\\x9e': 252,\n",
       "  b'\\x9f': 253,\n",
       "  b'\\xa0': 254,\n",
       "  b'\\xad': 255,\n",
       "  b'  ': 256,\n",
       "  b'    ': 257,\n",
       "  b'in': 258,\n",
       "  b' t': 259,\n",
       "  b'        ': 260,\n",
       "  b'er': 261,\n",
       "  b'   ': 262,\n",
       "  b'on': 263,\n",
       "  b' a': 264,\n",
       "  b're': 265,\n",
       "  b'at': 266,\n",
       "  b'st': 267,\n",
       "  b'en': 268,\n",
       "  b'or': 269,\n",
       "  b' th': 270,\n",
       "  b'\\n\\n': 271,\n",
       "  b' c': 272,\n",
       "  b'le': 273,\n",
       "  b' s': 274,\n",
       "  b'it': 275,\n",
       "  b'an': 276,\n",
       "  b'ar': 277,\n",
       "  b'al': 278,\n",
       "  b' the': 279,\n",
       "  b';\\n': 280,\n",
       "  b' p': 281,\n",
       "  b' f': 282,\n",
       "  b'ou': 283,\n",
       "  b' =': 284,\n",
       "  b'is': 285,\n",
       "  b'       ': 286,\n",
       "  b'ing': 287,\n",
       "  b'es': 288,\n",
       "  b' w': 289,\n",
       "  b'ion': 290,\n",
       "  b'ed': 291,\n",
       "  b'ic': 292,\n",
       "  b' b': 293,\n",
       "  b' d': 294,\n",
       "  b'et': 295,\n",
       "  b' m': 296,\n",
       "  b' o': 297,\n",
       "  b'\\t\\t': 298,\n",
       "  b'ro': 299,\n",
       "  b'as': 300,\n",
       "  b'el': 301,\n",
       "  b'ct': 302,\n",
       "  b'nd': 303,\n",
       "  b' in': 304,\n",
       "  b' h': 305,\n",
       "  b'ent': 306,\n",
       "  b'id': 307,\n",
       "  b' n': 308,\n",
       "  b'am': 309,\n",
       "  b'           ': 310,\n",
       "  b' to': 311,\n",
       "  b' re': 312,\n",
       "  b'--': 313,\n",
       "  b' {': 314,\n",
       "  b' of': 315,\n",
       "  b'om': 316,\n",
       "  b');\\n': 317,\n",
       "  b'im': 318,\n",
       "  b'\\r\\n': 319,\n",
       "  b' (': 320,\n",
       "  b'il': 321,\n",
       "  b'//': 322,\n",
       "  b' and': 323,\n",
       "  b'ur': 324,\n",
       "  b'se': 325,\n",
       "  b' l': 326,\n",
       "  b'ex': 327,\n",
       "  b' S': 328,\n",
       "  b'ad': 329,\n",
       "  b' \"': 330,\n",
       "  b'ch': 331,\n",
       "  b'ut': 332,\n",
       "  b'if': 333,\n",
       "  b'**': 334,\n",
       "  b' }': 335,\n",
       "  b'em': 336,\n",
       "  b'ol': 337,\n",
       "  b'                ': 338,\n",
       "  b'th': 339,\n",
       "  b')\\n': 340,\n",
       "  b' {\\n': 341,\n",
       "  b' g': 342,\n",
       "  b'ig': 343,\n",
       "  b'iv': 344,\n",
       "  b',\\n': 345,\n",
       "  b'ce': 346,\n",
       "  b'od': 347,\n",
       "  b' v': 348,\n",
       "  b'ate': 349,\n",
       "  b' T': 350,\n",
       "  b'ag': 351,\n",
       "  b'ay': 352,\n",
       "  b' *': 353,\n",
       "  b'ot': 354,\n",
       "  b'us': 355,\n",
       "  b' C': 356,\n",
       "  b' st': 357,\n",
       "  b' I': 358,\n",
       "  b'un': 359,\n",
       "  b'ul': 360,\n",
       "  b'ue': 361,\n",
       "  b' A': 362,\n",
       "  b'ow': 363,\n",
       "  b\" '\": 364,\n",
       "  b'ew': 365,\n",
       "  b' <': 366,\n",
       "  b'ation': 367,\n",
       "  b'()': 368,\n",
       "  b' for': 369,\n",
       "  b'ab': 370,\n",
       "  b'ort': 371,\n",
       "  b'um': 372,\n",
       "  b'ame': 373,\n",
       "  b' is': 374,\n",
       "  b'pe': 375,\n",
       "  b'tr': 376,\n",
       "  b'ck': 377,\n",
       "  b'\\xe2\\x80': 378,\n",
       "  b' y': 379,\n",
       "  b'ist': 380,\n",
       "  b'----': 381,\n",
       "  b'.\\n\\n': 382,\n",
       "  b'he': 383,\n",
       "  b' e': 384,\n",
       "  b'lo': 385,\n",
       "  b' M': 386,\n",
       "  b' be': 387,\n",
       "  b'ers': 388,\n",
       "  b' on': 389,\n",
       "  b' con': 390,\n",
       "  b'ap': 391,\n",
       "  b'ub': 392,\n",
       "  b' P': 393,\n",
       "  b'               ': 394,\n",
       "  b'ass': 395,\n",
       "  b'int': 396,\n",
       "  b'>\\n': 397,\n",
       "  b'ly': 398,\n",
       "  b'urn': 399,\n",
       "  b' $': 400,\n",
       "  b';\\n\\n': 401,\n",
       "  b'av': 402,\n",
       "  b'port': 403,\n",
       "  b'ir': 404,\n",
       "  b'->': 405,\n",
       "  b'nt': 406,\n",
       "  b'ction': 407,\n",
       "  b'end': 408,\n",
       "  b' de': 409,\n",
       "  b'00': 410,\n",
       "  b'ith': 411,\n",
       "  b'out': 412,\n",
       "  b'turn': 413,\n",
       "  b'our': 414,\n",
       "  b'     ': 415,\n",
       "  b'lic': 416,\n",
       "  b'res': 417,\n",
       "  b'pt': 418,\n",
       "  b'==': 419,\n",
       "  b' this': 420,\n",
       "  b' wh': 421,\n",
       "  b' if': 422,\n",
       "  b' D': 423,\n",
       "  b'ver': 424,\n",
       "  b'age': 425,\n",
       "  b' B': 426,\n",
       "  b'ht': 427,\n",
       "  b'ext': 428,\n",
       "  b'=\"': 429,\n",
       "  b' that': 430,\n",
       "  b'****': 431,\n",
       "  b' R': 432,\n",
       "  b' it': 433,\n",
       "  b'ess': 434,\n",
       "  b' F': 435,\n",
       "  b' r': 436,\n",
       "  b'os': 437,\n",
       "  b'and': 438,\n",
       "  b' as': 439,\n",
       "  b'ect': 440,\n",
       "  b'ke': 441,\n",
       "  b'rom': 442,\n",
       "  b' //': 443,\n",
       "  b'con': 444,\n",
       "  b' L': 445,\n",
       "  b'(\"': 446,\n",
       "  b'qu': 447,\n",
       "  b'lass': 448,\n",
       "  b' with': 449,\n",
       "  b'iz': 450,\n",
       "  b'de': 451,\n",
       "  b' N': 452,\n",
       "  b' al': 453,\n",
       "  b'op': 454,\n",
       "  b'up': 455,\n",
       "  b'get': 456,\n",
       "  b' }\\n': 457,\n",
       "  b'ile': 458,\n",
       "  b' an': 459,\n",
       "  b'ata': 460,\n",
       "  b'ore': 461,\n",
       "  b'ri': 462,\n",
       "  b' pro': 463,\n",
       "  b';\\r\\n': 464,\n",
       "  b'\\t\\t\\t\\t': 465,\n",
       "  b'ter': 466,\n",
       "  b'ain': 467,\n",
       "  b' W': 468,\n",
       "  b' E': 469,\n",
       "  b' com': 470,\n",
       "  b' return': 471,\n",
       "  b'art': 472,\n",
       "  b' H': 473,\n",
       "  b'ack': 474,\n",
       "  b'import': 475,\n",
       "  b'ublic': 476,\n",
       "  b' or': 477,\n",
       "  b'est': 478,\n",
       "  b'ment': 479,\n",
       "  b' G': 480,\n",
       "  b'able': 481,\n",
       "  b' -': 482,\n",
       "  b'ine': 483,\n",
       "  b'ill': 484,\n",
       "  b'ind': 485,\n",
       "  b'ere': 486,\n",
       "  b'::': 487,\n",
       "  b'ity': 488,\n",
       "  b' +': 489,\n",
       "  b' tr': 490,\n",
       "  b'elf': 491,\n",
       "  b'ight': 492,\n",
       "  b\"('\": 493,\n",
       "  b'orm': 494,\n",
       "  b'ult': 495,\n",
       "  b'str': 496,\n",
       "  b'..': 497,\n",
       "  b'\",': 498,\n",
       "  b' you': 499,\n",
       "  b'ype': 500,\n",
       "  b'pl': 501,\n",
       "  b' new': 502,\n",
       "  b' j': 503,\n",
       "  b'                   ': 504,\n",
       "  b' from': 505,\n",
       "  b' ex': 506,\n",
       "  b' O': 507,\n",
       "  b'20': 508,\n",
       "  b'ld': 509,\n",
       "  b' [': 510,\n",
       "  b'oc': 511,\n",
       "  b':\\n': 512,\n",
       "  b' se': 513,\n",
       "  b' le': 514,\n",
       "  b'--------': 515,\n",
       "  b'.s': 516,\n",
       "  b'{\\n': 517,\n",
       "  b\"',\": 518,\n",
       "  b'ant': 519,\n",
       "  b' at': 520,\n",
       "  b'ase': 521,\n",
       "  b'.c': 522,\n",
       "  b' ch': 523,\n",
       "  b'</': 524,\n",
       "  b'ave': 525,\n",
       "  b'ang': 526,\n",
       "  b' are': 527,\n",
       "  b' int': 528,\n",
       "  b'\\xe2\\x80\\x99': 529,\n",
       "  b'_t': 530,\n",
       "  b'ert': 531,\n",
       "  b'ial': 532,\n",
       "  b'act': 533,\n",
       "  b'}\\n': 534,\n",
       "  b'ive': 535,\n",
       "  b'ode': 536,\n",
       "  b'ost': 537,\n",
       "  b' class': 538,\n",
       "  b' not': 539,\n",
       "  b'og': 540,\n",
       "  b'ord': 541,\n",
       "  b'alue': 542,\n",
       "  b'all': 543,\n",
       "  b'ff': 544,\n",
       "  b'();\\n': 545,\n",
       "  b'ont': 546,\n",
       "  b'ime': 547,\n",
       "  b'are': 548,\n",
       "  b' U': 549,\n",
       "  b' pr': 550,\n",
       "  b' :': 551,\n",
       "  b'ies': 552,\n",
       "  b'ize': 553,\n",
       "  b'ure': 554,\n",
       "  b' by': 555,\n",
       "  b'ire': 556,\n",
       "  b' }\\n\\n': 557,\n",
       "  b'.p': 558,\n",
       "  b' sh': 559,\n",
       "  b'ice': 560,\n",
       "  b'ast': 561,\n",
       "  b'ption': 562,\n",
       "  b'tring': 563,\n",
       "  b'ok': 564,\n",
       "  b'__': 565,\n",
       "  b'cl': 566,\n",
       "  b'##': 567,\n",
       "  b' he': 568,\n",
       "  b'ard': 569,\n",
       "  b').': 570,\n",
       "  b' @': 571,\n",
       "  b'iew': 572,\n",
       "  b'\\t\\t\\t': 573,\n",
       "  b' was': 574,\n",
       "  b'ip': 575,\n",
       "  b'this': 576,\n",
       "  b' u': 577,\n",
       "  b' The': 578,\n",
       "  b'ide': 579,\n",
       "  b'ace': 580,\n",
       "  b'ib': 581,\n",
       "  b'ac': 582,\n",
       "  b'rou': 583,\n",
       "  b' we': 584,\n",
       "  b'ject': 585,\n",
       "  b' public': 586,\n",
       "  b'ak': 587,\n",
       "  b've': 588,\n",
       "  b'ath': 589,\n",
       "  b'oid': 590,\n",
       "  b' =>': 591,\n",
       "  b'ust': 592,\n",
       "  b'que': 593,\n",
       "  b' res': 594,\n",
       "  b'))': 595,\n",
       "  b\"'s\": 596,\n",
       "  b' k': 597,\n",
       "  b'ans': 598,\n",
       "  b'yst': 599,\n",
       "  b'unction': 600,\n",
       "  b'********': 601,\n",
       "  b' i': 602,\n",
       "  b' us': 603,\n",
       "  b'pp': 604,\n",
       "  b'10': 605,\n",
       "  b'one': 606,\n",
       "  b'ail': 607,\n",
       "  b'====': 608,\n",
       "  b'name': 609,\n",
       "  b' str': 610,\n",
       "  b' /': 611,\n",
       "  b' &': 612,\n",
       "  b'ach': 613,\n",
       "  b'div': 614,\n",
       "  b'ystem': 615,\n",
       "  b'ell': 616,\n",
       "  b' have': 617,\n",
       "  b'err': 618,\n",
       "  b'ould': 619,\n",
       "  b'ull': 620,\n",
       "  b'pon': 621,\n",
       "  b' J': 622,\n",
       "  b'_p': 623,\n",
       "  b' ==': 624,\n",
       "  b'ign': 625,\n",
       "  b'St': 626,\n",
       "  b'.\\n': 627,\n",
       "  b' pl': 628,\n",
       "  b');\\n\\n': 629,\n",
       "  b'form': 630,\n",
       "  b'put': 631,\n",
       "  b'ount': 632,\n",
       "  b'}\\n\\n': 633,\n",
       "  b'dd': 634,\n",
       "  b'ite': 635,\n",
       "  b' get': 636,\n",
       "  b'rr': 637,\n",
       "  b'ome': 638,\n",
       "  b' \\xe2\\x80': 639,\n",
       "  b'aram': 640,\n",
       "  b'cc': 641,\n",
       "  b' */': 642,\n",
       "  b'ER': 643,\n",
       "  b'In': 644,\n",
       "  b'les': 645,\n",
       "  b'_s': 646,\n",
       "  b'ong': 647,\n",
       "  b'ie': 648,\n",
       "  b' can': 649,\n",
       "  b' V': 650,\n",
       "  b'erv': 651,\n",
       "  b'pr': 652,\n",
       "  b' un': 653,\n",
       "  b'row': 654,\n",
       "  b'ber': 655,\n",
       "  b' do': 656,\n",
       "  b'll': 657,\n",
       "  b' el': 658,\n",
       "  b' self': 659,\n",
       "  b'ated': 660,\n",
       "  b'ary': 661,\n",
       "  b' .': 662,\n",
       "  b\"']\": 663,\n",
       "  b'ud': 664,\n",
       "  b' en': 665,\n",
       "  b' Th': 666,\n",
       "  b'                       ': 667,\n",
       "  b'te': 668,\n",
       "  b'_c': 669,\n",
       "  b'uct': 670,\n",
       "  b' ab': 671,\n",
       "  b'ork': 672,\n",
       "  b'.get': 673,\n",
       "  b' #': 674,\n",
       "  b'aw': 675,\n",
       "  b'ress': 676,\n",
       "  b'ob': 677,\n",
       "  b'Name': 678,\n",
       "  b'201': 679,\n",
       "  b'app': 680,\n",
       "  b\"['\": 681,\n",
       "  b' all': 682,\n",
       "  b'ory': 683,\n",
       "  b'ition': 684,\n",
       "  b'ance': 685,\n",
       "  b'ear': 686,\n",
       "  b' cont': 687,\n",
       "  b'vent': 688,\n",
       "  b'ia': 689,\n",
       "  b' will': 690,\n",
       "  b'IN': 691,\n",
       "  b'         ': 692,\n",
       "  b'return': 693,\n",
       "  b' </': 694,\n",
       "  b'data': 695,\n",
       "  b')\\n\\n': 696,\n",
       "  b'Re': 697,\n",
       "  b'ple': 698,\n",
       "  b'ild': 699,\n",
       "  b'ther': 700,\n",
       "  b' your': 701,\n",
       "  b'\"\\n': 702,\n",
       "  b'($': 703,\n",
       "  b' out': 704,\n",
       "  b'),': 705,\n",
       "  b' has': 706,\n",
       "  b'String': 707,\n",
       "  b'so': 708,\n",
       "  b' up': 709,\n",
       "  b'ax': 710,\n",
       "  b' def': 711,\n",
       "  b' bo': 712,\n",
       "  b'ge': 713,\n",
       "  b'alse': 714,\n",
       "  b'ON': 715,\n",
       "  b'per': 716,\n",
       "  b'12': 717,\n",
       "  b'ich': 718,\n",
       "  b' but': 719,\n",
       "  b' \\n': 720,\n",
       "  b' _': 721,\n",
       "  b'_m': 722,\n",
       "  b'add': 723,\n",
       "  b'quest': 724,\n",
       "  b'odel': 725,\n",
       "  b'self': 726,\n",
       "  b'ery': 727,\n",
       "  b'ft': 728,\n",
       "  b'ens': 729,\n",
       "  b'////': 730,\n",
       "  b'ake': 731,\n",
       "  b'.C': 732,\n",
       "  b' go': 733,\n",
       "  b' function': 734,\n",
       "  b' K': 735,\n",
       "  b'ivate': 736,\n",
       "  b' im': 737,\n",
       "  b' const': 738,\n",
       "  b'.t': 739,\n",
       "  b' */\\n': 740,\n",
       "  b');\\r\\n': 741,\n",
       "  b' void': 742,\n",
       "  b' set': 743,\n",
       "  b' System': 744,\n",
       "  b'cri': 745,\n",
       "  b'()\\n': 746,\n",
       "  b'li': 747,\n",
       "  b'\\tif': 748,\n",
       "  b'.m': 749,\n",
       "  b'ally': 750,\n",
       "  b'set': 751,\n",
       "  b'ep': 752,\n",
       "  b'\\xe2\\x80\\x99s': 753,\n",
       "  b'bo': 754,\n",
       "  b'def': 755,\n",
       "  b\"',\\n\": 756,\n",
       "  b' me': 757,\n",
       "  b' !': 758,\n",
       "  b'atch': 759,\n",
       "  b'\">': 760,\n",
       "  b'\",\\n': 761,\n",
       "  b'ec': 762,\n",
       "  b' In': 763,\n",
       "  b'ph': 764,\n",
       "  b' |': 765,\n",
       "  b'_f': 766,\n",
       "  b' var': 767,\n",
       "  b'ence': 768,\n",
       "  b'Id': 769,\n",
       "  b'ree': 770,\n",
       "  b'ink': 771,\n",
       "  b'lect': 772,\n",
       "  b'ug': 773,\n",
       "  b'eth': 774,\n",
       "  b' else': 775,\n",
       "  b'----------------': 776,\n",
       "  b'19': 777,\n",
       "  b'cont': 778,\n",
       "  b' so': 779,\n",
       "  b'atic': 780,\n",
       "  b' lo': 781,\n",
       "  b'pro': 782,\n",
       "  b'ton': 783,\n",
       "  b'ss': 784,\n",
       "  b'own': 785,\n",
       "  b'abel': 786,\n",
       "  b'oint': 787,\n",
       "  b'ous': 788,\n",
       "  b'eld': 789,\n",
       "  b'ST': 790,\n",
       "  b'The': 791,\n",
       "  b'                                ': 792,\n",
       "  b'RE': 793,\n",
       "  b'\":': 794,\n",
       "  b'olor': 795,\n",
       "  b'tp': 796,\n",
       "  b'eg': 797,\n",
       "  b'key': 798,\n",
       "  b'ude': 799,\n",
       "  b' St': 800,\n",
       "  b'ound': 801,\n",
       "  b' ar': 802,\n",
       "  b'\");\\n': 803,\n",
       "  b'ener': 804,\n",
       "  b'ser': 805,\n",
       "  b'11': 806,\n",
       "  b'bject': 807,\n",
       "  b'essage': 808,\n",
       "  b'fer': 809,\n",
       "  b' more': 810,\n",
       "  b'ations': 811,\n",
       "  b'ents': 812,\n",
       "  b' his': 813,\n",
       "  b' they': 814,\n",
       "  b'.S': 815,\n",
       "  b' Y': 816,\n",
       "  b'use': 817,\n",
       "  b'ne': 818,\n",
       "  b'ish': 819,\n",
       "  b'old': 820,\n",
       "  b'_d': 821,\n",
       "  b'io': 822,\n",
       "  b'ield': 823,\n",
       "  b' per': 824,\n",
       "  b'Cont': 825,\n",
       "  b'ings': 826,\n",
       "  b'####': 827,\n",
       "  b' data': 828,\n",
       "  b' sa': 829,\n",
       "  b'ef': 830,\n",
       "  b'fo': 831,\n",
       "  b' one': 832,\n",
       "  b'eng': 833,\n",
       "  b' dis': 834,\n",
       "  b'AT': 835,\n",
       "  b' name': 836,\n",
       "  b' true': 837,\n",
       "  b'val': 838,\n",
       "  b'led': 839,\n",
       "  b'.f': 840,\n",
       "  b' ne': 841,\n",
       "  b' end': 842,\n",
       "  b'32': 843,\n",
       "  b'.T': 844,\n",
       "  b'16': 845,\n",
       "  b'cre': 846,\n",
       "  b'ark': 847,\n",
       "  b'log': 848,\n",
       "  b'Ex': 849,\n",
       "  b'error': 850,\n",
       "  b'_id': 851,\n",
       "  b'urre': 852,\n",
       "  b'ange': 853,\n",
       "  b' null': 854,\n",
       "  b'rray': 855,\n",
       "  b' my': 856,\n",
       "  b'pan': 857,\n",
       "  b'ict': 858,\n",
       "  b'ator': 859,\n",
       "  b'View': 860,\n",
       "  b'List': 861,\n",
       "  b'\\treturn': 862,\n",
       "  b'\\xe2\\x80\\x9d': 863,\n",
       "  b' pre': 864,\n",
       "  b' x': 865,\n",
       "  b'clude': 866,\n",
       "  b'arg': 867,\n",
       "  b'15': 868,\n",
       "  b'ov': 869,\n",
       "  b'.h': 870,\n",
       "  b' >': 871,\n",
       "  b' their': 872,\n",
       "  b\"')\": 873,\n",
       "  b'irst': 874,\n",
       "  b'ick': 875,\n",
       "  b'gh': 876,\n",
       "  b'LE': 877,\n",
       "  b'OR': 878,\n",
       "  b' private': 879,\n",
       "  b'tem': 880,\n",
       "  b'\\r\\n\\r\\n': 881,\n",
       "  b'user': 882,\n",
       "  b' )': 883,\n",
       "  b'com': 884,\n",
       "  b'.A': 885,\n",
       "  b'\";\\n': 886,\n",
       "  b' id': 887,\n",
       "  b'read': 888,\n",
       "  b' who': 889,\n",
       "  b'_b': 890,\n",
       "  b'\">\\n': 891,\n",
       "  b' time': 892,\n",
       "  b' man': 893,\n",
       "  b'ry': 894,\n",
       "  b'========': 895,\n",
       "  b'roup': 896,\n",
       "  b'rop': 897,\n",
       "  b'public': 898,\n",
       "  b'vel': 899,\n",
       "  b'umber': 900,\n",
       "  b'ble': 901,\n",
       "  b' which': 902,\n",
       "  b'****************': 903,\n",
       "  b' any': 904,\n",
       "  b' false': 905,\n",
       "  b'we': 906,\n",
       "  b' value': 907,\n",
       "  b' li': 908,\n",
       "  b'\")': 909,\n",
       "  b'nder': 910,\n",
       "  b'gr': 911,\n",
       "  b' no': 912,\n",
       "  b'param': 913,\n",
       "  b'25': 914,\n",
       "  b'fig': 915,\n",
       "  b'.com': 916,\n",
       "  b' app': 917,\n",
       "  b'_l': 918,\n",
       "  b'ions': 919,\n",
       "  b'.D': 920,\n",
       "  b' Ch': 921,\n",
       "  b' about': 922,\n",
       "  b' add': 923,\n",
       "  b' su': 924,\n",
       "  b' string': 925,\n",
       "  b'ID': 926,\n",
       "  b' over': 927,\n",
       "  b'string': 928,\n",
       "  b'.l': 929,\n",
       "  b'ource': 930,\n",
       "  b'000': 931,\n",
       "  b'_C': 932,\n",
       "  b']\\n': 933,\n",
       "  b' qu': 934,\n",
       "  b' String': 935,\n",
       "  b'ca': 936,\n",
       "  b'SE': 937,\n",
       "  b' ro': 938,\n",
       "  b'sh': 939,\n",
       "  b'ual': 940,\n",
       "  b'Type': 941,\n",
       "  b'son': 942,\n",
       "  b'new': 943,\n",
       "  b'ern': 944,\n",
       "  b' ag': 945,\n",
       "  b'AR': 946,\n",
       "  b'];\\n': 947,\n",
       "  b'].': 948,\n",
       "  b' ?': 949,\n",
       "  b'ical': 950,\n",
       "  b' des': 951,\n",
       "  b'uth': 952,\n",
       "  b'ix': 953,\n",
       "  b'ays': 954,\n",
       "  b' type': 955,\n",
       "  b\"'t\": 956,\n",
       "  b'ault': 957,\n",
       "  b' inter': 958,\n",
       "  b'var': 959,\n",
       "  b'.b': 960,\n",
       "  b' part': 961,\n",
       "  b'.d': 962,\n",
       "  b'urrent': 963,\n",
       "  b'IT': 964,\n",
       "  b'EN': 965,\n",
       "  b'30': 966,\n",
       "  b'enc': 967,\n",
       "  b'(f': 968,\n",
       "  b'ra': 969,\n",
       "  b'value': 970,\n",
       "  b'cho': 971,\n",
       "  b'18': 972,\n",
       "  b'utton': 973,\n",
       "  b'ose': 974,\n",
       "  b'14': 975,\n",
       "  b' !=': 976,\n",
       "  b'ater': 977,\n",
       "  b'\\xc3\\xa9': 978,\n",
       "  b'reate': 979,\n",
       "  b'oll': 980,\n",
       "  b'pos': 981,\n",
       "  b'yle': 982,\n",
       "  b'ng': 983,\n",
       "  b'AL': 984,\n",
       "  b'using': 985,\n",
       "  b'ames': 986,\n",
       "  b' {\\r\\n': 987,\n",
       "  b'ates': 988,\n",
       "  b'ely': 989,\n",
       "  b' work': 990,\n",
       "  b' em': 991,\n",
       "  b'inal': 992,\n",
       "  b' sp': 993,\n",
       "  b' when': 994,\n",
       "  b'.set': 995,\n",
       "  b'      ': 996,\n",
       "  b'):\\n': 997,\n",
       "  b'to': 998,\n",
       "  b'quire': 999,\n",
       "  ...},\n",
       " 'special_tokens': {'<|endoftext|>': 100257,\n",
       "  '<|fim_prefix|>': 100258,\n",
       "  '<|fim_middle|>': 100259,\n",
       "  '<|fim_suffix|>': 100260,\n",
       "  '<|endofprompt|>': 100276,\n",
       "  '<|im_start|>': 100264,\n",
       "  '<|im_end|>': 100269}}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.max_token_value\n",
    "encoder_config = {\n",
    "    \"name\": enc.name,\n",
    "    \"pat_str\": enc._pat_str,\n",
    "    \"mergeable_ranks\": enc._mergeable_ranks,\n",
    "    \"special_tokens\": enc._special_tokens\n",
    "}\n",
    "encoder_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ee8c5211-3215-4ef9-ad2c-d406b66bd6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded text with special tokens: [100269, 3574, 122, 65455, 4916, 249, 31958, 9953, 100257]\n",
      "Decoded text: <|im_end|>举头望明月<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# 重新创建编码器\n",
    "loaded_enc = tiktoken.Encoding(\n",
    "    name=encoder_config[\"name\"],\n",
    "    pat_str=encoder_config[\"pat_str\"],\n",
    "    mergeable_ranks=encoder_config[\"mergeable_ranks\"],\n",
    "    special_tokens=encoder_config[\"special_tokens\"]\n",
    ")\n",
    "\n",
    "# # 测试加载的编码器\n",
    "# test_text = \"举头望明月\"\n",
    "encoded_test = [loaded_enc._special_tokens[\"<|im_end|>\"]] + loaded_enc.encode(test_text) + [loaded_enc._special_tokens[\"<|endoftext|>\"]]\n",
    "print(\"Encoded text with special tokens:\", encoded_test)\n",
    "\n",
    "decoded_test = loaded_enc.decode(encoded_test)\n",
    "print(\"Decoded text:\", decoded_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "291f19b3-d85a-4712-90c5-86cf37f2b233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<|endoftext|>': 100257,\n",
       " '<|fim_prefix|>': 100258,\n",
       " '<|fim_middle|>': 100259,\n",
       " '<|fim_suffix|>': 100260,\n",
       " '<|endofprompt|>': 100276,\n",
       " '<|im_start|>': 100264,\n",
       " '<|im_end|>': 100269}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_enc._special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ee63095f-5082-48e1-b7db-45de4cf550be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentencepiece.bpe.model special_tokens_map.json tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!ls /Users/wangaijun/pythoncode/github/data/text/custom_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "66315f54-115c-42e8-b6f8-178330867b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "special_tokens_map.json tokenizer_config.json   vocab.txt\n",
      "spiece.model            vocab.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!ls /Users/wangaijun/pythoncode/github/data/text/custom_tokenizer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5a1b3b6f-1577-4db7-a3e0-9d00c135f981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added_tokens.json       tokenizer_config.json\n",
      "special_tokens_map.json vocab.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!ls /Users/wangaijun/pythoncode/github/data/text/custom_tokenizer3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "38037635-5e0f-4f5e-97ca-e5eec97c82c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "special_tokens_map.json tokenizer_config.json   vocab.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!ls /Users/wangaijun/pythoncode/github/data/text/custom_tokenizer4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e34f8e0-0402-4024-84a0-830b4c4d5a22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "torch2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
