{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dab1faa-db45-40c3-9d5a-17bd5a426cb8",
   "metadata": {},
   "source": [
    "# 构建BERT(Transformer Encoder)模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453d008f-18b6-41e2-b478-dd421412ab98",
   "metadata": {},
   "source": [
    "# 1 使用pytorch Dataset格式读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86d6c14c-c34e-4b87-a85d-fee6bc887e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "input_path=\"./data/bert_output_data2.json\"\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        self.data = []\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                data = json.loads(line)\n",
    "                self.data.append(data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(item[\"input_ids\"]),\n",
    "            \"input_mask\": torch.tensor(item[\"input_mask\"]),\n",
    "            \"segment_ids\": torch.tensor(item[\"segment_ids\"]),\n",
    "            \"masked_lm_ids\": torch.tensor(item[\"masked_lm_ids\"]),\n",
    "            \"masked_lm_positions\": torch.tensor(item[\"masked_lm_positions\"]),\n",
    "            \"masked_lm_weights\": torch.tensor(item[\"masked_lm_weights\"]),\n",
    "            \"next_sentence_labels\": torch.tensor(item[\"next_sentence_labels\"])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3be040c6-b855-4bfe-9a67-994d66ff9dda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1128, 5425,  ...,  100,  100,  102],\n",
       "         [ 101,  678, 7481,  ...,    0,    0,    0],\n",
       "         [ 101, 2226, 5052,  ..., 6405, 7415,  102],\n",
       "         ...,\n",
       "         [ 101,  100, 8024,  ...,    0,    0,    0],\n",
       "         [ 101, 1062, 2398,  ..., 2832, 6598,  102],\n",
       "         [ 101, 3315,  782,  ...,    0,    0,    0]]),\n",
       " 'input_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'segment_ids': tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 1, 1, 1],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]),\n",
       " 'masked_lm_ids': tensor([[5722, 4638, 2466, 3144, 4276, 1372,  128, 2972,  100,  868, 5445, 3749,\n",
       "          6237,  100,  100,  100, 8024,  100,  100,  100],\n",
       "         [1963, 7302, 2644, 7288, 7971, 2485, 3780, 2356,  738, 2797, 2768,  510,\n",
       "          7094, 7768, 7546, 3490, 4151, 1146, 3490, 2251],\n",
       "         [6639,  782, 8024, 1744, 6432, 1736, 4946, 1894, 3647, 8024, 4639,  511,\n",
       "          5632, 2533, 4691, 4374, 6476, 7572, 1061, 3421],\n",
       "         [4708, 1184, 2689, 1914, 4212, 6848, 6443,  881, 6422, 3796, 4125, 7755,\n",
       "          6944, 7676, 2458, 5517, 8024, 1377,  683, 7030],\n",
       "         [1146, 8024, 2137, 4638, 3309,  769, 2398, 1920, 1378,  671, 3309,  955,\n",
       "          4906, 3613, 1355, 2357, 8024,  100, 1217, 6379],\n",
       "         [8363,  100,  100, 1092, 3175, 1824, 6206, 2397,  782, 8024, 2398, 1322,\n",
       "           754, 4794, 1054, 3124,  818, 6381, 2408,  511],\n",
       "         [1372, 2496, 4638, 2832, 1767, 6814, 2442, 1072,  809, 6858, 1146, 8024,\n",
       "           800,  100, 3315, 2428, 4638, 1068, 7579, 3846],\n",
       "         [2772, 3326, 6435, 4638, 2418, 4638, 4509,  782, 1164, 3326, 6835, 8025,\n",
       "           782, 8024, 2902, 1164, 8038, 6418, 4509, 6435]]),\n",
       " 'masked_lm_positions': tensor([[ 14,  76,  97, 124, 132, 139, 141, 146, 197, 207, 218, 310, 325, 357,\n",
       "          367, 398, 439, 440, 470, 501],\n",
       "         [ 12,  21,  48,  50,  56,  63,  86,  90, 102, 143, 150, 196, 197, 240,\n",
       "          261, 271, 283, 293, 303, 328],\n",
       "         [  7,  37,  39,  49,  77,  90, 111, 141, 153, 178, 181, 198, 250, 252,\n",
       "          268, 317, 387, 426, 439, 497],\n",
       "         [  2,   6,  17,  51,  66,  78,  81,  83,  91, 122, 146, 150, 164, 166,\n",
       "          169, 170, 171, 196, 208, 297],\n",
       "         [ 12,  15,  37,  40,  60,  72,  78,  95, 101, 107, 110, 118, 140, 168,\n",
       "          169, 170, 173, 209, 226, 237],\n",
       "         [ 39,  69,  72,  91, 113, 116, 138, 141, 157, 163, 198, 226, 240, 283,\n",
       "          315, 342, 351, 363, 365, 377],\n",
       "         [  4,  10,  26, 172, 197, 232, 248, 253, 268, 269, 330, 344, 378, 405,\n",
       "          424, 432, 447, 461, 464, 506],\n",
       "         [  5,  41,  73,  79,  83, 109, 126, 128, 131, 132, 150, 154, 187, 224,\n",
       "          226, 235, 254, 258, 261, 262]]),\n",
       " 'masked_lm_weights': tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.]]),\n",
       " 'next_sentence_labels': tensor([0, 1, 0, 0, 0, 0, 1, 1])}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = MyDataset(input_path)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "for data in train_loader:\n",
    "    data\n",
    "    break\n",
    "print(data[\"input_ids\"].shape)\n",
    "print(data[\"input_mask\"].shape)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8564178-6ee5-4e24-926a-4c22dbe53d8f",
   "metadata": {},
   "source": [
    "# BERT 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b1b82b8-6bee-4ddc-a097-7dc904eb920d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21128"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BertConfig:\n",
    "    def __init__(self, vocab_size, hidden_size=768, num_hidden_layers=12, num_attention_heads=12,\n",
    "                 intermediate_size=3072, hidden_act='gelu', hidden_dropout_prob=0.1,\n",
    "                 attention_probs_dropout_prob=0.1, max_position_embeddings=512,\n",
    "                 type_vocab_size=2, initializer_range=0.02):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.hidden_act = hidden_act\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.hidden_dropout_prob = hidden_dropout_prob\n",
    "        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.type_vocab_size = type_vocab_size\n",
    "        self.initializer_range = initializer_range\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('/Users/wangaijun/pythoncode/github/model/bert-base-chinese')\n",
    "vocab_words = list(tokenizer.vocab.keys())\n",
    "config=BertConfig(len(vocab_words))\n",
    "config.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb372d03-c4af-4e56-aaa1-584ed9791389",
   "metadata": {},
   "source": [
    "# 2.1 Embedding层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27a5ad05-8c5b-4641-9688-e1c874ed5802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids shape: torch.Size([8, 512]) token_type_ids shape torch.Size([8, 512])\n"
     ]
    }
   ],
   "source": [
    "input_ids=data[\"input_ids\"]\n",
    "token_type_ids=data[\"segment_ids\"]\n",
    "print(\"input_ids shape:\",input_ids.shape,\"token_type_ids shape\",token_type_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2127ce72-2172-4f9f-ae36-44d605be4b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 512, 768]) torch.Size([512, 768]) torch.Size([8, 512, 768])\n"
     ]
    }
   ],
   "source": [
    "word_embeddings = nn.Embedding(21128, config.hidden_size)\n",
    "position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
    "\n",
    "token_emb=word_embeddings(input_ids)\n",
    "position_emb=position_embeddings(torch.arange(input_ids.shape[1], dtype=torch.long, device=input_ids.device))\n",
    "sentence_emb=token_type_embeddings(token_type_ids)\n",
    "\n",
    "print(token_emb.shape,position_emb.shape,sentence_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05d5a630-19fb-4dac-b9e4-39cd7d227a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEmbeddings(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(BertEmbeddings, self).__init__()\n",
    "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None):\n",
    "        seq_length = input_ids.size(1)\n",
    "        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros_like(input_ids)\n",
    "\n",
    "        words_embeddings = self.word_embeddings(input_ids)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
    "\n",
    "        embeddings = words_embeddings + position_embeddings + token_type_embeddings\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "beaa797a-534a-4506-a640-3f3838d95e37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 512, 768])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embModel=BertEmbeddings(config)\n",
    "x_emb=embModel(input_ids,token_type_ids)\n",
    "x_emb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6883a335-b150-4e6e-831b-dffcb35cd4ad",
   "metadata": {},
   "source": [
    "# 2.2 attention 层"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83367081-4808-4411-8fd4-80fc2789858d",
   "metadata": {},
   "source": [
    "### 2.2.1 q,k,v加工及多头变换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4716e00-aa0b-4c86-b9bc-9aa7c466ef0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 512, 768]) torch.Size([8, 512, 768]) torch.Size([8, 512, 768])\n",
      "torch.Size([8, 12, 512, 64]) torch.Size([8, 12, 512, 64]) torch.Size([8, 12, 512, 64])\n"
     ]
    }
   ],
   "source": [
    "c_attn = nn.Linear(config.hidden_size, 3 * config.hidden_size)\n",
    "\n",
    "# q,k,v 都来自于x \n",
    "q, k, v  = c_attn(x_emb).split(config.hidden_size, dim=2)\n",
    "print(q.shape,k.shape,v.shape)\n",
    "\n",
    "B,T,C=x_emb.shape\n",
    "# 给q,k,v 增加head\n",
    "q = q.view(B, T, config.num_attention_heads, C // config.num_attention_heads).transpose(1, 2) # (B, nh, T, hs)\n",
    "k = k.view(B, T, config.num_attention_heads, C // config.num_attention_heads).transpose(1, 2) # (B, nh, T, hs)\n",
    "v = v.view(B, T, config.num_attention_heads, C // config.num_attention_heads).transpose(1, 2) # (B, nh, T, hs)\n",
    "print(q.shape,k.shape,v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cd1794-2895-48fc-b166-357c05381cbe",
   "metadata": {},
   "source": [
    "#### 2.2.2 attention score 计算以及mask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ae97d39-1829-434e-8787-ea458270a24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_scores shape : torch.Size([8, 12, 512, 512])\n",
      "attention_mask shape : torch.Size([8, 1, 1, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 512, 768])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "attention_scores = torch.matmul(q, k.transpose(-1, -2))/ math.sqrt(q.shape[-1])\n",
    "print(\"attention_scores shape :\",attention_scores.shape)\n",
    "\n",
    "attention_mask=data[\"input_mask\"]\n",
    "attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "print(\"attention_mask shape :\",attention_mask.shape)\n",
    "\n",
    "attention_scores = attention_scores.masked_fill(attention_mask == 0, float('-inf'))\n",
    "\n",
    "# Normalize the attention scores to probabilities.\n",
    "attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
    "\n",
    "context_layer = torch.matmul(attention_probs, v)\n",
    "context_layer = context_layer.transpose(1, 2).contiguous().view(B, T, C) \n",
    "context_layer.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90857b92-e575-4807-9c5c-cd43df751a1c",
   "metadata": {
    "scrolled": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc0265a6-78a2-4c56-bc47-e266b09bc066",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(BertSelfAttention, self).__init__()\n",
    "        if config.hidden_size % config.num_attention_heads!= 0:\n",
    "            raise ValueError(\n",
    "                \"The hidden size (%d) is not a multiple of the number of attention heads (%d)\" % (\n",
    "                    config.hidden_size, config.num_attention_heads))\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
    "        self.hidden_size=config.hidden_size\n",
    "\n",
    "        self.query = nn.Linear(config.hidden_size, self.hidden_size)\n",
    "        self.key = nn.Linear(config.hidden_size, self.hidden_size)\n",
    "        self.value = nn.Linear(config.hidden_size, self.hidden_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask):\n",
    "        B, T, C = hidden_states.size()\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "        mixed_key_layer = self.key(hidden_states)\n",
    "        mixed_value_layer = self.value(hidden_states)\n",
    "        \n",
    "        query_layer = mixed_query_layer.view(B, T, self.num_attention_heads, C // self.num_attention_heads).transpose(1, 2) # (B, nh, T, hs)\n",
    "        key_layer = mixed_key_layer.view(B, T, self.num_attention_heads, C // self.num_attention_heads).transpose(1, 2) # (B, nh, T, hs)\n",
    "        value_layer = mixed_value_layer.view(B, T, self.num_attention_heads, C // self.num_attention_heads).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "            attention_scores = attention_scores.masked_fill(attention_mask == 0, float('-inf'))\n",
    "\n",
    "        # Normalize the attention scores to probabilities.\n",
    "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
    "\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        context_layer = context_layer.transpose(1, 2).contiguous().view(B, T, C) \n",
    "        return context_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d5ba4f1-6b6c-4bb0-9299-9edf06172d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_mask shape torch.Size([8, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 512, 768])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_mask=data[\"input_mask\"]\n",
    "print(\"input_mask shape\",input_mask.shape)\n",
    "attenModel=BertSelfAttention(config)\n",
    "x_att=attenModel(x_emb,input_mask)\n",
    "x_att.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099eae3a-47c3-4f84-964b-a0900e8ec428",
   "metadata": {},
   "source": [
    "## 3 attention +add&Norm\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"images/encoder_atten.png\" alt=\"Image\" style=\"width:300px;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e718e0d5-b628-4908-bbb9-5cc96c2a5482",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSelfOutput(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(BertSelfOutput, self).__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states\n",
    "\n",
    "class BertAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(BertAttention, self).__init__()\n",
    "        self.self = BertSelfAttention(config)\n",
    "        self.output = BertSelfOutput(config)\n",
    "\n",
    "    def forward(self, input_tensor, attention_mask):\n",
    "        self_output = self.self(input_tensor, attention_mask)\n",
    "        attention_output = self.output(self_output, input_tensor)\n",
    "        return attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74839b6b-e119-45e2-9089-62ab46835082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 512, 768])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bertAttModel=BertAttention(config)\n",
    "x_att=bertAttModel(x_emb,input_mask)\n",
    "x_att.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b21de25-391a-4705-8879-71b541f43071",
   "metadata": {},
   "source": [
    "## 4 FFN层\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"images/encoder_mlp.png\" alt=\"Image\" style=\"width:300px;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8bed3288-d88a-481c-a096-1845296951a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(FFN, self).__init__()\n",
    "        self.dense1 = nn.Linear(config.hidden_size, config.intermediate_size)        \n",
    "        self.dense2 = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        \n",
    "    def gelu(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden_states=x\n",
    "        hidden_states = self.dense1(hidden_states)\n",
    "        hidden_states = self.gelu(hidden_states)\n",
    "        hidden_states = self.dense2(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + x)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8990bf2-cae7-4c5d-b516-d83bc8c61c31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 512, 768])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp=FFN(config)\n",
    "x_mlp=mlp(x_att)\n",
    "x_mlp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e149bf2-3ca6-4399-8f46-2950c2e81d6d",
   "metadata": {},
   "source": [
    "## 4 Block层\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"images/encoder_block.png\" alt=\"Image\" style=\"width:300px;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d090f55-1ae0-4e8a-8c74-12991178fb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOCK 层\n",
    "class BertLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(BertLayer, self).__init__()\n",
    "        self.attention = BertAttention(config)\n",
    "        self.mlp = FFN(config)\n",
    "       \n",
    "    def forward(self, hidden_states, attention_mask):\n",
    "        attention_output = self.attention(hidden_states, attention_mask)\n",
    "        layer_output = self.mlp(attention_output)\n",
    "    \n",
    "        return layer_output\n",
    "\n",
    "# 多层BLOCK层\n",
    "class BertEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(BertEncoder, self).__init__()\n",
    "        self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask, output_all_encoded_layers=True):\n",
    "        all_encoder_layers = []\n",
    "        for layer_module in self.layer:\n",
    "            hidden_states = layer_module(hidden_states, attention_mask)\n",
    "            if output_all_encoded_layers:\n",
    "                all_encoder_layers.append(hidden_states)\n",
    "        if not output_all_encoded_layers:\n",
    "            all_encoder_layers.append(hidden_states)\n",
    "        return all_encoder_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf81a306-c001-4076-885f-5268fbdfc7c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 512, 768])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model=BertEncoder(config)\n",
    "x_bloks=bert_model(x_emb,input_mask)\n",
    "x_bloks[-1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcdd7e3-db24-4a85-a461-cd8b710dab74",
   "metadata": {},
   "source": [
    "## 5 CLS 输出层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "563e9e54-7e4f-4850-a730-7206c0a72f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertPooler(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(BertPooler, self).__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
    "        # to the first token.\n",
    "        first_token_tensor = hidden_states[:, 0]\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5dbf6012-85db-4b88-b691-e57f2a0477f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 768])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clsModel=BertPooler(config)\n",
    "x_cls=clsModel(x_bloks[-1])\n",
    "x_cls.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9617bd29-93b0-4faa-89b7-6b408fa43bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 768]) torch.Size([8, 512, 768]) 12\n",
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(21128, 768)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (mlp): FFN(\n",
      "          (dense1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (dense2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class BertModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(BertModel, self).__init__()\n",
    "        self.embeddings = BertEmbeddings(config)\n",
    "        self.encoder = BertEncoder(config)\n",
    "        self.pooler = BertPooler(config)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None):\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones_like(input_ids)\n",
    "        embedding_output = self.embeddings(input_ids, token_type_ids)\n",
    "       \n",
    "        encoded_layers = self.encoder(embedding_output, attention_mask)\n",
    "        sequence_output = encoded_layers[-1]\n",
    "        pooled_output = self.pooler(sequence_output)\n",
    "        return pooled_output, sequence_output, encoded_layers\n",
    "\n",
    "bert_model = BertModel(config)\n",
    "# 假设你已经定义了你的模型 model\n",
    "total_params = sum(p.numel() for p in bert_model.parameters())\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "\n",
    "input_mask=data[\"input_mask\"]\n",
    "input_ids=data[\"input_ids\"]\n",
    "token_type_ids=data[\"segment_ids\"]\n",
    "pooled_output, sequence_output, encoded_layers=bert_model(input_ids,token_type_ids,input_mask)\n",
    "print(pooled_output.shape,sequence_output.shape,len(encoded_layers))\n",
    "print(bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ebb01d-26bf-4908-9aea-0ce9bf7dc555",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9432f98b-889f-4e99-90d2-17c0bbc65e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 102267648\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539219dd-69a0-42fc-8d75-451962d81ef1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20cc4df-6d21-4882-adeb-b2b460c16722",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "torch2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
