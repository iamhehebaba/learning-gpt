{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f43d64e4-04dd-4fd3-84e5-e487dd550998",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert.bert_model import BertModel\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from transformers import BertTokenizer\n",
    "input_path=\"./data/bert_output_data2.json\"\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        self.data = []\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                data = json.loads(line)\n",
    "                self.data.append(data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(item[\"input_ids\"]),\n",
    "            \"input_mask\": torch.tensor(item[\"input_mask\"]),\n",
    "            \"segment_ids\": torch.tensor(item[\"segment_ids\"]),\n",
    "            \"masked_lm_ids\": torch.tensor(item[\"masked_lm_ids\"]),\n",
    "            \"masked_lm_positions\": torch.tensor(item[\"masked_lm_positions\"]),\n",
    "            \"masked_lm_weights\": torch.tensor(item[\"masked_lm_weights\"]),\n",
    "            \"next_sentence_labels\": torch.tensor(item[\"next_sentence_labels\"])\n",
    "        }\n",
    "\n",
    "class BertConfig:\n",
    "    def __init__(self, vocab_size, hidden_size=144, num_hidden_layers=3, num_attention_heads=12,\n",
    "                 intermediate_size=512, hidden_act='gelu', hidden_dropout_prob=0.1,\n",
    "                 attention_probs_dropout_prob=0.1, max_position_embeddings=512,\n",
    "                 type_vocab_size=2, initializer_range=0.02):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.hidden_act = hidden_act\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.hidden_dropout_prob = hidden_dropout_prob\n",
    "        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.type_vocab_size = type_vocab_size\n",
    "        self.initializer_range = initializer_range\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc8e4479-5ff2-4266-9062-bdf46201307a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  4263,  2209,  ...,     0,     0,     0],\n",
       "         [  101,  1462,   103,  ...,  1469,   821,   102],\n",
       "         [  101,  7216,   677,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,   782,  1920,  ...,  4852,   833,   102],\n",
       "         [  101, 19688,  6371,  ...,   100,   100,   102],\n",
       "         [  101,  4638,  3175,  ...,  2821,   103,   102]]),\n",
       " 'input_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]]),\n",
       " 'segment_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 1, 1, 1],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 1, 1, 1],\n",
       "         [0, 0, 0,  ..., 1, 1, 1],\n",
       "         [0, 0, 0,  ..., 1, 1, 1]]),\n",
       " 'masked_lm_ids': tensor([[8205,  782,  807, 6848, 1999,  868, 4638, 8024,  517,  511, 1068,  704,\n",
       "          1059, 6375, 4638, 2798,  833, 3635, 2399, 3299],\n",
       "         [1092, 5682, 4761, 1088, 3194,  818, 1213,  738, 5273, 1092, 3613, 1093,\n",
       "          2399, 1168, 1076, 5018, 1092, 1076, 1912, 8024],\n",
       "         [2157, 1355, 6598, 7354, 2658, 1920, 6874, 7372, 1158,  752, 1366, 1440,\n",
       "          4638,  711,  511, 3634, 4997, 7032, 6631,  955],\n",
       "         [5722, 4638, 2466, 3144, 4276, 1372,  128, 2972,  100,  868, 5445, 3749,\n",
       "          6237,  100,  100,  100, 8024,  100,  100,  100],\n",
       "         [1168, 6226, 3297, 3297, 1290, 1305, 5296, 4692, 2792, 3341,  757,  741,\n",
       "           868, 6121, 5966,  517, 1305, 1762, 2399, 2814],\n",
       "         [4638, 1139,  671, 1158, 2658,  749, 7360, 8024,  852, 6874, 1139, 2956,\n",
       "          3791, 3326, 8032, 7557,  683, 1862, 3221, 1744],\n",
       "         [5307, 3221, 2885, 6206, 8043, 2190, 2137, 1469,  966, 6587, 7770, 1469,\n",
       "          3419,  678, 4873,  966, 1963,  812,  812,  510],\n",
       "         [5442, 6121, 1215, 4638, 6768, 5018, 6820, 2137, 3791,  518, 1152, 1469,\n",
       "          1355, 5865, 2495, 1072, 5320, 4495, 3341, 2689],\n",
       "         [6371, 6206, 6121, 4638,  100, 7410,  809, 6598,  782,  678, 8024,  671,\n",
       "          8024, 3918, 3680, 5688, 2190, 2418, 7216, 8024],\n",
       "         [ 511, 6821, 1068, 1925,  704,  712, 2340,  697, 3189, 6205, 1359, 3341,\n",
       "          1104, 7151, 1066, 8024, 1298, 1139, 4664, 1066],\n",
       "         [1346, 1453, 8024, 1266, 1054, 1103, 1894,  511, 2128,  833, 6379, 4638,\n",
       "          3299, 5745, 1741, 6237, 7270, 2141, 5298, 6843],\n",
       "         [8024,  855, 6444, 1333,  677,  833, 1762, 1744, 1920, 3299,  928, 4638,\n",
       "          4852,  517,  518, 4638, 5439, 8024, 8629,  776],\n",
       "         [7028, 2398, 7270,  721, 1762, 7583, 2397, 6956, 8024, 2991, 3833, 8911,\n",
       "          8024, 8024, 1220, 8024, 7481, 3144, 4810, 4638],\n",
       "         [1079, 1062,  722, 5445, 6890, 7361, 1164, 8024, 1744, 3318, 8024, 5865,\n",
       "          4638, 4507, 2496, 8020, 8024, 1072, 1358, 8020],\n",
       "         [1814,  684, 3613, 4638, 2577, 5307,  511, 1079, 1071, 1378, 2772,  924,\n",
       "          4955, 4638, 5632, 1355, 3566, 8024,  718, 2094],\n",
       "         [ 511, 6598, 2207, 7028, 7216, 6206, 8024, 2357,  897, 3291, 4638, 6848,\n",
       "           711, 3918, 7216, 4638, 3680, 6121,  689,  511],\n",
       "         [1469, 1153, 8024, 3621, 1772, 7372, 5445, 6814, 7583,  886, 6598, 1920,\n",
       "          1105, 3221,  511, 8038, 4500, 1501, 4307,  511],\n",
       "         [ 510,  852, 3221, 6158, 2496, 7354,  762, 8024, 2094, 5815,  678, 6639,\n",
       "          6848, 5468, 7379, 4638, 1395, 6639, 7313, 4685],\n",
       "         [5722, 4993, 5468, 6206, 8024, 4873, 1199, 7027,  671, 2809,  782, 1862,\n",
       "          2466, 1395, 6929,  511,  800, 1469, 1291,  704],\n",
       "         [1348,  800,  738, 4995, 6371, 1542, 1447, 6150, 7368, 1146, 4638, 6716,\n",
       "           511, 7008, 3198, 3300, 2376, 6413, 8024, 4638],\n",
       "         [8363,  100,  100, 1092, 3175, 1824, 6206, 2397,  782, 8024, 2398, 1322,\n",
       "           754, 4794, 1054, 3124,  818, 6381, 2408,  511],\n",
       "         [2772, 3326, 6435, 4638, 2418, 4638, 4509,  782, 1164, 3326, 6835, 8025,\n",
       "           782, 8024, 2902, 1164, 8038, 6418, 4509, 6435],\n",
       "         [2262, 6432, 8024,  677, 8188, 2399, 3791, 2137, 8038,  862, 8043,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0],\n",
       "         [3309, 3791, 4638, 2501, 2501, 2418, 4638,  886, 1744, 1164,  683, 1164,\n",
       "           671, 3309, 3326, 3634, 1062, 8024, 6848, 7028],\n",
       "         [ 100,  116, 3563, 2458, 5052, 6436, 3519, 3266, 5966,  510, 6206,  511,\n",
       "          6121,  897, 7032, 1486, 5468, 6084,  680, 5440],\n",
       "         [7555, 3837, 4684, 2137, 7555, 1168, 5277,  511, 6587, 4680, 6598, 6574,\n",
       "          2487, 6444, 7309, 2418, 7557, 7481, 1392, 5390],\n",
       "         [7676, 5546, 2458, 8024, 4850, 1103, 5442, 7831, 3384, 3297, 8220,  704,\n",
       "          1227, 4171, 2466, 7883, 3022, 6843,  683, 1545],\n",
       "         [4680, 5290,  782, 4919, 4802,  769, 2496, 4415, 3696, 2137, 1104,  782,\n",
       "          3326, 2772, 1394, 4995, 2400, 4667, 3791, 1398],\n",
       "         [1963, 7302, 2644, 7288, 7971, 2485, 3780, 2356,  738, 2797, 2768,  510,\n",
       "          7094, 7768, 7546, 3490, 4151, 1146, 3490, 2251],\n",
       "         [6134,  749, 2399,  697, 4638, 6392, 6574, 4638, 1939, 3791, 2137,  976,\n",
       "           754, 2141, 4638, 6821, 6448, 4862, 1282, 1920],\n",
       "         [6795, 8039, 4638, 3022,  511, 1216, 4638, 2949,  860, 3300, 5543,  100,\n",
       "          3291, 1066, 1104,  749,  100, 8363,  118,  100],\n",
       "         [3696, 1380, 3209, 2229, 3583, 1999,  511, 1092, 4638, 6808, 2514, 1059,\n",
       "          1265, 6393, 6449, 7270, 1156,  749, 3118, 4906]]),\n",
       " 'masked_lm_positions': tensor([[  8,  43,  45, 108, 119, 189, 246, 258, 262, 271, 298, 339, 340, 356,\n",
       "          363, 365, 377, 381, 434, 436],\n",
       "         [  2,  18,  44,  60, 142, 145, 172, 200, 238, 239, 268, 281, 315, 325,\n",
       "          345, 389, 417, 422, 484, 485],\n",
       "         [  5,  31,  71,  81,  98, 109, 111, 114, 117, 120, 125, 128, 146, 180,\n",
       "          185, 187, 191, 213, 223, 227],\n",
       "         [ 14,  76,  97, 124, 132, 139, 141, 146, 197, 207, 218, 310, 325, 357,\n",
       "          367, 398, 439, 440, 470, 501],\n",
       "         [  2,  31,  33,  46,  48,  63,  65,  67,  71,  76,  78,  89,  94, 100,\n",
       "          114, 115, 120, 121, 135, 143],\n",
       "         [  5,  11,  21,  30,  42,  61,  74,  78,  79,  86,  91, 102, 131, 138,\n",
       "          157, 183, 193, 223, 230, 234],\n",
       "         [ 11,  36,  63, 143, 144, 180, 201, 202, 228, 282, 364, 373, 378, 388,\n",
       "          425, 459, 461, 474, 480, 504],\n",
       "         [  6,  10,  13,  28,  45,  55,  76,  80,  84,  85,  88, 105, 130, 133,\n",
       "          142, 177, 214, 219, 222, 223],\n",
       "         [  4,  10,  28,  45,  49,  64,  65, 112, 140, 160, 161, 168, 173, 179,\n",
       "          186, 190, 207, 209, 210, 219],\n",
       "         [ 40,  45,  49, 123, 163, 172, 181, 184, 216, 260, 263, 272, 286, 295,\n",
       "          311, 331, 400, 439, 446, 481],\n",
       "         [  7,  28,  37,  66,  84, 101, 166, 179, 189, 207, 224, 235, 240, 309,\n",
       "          310, 312, 363, 402, 407, 418],\n",
       "         [ 12,  36,  39,  57,  68,  81,  92,  97, 140, 168, 178, 182, 227, 318,\n",
       "          326, 390, 450, 476, 489, 499],\n",
       "         [  6,  60, 109, 139, 149, 163, 176, 177, 192, 238, 245, 310, 345, 360,\n",
       "          387, 391, 397, 430, 451, 482],\n",
       "         [ 19,  42,  59,  74, 124, 157, 162, 167, 168, 203, 237, 242, 320, 334,\n",
       "          381, 400, 416, 439, 442, 477],\n",
       "         [ 36,  43,  45,  76,  87, 105, 150, 162, 191, 217, 239, 291, 295, 319,\n",
       "          340, 385, 435, 453, 482, 499],\n",
       "         [ 17,  28,  52,  61,  70,  74,  79,  80,  90,  96, 100, 101, 105, 117,\n",
       "          121, 122, 123, 136, 137, 154],\n",
       "         [  8,  12,  13,  16,  18,  29,  88, 104, 108, 110, 127, 157, 175, 190,\n",
       "          192, 199, 201, 208, 211, 235],\n",
       "         [ 15,  22,  36,  78, 106, 135, 143, 191, 248, 260, 262, 267, 282, 319,\n",
       "          328, 348, 414, 421, 462, 471],\n",
       "         [  1,  22,  27,  39,  73,  83, 109, 125, 145, 151, 173, 214, 242, 296,\n",
       "          328, 348, 378, 439, 443, 484],\n",
       "         [ 70,  76,  77, 135, 197, 212, 225, 234, 270, 279, 340, 341, 343, 370,\n",
       "          372, 436, 438, 443, 444, 454],\n",
       "         [ 39,  69,  72,  91, 113, 116, 138, 141, 157, 163, 198, 226, 240, 283,\n",
       "          315, 342, 351, 363, 365, 377],\n",
       "         [  5,  41,  73,  79,  83, 109, 126, 128, 131, 132, 150, 154, 187, 224,\n",
       "          226, 235, 254, 258, 261, 262],\n",
       "         [  2,   3,   4,   6,   9,  10,  20,  35,  42,  44,  52,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0],\n",
       "         [ 55, 151, 176, 177, 191, 212, 243, 249, 280, 317, 328, 338, 347, 377,\n",
       "          381, 386, 422, 424, 466, 491],\n",
       "         [  1,   2,   9,  15,  17,  20,  21,  27,  28,  39,  56,  58,  69,  90,\n",
       "           93, 105, 108, 111, 149, 163],\n",
       "         [  7,  42,  52,  55,  60,  65,  75,  85,  90,  95, 109, 110, 115, 116,\n",
       "          159, 171, 181, 184, 222, 235],\n",
       "         [  6,   9,  57,  69,  73,  78,  97, 119, 182, 205, 265, 274, 278, 293,\n",
       "          297, 309, 354, 377, 385, 473],\n",
       "         [ 11,  67,  95, 105, 122, 129, 162, 225, 232, 271, 281, 293, 413, 423,\n",
       "          441, 473, 475, 477, 478, 507],\n",
       "         [ 12,  21,  48,  50,  56,  63,  86,  90, 102, 143, 150, 196, 197, 240,\n",
       "          261, 271, 283, 293, 303, 328],\n",
       "         [  9,  10, 134, 159, 164, 174, 178, 181, 223, 239, 253, 317, 343, 351,\n",
       "          363, 410, 414, 463, 488, 495],\n",
       "         [  1,  21,  34,  71,  92, 109, 119, 150, 163, 187, 278, 322, 363, 386,\n",
       "          399, 400, 471, 500, 503, 506],\n",
       "         [ 36,  47,  49,  66,  79, 119, 158, 215, 229, 262, 273, 318, 336, 404,\n",
       "          438, 463, 488, 495, 497, 510]]),\n",
       " 'masked_lm_weights': tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.]]),\n",
       " 'next_sentence_labels': tensor([0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,\n",
       "         0, 1, 1, 1, 1, 1, 1, 1])}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_dataset = MyDataset(input_path)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "for data in train_loader:\n",
    "    data\n",
    "    break\n",
    "print(data[\"input_ids\"].shape)\n",
    "print(data[\"input_mask\"].shape)\n",
    "input_ids=data[\"input_ids\"]\n",
    "token_type_ids=data[\"segment_ids\"]\n",
    "attention_mask=data[\"input_mask\"]\n",
    "masked_lm_positions=data[\"masked_lm_positions\"]\n",
    "masked_lm_ids=data[\"masked_lm_ids\"]\n",
    "masked_lm_weights=data[\"masked_lm_weights\"]\n",
    "next_sentence_labels=data[\"next_sentence_labels\"]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51e772d7-24c2-423e-9d4e-4c65aff59fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_indexes(sequence_tensor, positions):\n",
    "    sequence_shape = list(sequence_tensor.shape)\n",
    "    batch_size = sequence_shape[0]\n",
    "    seq_length = sequence_shape[1]\n",
    "    width = sequence_shape[2]\n",
    "\n",
    "    flat_offsets = torch.arange(0, batch_size, dtype=torch.int64).reshape(-1, 1) * seq_length\n",
    "    flat_positions = (positions + flat_offsets).reshape(-1)\n",
    "    flat_sequence_tensor = sequence_tensor.reshape(batch_size * seq_length, width)\n",
    "    output_tensor = flat_sequence_tensor[flat_positions]\n",
    "    return output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "160c53d9-1062-4842-a0ed-17c1eb1a4fd3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.4812, 0.7262, 0.2435, 0.5518, 0.9686, 0.1407, 0.0784, 0.4719,\n",
      "          0.6008, 0.7062],\n",
      "         [0.4333, 0.2983, 0.3437, 0.6014, 0.0281, 0.1652, 0.1080, 0.0771,\n",
      "          0.3824, 0.0508],\n",
      "         [0.9324, 0.5528, 0.4446, 0.5430, 0.5504, 0.4381, 0.9909, 0.2223,\n",
      "          0.0389, 0.1210],\n",
      "         [0.6169, 0.9299, 0.5685, 0.5774, 0.2957, 0.0931, 0.8956, 0.4808,\n",
      "          0.1888, 0.9418],\n",
      "         [0.0100, 0.8559, 0.0202, 0.4111, 0.6520, 0.4036, 0.6835, 0.6983,\n",
      "          0.5808, 0.2298],\n",
      "         [0.0159, 0.4673, 0.8373, 0.5593, 0.6574, 0.9926, 0.4005, 0.3090,\n",
      "          0.4683, 0.7394]],\n",
      "\n",
      "        [[0.6759, 0.8836, 0.5332, 0.9862, 0.3775, 0.1172, 0.7190, 0.9611,\n",
      "          0.8943, 0.3026],\n",
      "         [0.2646, 0.3314, 0.2111, 0.9224, 0.3554, 0.5549, 0.1032, 0.0603,\n",
      "          0.7090, 0.5561],\n",
      "         [0.1240, 0.1333, 0.3225, 0.9056, 0.3678, 0.5768, 0.1364, 0.9830,\n",
      "          0.1813, 0.1304],\n",
      "         [0.0517, 0.8949, 0.9011, 0.5888, 0.8939, 0.4046, 0.2699, 0.6114,\n",
      "          0.4632, 0.8878],\n",
      "         [0.8891, 0.2688, 0.0545, 0.4445, 0.2563, 0.1139, 0.7542, 0.2109,\n",
      "          0.3799, 0.6123],\n",
      "         [0.6340, 0.9508, 0.8339, 0.5182, 0.5434, 0.1386, 0.5247, 0.9030,\n",
      "          0.6434, 0.1408]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.4812, 0.7262, 0.2435, 0.5518, 0.9686, 0.1407, 0.0784, 0.4719, 0.6008,\n",
       "         0.7062],\n",
       "        [0.6169, 0.9299, 0.5685, 0.5774, 0.2957, 0.0931, 0.8956, 0.4808, 0.1888,\n",
       "         0.9418],\n",
       "        [0.0159, 0.4673, 0.8373, 0.5593, 0.6574, 0.9926, 0.4005, 0.3090, 0.4683,\n",
       "         0.7394],\n",
       "        [0.6759, 0.8836, 0.5332, 0.9862, 0.3775, 0.1172, 0.7190, 0.9611, 0.8943,\n",
       "         0.3026],\n",
       "        [0.0517, 0.8949, 0.9011, 0.5888, 0.8939, 0.4046, 0.2699, 0.6114, 0.4632,\n",
       "         0.8878],\n",
       "        [0.6340, 0.9508, 0.8339, 0.5182, 0.5434, 0.1386, 0.5247, 0.9030, 0.6434,\n",
       "         0.1408]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_=torch.rand(2,6,10)\n",
    "positions=torch.tensor([0,3,5])#torch.arange(10)\n",
    "print(input_)\n",
    "gather_indexes(input_,positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8da98777-e36f-4571-be4a-8aea50d44c05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 144]) torch.Size([32, 512, 144]) 3\n",
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(21128, 144)\n",
      "    (position_embeddings): Embedding(512, 144)\n",
      "    (token_type_embeddings): Embedding(2, 144)\n",
      "    (LayerNorm): LayerNorm((144,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-2): 3 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=144, out_features=144, bias=True)\n",
      "            (key): Linear(in_features=144, out_features=144, bias=True)\n",
      "            (value): Linear(in_features=144, out_features=144, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=144, out_features=144, bias=True)\n",
      "            (LayerNorm): LayerNorm((144,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (mlp): FFN(\n",
      "          (dense1): Linear(in_features=144, out_features=512, bias=True)\n",
      "          (dense2): Linear(in_features=512, out_features=144, bias=True)\n",
      "          (LayerNorm): LayerNorm((144,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=144, out_features=144, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('/Users/wangaijun/pythoncode/github/model/bert-base-chinese')\n",
    "vocab_words = list(tokenizer.vocab.keys())\n",
    "config=BertConfig(len(vocab_words))\n",
    "config.vocab_size\n",
    "bert_model = BertModel(config)\n",
    "\n",
    "pooled_output, sequence_output, encoded_layers=bert_model(input_ids,token_type_ids,attention_mask)\n",
    "print(pooled_output.shape,sequence_output.shape,len(encoded_layers))\n",
    "print(bert_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3141904c-9347-416c-9a21-e9bd52cee29e",
   "metadata": {},
   "source": [
    "# 2 mask预测损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1dddca1-c230-432b-8bf9-72b7f81fe60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_masked_lm_output(bert_config, input_tensor, output_weights, positions, label_ids, label_weights):\n",
    "    input_tensor = gather_indexes(input_tensor, positions)\n",
    "    sequential = nn.Sequential(\n",
    "        nn.Linear(bert_config.hidden_size, bert_config.hidden_size),\n",
    "        nn.LayerNorm(bert_config.hidden_size),\n",
    "        nn.ReLU()\n",
    "    )\n",
    "    input_tensor = sequential(input_tensor)  # 使用sequential处理input_tensor\n",
    "    output_bias = nn.Parameter(torch.zeros(bert_config.vocab_size))\n",
    "    logits = torch.matmul(input_tensor, output_weights.transpose(0, 1)) + output_bias\n",
    "    log_probs = nn.functional.log_softmax(logits, dim=-1)\n",
    "\n",
    "    label_ids = label_ids.reshape(-1)\n",
    "    label_weights = label_weights.reshape(-1)\n",
    "    one_hot_labels = torch.nn.functional.one_hot(label_ids, num_classes=bert_config.vocab_size).float()\n",
    "    per_example_loss = -torch.sum(log_probs * one_hot_labels, dim=-1)\n",
    "    numerator = torch.sum(label_weights * per_example_loss)\n",
    "    denominator = torch.sum(label_weights) + 1e-5\n",
    "    loss = numerator / denominator\n",
    "\n",
    "    return loss, per_example_loss, log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b75a2d1f-d99e-4980-ad69-cd743ad84620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33.317161560058594"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_lm_loss, _, _ = get_masked_lm_output(config, sequence_output, bert_model.embeddings.word_embeddings.weight,\n",
    "                                            masked_lm_positions, masked_lm_ids, masked_lm_weights)\n",
    "masked_lm_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9bdbd7d-410f-45ba-ab53-39cea798fb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_sentence_output(bert_config, input_tensor, labels):\n",
    "    output_weights = nn.Parameter(torch.randn(2, bert_config.hidden_size))\n",
    "    output_bias = nn.Parameter(torch.zeros(2))\n",
    "    logits = torch.matmul(input_tensor, output_weights.transpose(0, 1)) + output_bias\n",
    "    log_probs = nn.functional.log_softmax(logits, dim=-1)\n",
    "    labels = labels.reshape(-1)\n",
    "    one_hot_labels = torch.nn.functional.one_hot(labels, num_classes=2).float()\n",
    "    per_example_loss = -torch.sum(one_hot_labels * log_probs, dim=-1)\n",
    "    loss = torch.mean(per_example_loss)\n",
    "    return loss, per_example_loss, log_probs\n",
    "next_sentence_loss, _, _ = get_next_sentence_output(config, pooled_output, next_sentence_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ced160f-090b-45d4-b361-2ee1307293af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.5587, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_sentence_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "611572ab-d3f0-4bad-acce-183542e8ae85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(37.8759, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = masked_lm_loss + next_sentence_loss\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0dce9ad9-2f1f-4949-adec-3d02fc334e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(bert_model.parameters(), lr=5e-4)\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed53cc21-ad9f-47a4-9fac-1fc22bf861ca",
   "metadata": {},
   "source": [
    "# 5 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a769d87-19db-4f17-835d-bfea6c56edfc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding weight step 0 tensor([[-0.3895, -2.1538,  0.7998,  2.3261, -0.5966],\n",
      "        [ 0.8771,  0.7261,  1.3902, -0.0718,  0.8855],\n",
      "        [-1.0391, -0.7370, -1.7500, -0.3334, -0.2318]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "embedding weight step 1 tensor([[-0.3895, -2.1539,  0.7998,  2.3259, -0.5966],\n",
      "        [ 0.8770,  0.7258,  1.3899, -0.0720,  0.8852],\n",
      "        [-1.0391, -0.7372, -1.7500, -0.3335, -0.2318]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "embedding weight step 2 tensor([[-0.3895, -2.1539,  0.7998,  2.3258, -0.5966],\n",
      "        [ 0.8770,  0.7256,  1.3897, -0.0721,  0.8850],\n",
      "        [-1.0391, -0.7373, -1.7501, -0.3335, -0.2319]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "embedding weight step 3 tensor([[-0.3896, -2.1539,  0.7996,  2.3256, -0.5969],\n",
      "        [ 0.8770,  0.7254,  1.3895, -0.0723,  0.8849],\n",
      "        [-1.0391, -0.7376, -1.7501, -0.3336, -0.2322]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "embedding weight step 3 tensor([[-0.3897, -2.1540,  0.7994,  2.3254, -0.5971],\n",
      "        [ 0.8770,  0.7252,  1.3893, -0.0724,  0.8847],\n",
      "        [-1.0392, -0.7378, -1.7502, -0.3338, -0.2324]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "embedding weight step 3 tensor([[-0.3899, -2.1541,  0.7991,  2.3251, -0.5974],\n",
      "        [ 0.8769,  0.7251,  1.3891, -0.0725,  0.8846],\n",
      "        [-1.0392, -0.7380, -1.7502, -0.3339, -0.2326]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "embedding weight step 3 tensor([[-0.3902, -2.1542,  0.7988,  2.3249, -0.5976],\n",
      "        [ 0.8769,  0.7249,  1.3890, -0.0725,  0.8845],\n",
      "        [-1.0392, -0.7382, -1.7502, -0.3340, -0.2327]],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "EPOCHS=5\n",
    "i=0\n",
    "for epoch in range(EPOCHS):\n",
    "    for data in train_loader:\n",
    "        input_ids=data[\"input_ids\"]\n",
    "        token_type_ids=data[\"segment_ids\"]\n",
    "        attention_mask=data[\"input_mask\"]\n",
    "        masked_lm_positions=data[\"masked_lm_positions\"]\n",
    "        masked_lm_ids=data[\"masked_lm_ids\"]\n",
    "        masked_lm_weights=data[\"masked_lm_weights\"]\n",
    "        next_sentence_labels=data[\"next_sentence_labels\"]\n",
    "        optimizer.zero_grad()\n",
    "        pooled_output, sequence_output, encoded_layers=bert_model(input_ids,token_type_ids,attention_mask)\n",
    "        masked_lm_loss, _, _ = get_masked_lm_output(config, sequence_output, bert_model.embeddings.word_embeddings.weight,\n",
    "                                            masked_lm_positions, masked_lm_ids, masked_lm_weights)\n",
    "        next_sentence_loss, _, _ = get_next_sentence_output(config, pooled_output, next_sentence_labels)\n",
    "        loss = masked_lm_loss + next_sentence_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"embedding weight step {i}\",bert_model.embeddings.word_embeddings.weight[:3,:5])\n",
    "        if i>=3:\n",
    "            break\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20e65604-36de-478a-92bb-4847351603c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(bert_model.state_dict(), 'bert_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da7620e0-5595-492e-a9df-71261ca6ec11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(21128, 144)\n",
       "    (position_embeddings): Embedding(512, 144)\n",
       "    (token_type_embeddings): Embedding(2, 144)\n",
       "    (LayerNorm): LayerNorm((144,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-2): 3 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=144, out_features=144, bias=True)\n",
       "            (key): Linear(in_features=144, out_features=144, bias=True)\n",
       "            (value): Linear(in_features=144, out_features=144, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=144, out_features=144, bias=True)\n",
       "            (LayerNorm): LayerNorm((144,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): FFN(\n",
       "          (dense1): Linear(in_features=144, out_features=512, bias=True)\n",
       "          (dense2): Linear(in_features=512, out_features=144, bias=True)\n",
       "          (LayerNorm): LayerNorm((144,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=144, out_features=144, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 模型加载\n",
    "tokenizer = BertTokenizer.from_pretrained('/Users/wangaijun/pythoncode/github/model/bert-base-chinese')\n",
    "vocab_words = list(tokenizer.vocab.keys())\n",
    "config=BertConfig(len(vocab_words))\n",
    "model = BertModel(config)\n",
    "\n",
    "model.load_state_dict(torch.load('bert_model.pth'))\n",
    "model.eval()  # 切换到评估模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72842134-af2e-48fb-89c7-f77ebb1e6dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_output, sequence_output, encoded_layers=model(input_ids,token_type_ids,attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "33f8465e-1a11-4a6d-a04d-68e1978f0484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 144])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08702765-44b6-4bc1-b652-0a85584e7f0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "torch2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
