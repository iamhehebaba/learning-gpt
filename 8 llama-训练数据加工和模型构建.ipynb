{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bc084af-6dcd-4e3f-af17-c5d8c09303e8",
   "metadata": {},
   "source": [
    "# llama2模型训练——数据处理，模型构建，模型训练及推理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9a4c26-d8da-4b8b-8691-a418c10ececd",
   "metadata": {},
   "source": [
    "## 参考 ：https://github.com/meta-llama/llama3/tree/main\n",
    "## 数据集：https://modelscope.cn/datasets/AI-ModelScope/chinese-c4\n",
    "## paper : \n",
    "### llama2: https://arxiv.org/abs/2307.09288\n",
    "###         llama 3: https://arxiv.org/abs/2407.21783\n",
    "## 本代码git: https://github.com/zhangjianzoujianghu18/learning-gpt.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598ef4d9-a529-4d18-9c62-6ca250594490",
   "metadata": {},
   "source": [
    "# 1 训练数据集加工 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "13c521f2-c748-418f-a20e-0aae692c222d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zstandard as zstd\n",
    "import json\n",
    "import io\n",
    "import os\n",
    "import glob\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import tiktoken\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset, Dataset, IterableDataset\n",
    "from typing import Any, Optional, Tuple\n",
    "\n",
    "# 定义一个生成器函数来读取 .jsonl.zst 文件\n",
    "def read_jsonl_zst(file_path):\n",
    "    with open(file_path, 'rb') as fh:\n",
    "        dctx = zstd.ZstdDecompressor()\n",
    "        stream_reader = dctx.stream_reader(fh)\n",
    "        text_stream = io.TextIOWrapper(stream_reader, encoding='utf-8')\n",
    "        for line in text_stream:\n",
    "            yield json.loads(line)\n",
    "\n",
    "# 定义一个生成器函数来读取所有 .jsonl.zst 文件\n",
    "def read_all_jsonl_zst(files):\n",
    "    for file_path in files:\n",
    "        yield from read_jsonl_zst(file_path)\n",
    "\n",
    "\n",
    "def process(example):\n",
    "    ids = enc.encode_ordinary(example['text'])\n",
    "    ids.append(enc.eot_token)\n",
    "    return {'ids': ids, 'len': len(ids)}\n",
    "\n",
    "class StreamingParquetDataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, jsonl_zst_files, split, block_size, num_proc=14):\n",
    "        self.data_files = jsonl_zst_files\n",
    "        self.split = split\n",
    "        self.block_size = block_size\n",
    "        self.num_proc = num_proc\n",
    "        self.dataset = IterableDataset.from_generator( lambda: read_all_jsonl_zst(jsonl_zst_files))\n",
    "#              load_dataset(\"arrow\", data_files={split: data_files}, streaming=True)\n",
    "        self.tokenized = self.dataset.map(process)\n",
    "    def __iter__(self):\n",
    "        for example in self.tokenized:\n",
    "            ids = example['ids']\n",
    "            for i in range(0, len(ids) - self.block_size, self.block_size):\n",
    "                x = torch.tensor(ids[i:i + self.block_size], dtype=torch.int64)\n",
    "                y = torch.tensor(ids[i + 1:i + 1 + self.block_size], dtype=torch.int64)\n",
    "                yield x, y\n",
    "\n",
    "# 示例函数：获取一个批次的数据\n",
    "def get_batch(loader, device,device_type):\n",
    "    for x, y in loader:\n",
    "        if device_type == 'cuda':\n",
    "            x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "        else:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "        yield x, y\n",
    "        \n",
    "def get_train_data_from_stream_data(data_path_root,enc,batch_size=32,block_size=128):\n",
    "    block_size = block_size  # 根据你的模型设置合适的块大小\n",
    "    batch_size = batch_size  # 根据你的硬件设置合适的批次大小\n",
    " \n",
    "    # 查找所有 .jsonl.zst 文件\n",
    "    jsonl_zst_files = glob.glob(f'{data_path_root}/*.jsonl.zst', recursive=True)\n",
    "\n",
    "    # 创建数据集\n",
    "    train_dataset = StreamingParquetDataset(jsonl_zst_files[:-1], 'train', block_size)\n",
    "    val_dataset = StreamingParquetDataset([jsonl_zst_files[-1]], 'val', block_size)\n",
    "\n",
    "    # 创建数据加载器\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "    return train_loader,val_loader\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a7039d7b-80d9-42e7-be2c-857603fd4f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_path_root = \"/Users/wangaijun/pythoncode/github/data/text/chinese-c4\"\n",
    "batch_size=32\n",
    "max_seq_len=512\n",
    "# 查找所有 .jsonl.zst 文件\n",
    "\n",
    "train_loader,val_loader=get_train_data_from_stream_data(data_path_root,enc,batch_size=batch_size,block_size=max_seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d3a0b1ff-849a-4f41-abc7-2250f3147004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: torch.Size([32, 512])  y shape:  torch.Size([32, 512])\n",
      "tensor([[16175,   121, 36651,  ..., 50266,   111, 50182],\n",
      "        [15355,   100, 21589,  ..., 37767, 54493, 17599],\n",
      "        [37795,   237, 84949,  ...,   238, 42506,  9174],\n",
      "        ...,\n",
      "        [14354,   606, 36969,  ...,   114, 71600, 76537],\n",
      "        [12870,    97,   163,  ..., 43240, 19361, 41914],\n",
      "        [43032,  1811, 56235,  ..., 13646, 17885,   245]])\n",
      "tensor([[  121, 36651, 85315,  ...,   111, 50182, 15355],\n",
      "        [  100, 21589,   242,  ..., 54493, 17599,   230],\n",
      "        [  237, 84949,   222,  ..., 42506,  9174, 20713],\n",
      "        ...,\n",
      "        [  606, 36969,    64,  ..., 71600, 76537, 67178],\n",
      "        [   97,   163,   123,  ..., 19361, 41914, 43032],\n",
      "        [ 1811, 56235, 32943,  ..., 17885,   245, 43240]])\n"
     ]
    }
   ],
   "source": [
    "for x,y in train_loader:\n",
    "    print(\"x shape:\",x.shape,\" y shape: \",y.shape)\n",
    "    print(x)\n",
    "    print(y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356f86ab-f664-4723-9143-a4ccad35b73f",
   "metadata": {},
   "source": [
    "# 2 模型构建"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac7a243-c8a6-4d8f-859c-44ce32b79450",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"images/transformer.png\" alt=\"Image\" style=\"width:600px;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1bda96ac-d313-4221-8478-6294ff7a5f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: torch.Size([32, 256])  y shape:  torch.Size([32, 256])\n",
      "tensor([[16175,   121, 36651,  ...,   255, 25781,   252],\n",
      "        [31867,  5486, 29504,  ..., 50266,   111, 50182],\n",
      "        [15355,   100, 21589,  ...,  3922, 81258, 66201],\n",
      "        ...,\n",
      "        [ 8107,    22,  9953,  ..., 97150, 64803, 23602],\n",
      "        [  232, 17792, 27384,  ..., 83799,  8239,   123],\n",
      "        [ 3922,  6701,   249,  ...,  4468,    24,  8107]])\n",
      "tensor([[  121, 36651, 85315,  ..., 25781,   252, 31867],\n",
      "        [ 5486, 29504, 14309,  ...,   111, 50182, 15355],\n",
      "        [  100, 21589,   242,  ..., 81258, 66201, 86436],\n",
      "        ...,\n",
      "        [   22,  9953,  3922,  ..., 64803, 23602,   232],\n",
      "        [17792, 27384, 30537,  ...,  8239,   123,  3922],\n",
      "        [ 6701,   249, 67178,  ...,    24,  8107,    16]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "\n",
    "# @dataclass\n",
    "# class ModelArgs:\n",
    "#     dim: int = 4096\n",
    "#     n_layers: int = 32\n",
    "#     n_heads: int = 32\n",
    "#     n_kv_heads: Optional[int] = None\n",
    "#     vocab_size: int = -1\n",
    "#     multiple_of: int = 256  # make SwiGLU hidden layer size multiple of large power of 2\n",
    "#     ffn_dim_multiplier: Optional[float] = None\n",
    "#     norm_eps: float = 1e-5\n",
    "#     rope_theta: float = 500000\n",
    "\n",
    "#     max_batch_size: int = 32\n",
    "#     max_seq_len: int = 2048\n",
    "\n",
    "\n",
    "class config:\n",
    "    block_size=256\n",
    "    vocab_size=100277\n",
    "    n_layers=3\n",
    "    n_head=4\n",
    "    n_embd=128\n",
    "    dropout=0.0\n",
    "    bias=False\n",
    "    norm_eps = 1e-5\n",
    "    multiple_of: int = 20\n",
    "    dim=128\n",
    "device_type = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.device(device_type)\n",
    "\n",
    "train_loader,val_loader=get_train_data_from_stream_data(data_path_root,enc,batch_size=batch_size,block_size=config.block_size)\n",
    "for x,y in train_loader:\n",
    "    print(\"x shape:\",x.shape,\" y shape: \",y.shape)\n",
    "    print(x)\n",
    "    print(y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d86a28-7c90-4ff1-b55d-afc7dbfc4175",
   "metadata": {},
   "source": [
    "## 2.1 embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "90bd0d73-733d-484f-a388-cba485401969",
   "metadata": {},
   "outputs": [],
   "source": [
    "wte = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "# wpe = nn.Embedding(config.block_size, config.n_embd)// 改为旋转位置编码\n",
    "drop = nn.Dropout(config.dropout)\n",
    "ln_f = nn.LayerNorm(config.n_embd, bias=config.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "63c31904-1660-425d-bab4-f07c0b5fe41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_embd shape:  torch.Size([32, 256, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 256, 128])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_embd=wte(x)\n",
    "print(\"x_embd shape: \",x_embd.shape)\n",
    "x_embd_ln=ln_f(x_embd)\n",
    "x_embd_ln.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641a61c2-ed1b-4376-865f-7cea099fa0f9",
   "metadata": {},
   "source": [
    "## 2.2 FFN层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "88137426-b375-40d6-b0f8-a25d97845ded",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 256, 128])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim: int, hidden_dim: int, multiple_of: int, dropout: float):\n",
    "        super().__init__()\n",
    "        hidden_dim = int(2 * hidden_dim / 3)\n",
    "        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n",
    "        self.w1 = nn.Linear(dim, hidden_dim, bias=False)\n",
    "        self.w2 = nn.Linear(hidden_dim, dim, bias=False)\n",
    "        self.w3 = nn.Linear(dim, hidden_dim, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.w2(F.silu(self.w1(x)) * self.w3(x)))\n",
    "\n",
    "mlp=FeedForward(config.n_embd,config.n_embd,config.multiple_of,config.dropout)(x_embd_ln)\n",
    "mlp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1787108e-c837-4cf8-b200-1ff0202647b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(torch.nn.Module):\n",
    "    def __init__(self, dim: int, eps: float):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self._norm(x.float()).type_as(x)\n",
    "        return output * self.weight\n",
    "rms=RMSNorm(config.n_embd,config.norm_eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a645a915-40f9-4149-86bb-bf13dd668bb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 256, 128])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rms=rms(mlp)\n",
    "rms.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4ec4f3-e91d-4726-b569-27355ce6fd6b",
   "metadata": {},
   "source": [
    "## 2.4 旋转位置编码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3803fb42-1959-4fbe-90ce-a5547bcced3d",
   "metadata": {},
   "source": [
    "### B站视频： https://www.bilibili.com/video/BV14JB1Y3E6i/?spm_id_from=333.999.0.0&vd_source=6f858f592d89bed2a97f471f3232ad57"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "187c5ad1-70c3-430a-aae9-121b6f714cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n",
      "          0.0000e+00,  1.0000e+00],\n",
      "        [ 8.4147e-01,  5.4030e-01,  5.3317e-01,  ...,  1.0000e+00,\n",
      "          1.7783e-04,  1.0000e+00],\n",
      "        [ 9.0930e-01, -4.1615e-01,  9.0213e-01,  ...,  1.0000e+00,\n",
      "          3.5566e-04,  1.0000e+00],\n",
      "        ...,\n",
      "        [ 9.9482e-01, -1.0162e-01, -7.8375e-01,  ...,  9.9680e-01,\n",
      "          4.4975e-02,  9.9899e-01],\n",
      "        [ 4.5200e-01, -8.9202e-01, -9.9420e-01,  ...,  9.9678e-01,\n",
      "          4.5153e-02,  9.9898e-01],\n",
      "        [-5.0639e-01, -8.6230e-01, -8.9845e-01,  ...,  9.9675e-01,\n",
      "          4.5331e-02,  9.9897e-01]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "def get_rotary_position_embedding(seq_len, dim):\n",
    "    position = torch.arange(0, seq_len, dtype=torch.float32).unsqueeze(1)\n",
    "    div_term = 1. / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "    pe = torch.zeros(seq_len, dim)\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    return pe\n",
    "rope=get_rotary_position_embedding(config.block_size, int(config.n_embd/config.n_head))\n",
    "print(rope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "48e458c1-a75f-4181-9646-e96b70dd822f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 4, 256, 32])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def apply_rotary_position_embedding(x, rope):\n",
    "    # 分离奇数和偶数索引\n",
    "    x1 = x[..., ::2]  # 偶数索引\n",
    "    x2 = x[..., 1::2]  # 奇数索引\n",
    "    # 从rope中获取相应的cosine和sine部分\n",
    "    cos_pos = rope[:, 0::2].repeat((x.size(0), x.size(1), 1, 1))  # 偶数索引\n",
    "    sin_pos = rope[:, 1::2].repeat((x.size(0), x.size(1), 1, 1))  # 奇数索引\n",
    "    # 应用旋转位置编码\n",
    "    return torch.cat([x1 * cos_pos - x2 * sin_pos, x2 * cos_pos + x1 * sin_pos], dim=-1)\n",
    "\n",
    "x=rms.reshape(32,config.n_head,config.block_size,-1)\n",
    "apply_rotary_position_embedding(x, rope).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c33cfe-88a7-4234-96ef-1ecef0322202",
   "metadata": {},
   "source": [
    "## 2.5 attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "847032c8-5f0e-4faa-baf5-85c2378e4e6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
    "            # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                        .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x,rope):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs) \n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # 添加旋转位置编码\n",
    "        q=apply_rotary_position_embedding(q, rope)\n",
    "        k= apply_rotary_position_embedding(k, rope)\n",
    "        \n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        if self.flash:\n",
    "            # efficient attention using Flash Attention CUDA kernels\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
    "        else:\n",
    "            # manual implementation of attention\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8a372631-ecb1-4f46-b65e-ea0dcbb29c1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 256, 128])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atten=CausalSelfAttention(config)(rms,rope)\n",
    "atten.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435c50a9-3a42-45b4-a074-3310fe9dfd15",
   "metadata": {},
   "source": [
    "## 2.6 block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "494e3e6e-9e28-439b-8927-00196d39a0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, layer_id: int, config):\n",
    "        super().__init__()\n",
    "        self.n_heads = config.n_head\n",
    "        self.dim = config.n_embd\n",
    "        self.head_dim = config.n_embd // config.n_head\n",
    "        self.attention = CausalSelfAttention(config)\n",
    "        self.feed_forward = FeedForward(\n",
    "            dim=config.n_embd,\n",
    "            hidden_dim=1 * config.n_embd,\n",
    "            multiple_of=config.multiple_of,\n",
    "            dropout=config.dropout,\n",
    "        )\n",
    "        self.layer_id = layer_id\n",
    "        self.attention_norm = RMSNorm(config.n_embd, eps=config.norm_eps)\n",
    "        self.ffn_norm = RMSNorm(config.n_embd, eps=config.norm_eps)\n",
    "\n",
    "    def forward(self, x,rope):\n",
    "        h = x + self.attention(self.attention_norm(x), rope)\n",
    "        out = h + self.feed_forward.forward(self.ffn_norm(h))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "74f4566b-9fdc-4583-b074-238c459b9130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 256, 128])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TransformerBlock(1,config)(rms,rope).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "53d46c45-4075-476b-8c59-5664261dee99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    last_loss: Optional[torch.Tensor]\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "        self.params = params\n",
    "        self.vocab_size = params.vocab_size\n",
    "        self.n_layers = params.n_layers\n",
    "\n",
    "        self.tok_embeddings = nn.Embedding(params.vocab_size, params.dim)\n",
    "        self.dropout = nn.Dropout(params.dropout)\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for layer_id in range(params.n_layers):\n",
    "            self.layers.append(TransformerBlock(layer_id, params))\n",
    "        self.norm = RMSNorm(params.dim, eps=params.norm_eps)\n",
    "        self.output = nn.Linear(params.dim, params.vocab_size, bias=False)\n",
    "\n",
    "        # share the unembedding parameters with the embedding parameters\n",
    "        self.tok_embeddings.weight = self.output.weight # https://paperswithcode.com/method/weight-tying\n",
    "\n",
    "        # some useful precompute for the RoPE relative positional embeddings\n",
    "        # freqs_cos, freqs_sin = precompute_freqs_cis(self.params.dim // self.params.n_heads, self.params.max_seq_len)\n",
    "        # self.register_buffer(\"freqs_cos\", freqs_cos, persistent=False)\n",
    "        # self.register_buffer(\"freqs_sin\", freqs_sin, persistent=False)\n",
    "        rope=get_rotary_position_embedding(config.block_size, int(config.n_embd/config.n_head))\n",
    "        self.register_buffer(\"rope\", rope, persistent=False)\n",
    "        \n",
    "        # init all weights\n",
    "        self.apply(self._init_weights)\n",
    "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('w3.weight') or pn.endswith('wo.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * params.n_layers))\n",
    "\n",
    "        # Initialize attribute for the loss of the last forward call. This will be set if the forward is called with a targets tensor.\n",
    "        self.last_loss = None\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def get_num_params(self):\n",
    "        \"\"\"\n",
    "        Return the number of parameters in the model.\n",
    "        For non-embedding count (default), the position embeddings get subtracted.\n",
    "        The token embeddings would too, except due to the parameter sharing these\n",
    "        params are actually used as weights in the final layer, so we include them.\n",
    "        \"\"\"\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        return n_params\n",
    "        \n",
    "    def forward(self, tokens: torch.Tensor, targets: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        _bsz, seqlen = tokens.shape\n",
    "        h = self.tok_embeddings(tokens)\n",
    "        h = self.dropout(h)\n",
    "        rope = self.rope[:seqlen]\n",
    "    \n",
    "        for layer in self.layers:\n",
    "            h = layer(h, rope)\n",
    "        h = self.norm(h)\n",
    "\n",
    "        if targets is not None:\n",
    "            # if we are given some desired targets also calculate the loss\n",
    "            logits = self.output(h)\n",
    "            self.last_loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            # inference-time mini-optimization: only forward the output on the very last position\n",
    "            logits = self.output(h[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
    "            self.last_loss = None\n",
    "\n",
    "        return logits,self.last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "211b86b9-d7ac-4b67-889e-40b8d71aca6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型参数： 13148160\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (tok_embeddings): Embedding(100277, 128)\n",
       "  (dropout): Dropout(p=0.0, inplace=False)\n",
       "  (layers): ModuleList(\n",
       "    (0-2): 3 x TransformerBlock(\n",
       "      (attention): CausalSelfAttention(\n",
       "        (c_attn): Linear(in_features=128, out_features=384, bias=False)\n",
       "        (c_proj): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (w1): Linear(in_features=128, out_features=100, bias=False)\n",
       "        (w2): Linear(in_features=100, out_features=128, bias=False)\n",
       "        (w3): Linear(in_features=128, out_features=100, bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (attention_norm): RMSNorm()\n",
       "      (ffn_norm): RMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (norm): RMSNorm()\n",
       "  (output): Linear(in_features=128, out_features=100277, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama2_model=Transformer(config)\n",
    "print(\"模型参数：\",llama2_model.get_num_params())\n",
    "llama2_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de69c931-f962-45f6-855d-d0f1780f710d",
   "metadata": {},
   "source": [
    "# 3 模型训练\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4548824f-1ebe-46a5-bb32-a14d47b34c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adamw optimizer\n",
    "learning_rate = 6e-4 # max learning rate\n",
    "max_iters = 600000 # total number of training iterations\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.95\n",
    "grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n",
    "optimizer = torch.optim.AdamW(llama2_model.parameters(), lr=learning_rate, betas=(beta1,beta2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "50f05fb6-7258-41da-b406-f5bf1fe745f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 256]) torch.Size([32, 256])\n",
      "embedding weight step 0 tensor([[-1.3147e-02, -2.3653e-02, -2.3193e-02,  8.1439e-03, -3.8375e-03],\n",
      "        [ 3.0824e-02, -2.1510e-02,  2.2791e-05, -2.2051e-02, -2.2887e-02],\n",
      "        [ 3.2242e-03, -1.5278e-02,  2.0917e-02, -5.3500e-03,  3.0080e-04]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "torch.Size([32, 256]) torch.Size([32, 256])\n",
      "embedding weight step 1 tensor([[-0.0125, -0.0231, -0.0226,  0.0087, -0.0044],\n",
      "        [ 0.0314, -0.0209,  0.0006, -0.0215, -0.0235],\n",
      "        [ 0.0038, -0.0147,  0.0215, -0.0048, -0.0003]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "torch.Size([32, 256]) torch.Size([32, 256])\n",
      "embedding weight step 2 tensor([[-0.0120, -0.0225, -0.0220,  0.0093, -0.0050],\n",
      "        [ 0.0320, -0.0204,  0.0012, -0.0209, -0.0241],\n",
      "        [ 0.0044, -0.0141,  0.0221, -0.0042, -0.0009]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "torch.Size([32, 256]) torch.Size([32, 256])\n",
      "embedding weight step 3 tensor([[-0.0114, -0.0220, -0.0215,  0.0099, -0.0056],\n",
      "        [ 0.0317, -0.0207,  0.0009, -0.0211, -0.0237],\n",
      "        [ 0.0050, -0.0136,  0.0227, -0.0036, -0.0015]],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for x, y in get_batch(train_loader, device,\"cpu\"):\n",
    "    # 在这里进行模型训练\n",
    "    print(x.shape, y.shape)\n",
    "    print(f\"embedding weight step {i}\",llama2_model.tok_embeddings.weight[:3,:5])\n",
    "    logits, loss = llama2_model(x, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i==3:\n",
    "        break  # 只打印一个批次的数据\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ee45c3c-7299-425d-ba57-a667d4533ac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 256])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e6604af-1753-4f8e-8a54-e07a21251ba7",
   "metadata": {},
   "source": [
    "# 4 模型推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4a093946-b87d-4445-aa7f-d5aa9f508d55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before idx: tensor([[9736, 7584, 3388, 2397],\n",
      "        [2889, 7981, 6261, 8787]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 9736,  7584,  3388,  2397, 27899, 97338, 97338, 57318, 58571, 65182,\n",
       "         97338, 14415, 65182, 65182, 97338, 65182, 65182,  3045, 65182,  3045,\n",
       "         12167, 97338, 97338, 65182, 65182, 65182, 39294, 39294, 65182,  3045,\n",
       "         65182, 65182, 39294, 39294],\n",
       "        [ 2889,  7981,  6261,  8787, 86636, 86636, 48389, 44959, 34522, 60240,\n",
       "         60240, 34522, 34993, 34993, 17885, 34732, 90718, 90718, 35722, 75778,\n",
       "         35722, 34732, 17885,  3922, 35722, 17885,  3922, 99046, 35722, 35722,\n",
       "         35722,  3922,  3922,  4130]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_new_tokens=30\n",
    "temperature=1.0\n",
    "top_k=4\n",
    "idx=torch.randint(0,10000,(2,4))\n",
    "print(\"before idx:\",idx)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits, _ = llama2_model(idx)\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        # optionally crop the logits to only the top k options\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "            logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "        # apply softmax to convert logits to (normalized) probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        # sample from the distribution\n",
    "        idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        # append sampled index to the running sequence and continue\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5be19c4a-feec-4a88-b1bf-b573f756ea7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 34])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028b3907-3d0e-45bc-bddf-caa1e72cd664",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "torch2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
